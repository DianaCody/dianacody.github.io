---
layout: post
title: 防止爬虫被墙的方法总结
description: 爬虫的目的就是大规模地、长时间地获取数据，大规模集中对服务器访问，时间一长就有可能被拒绝。大规模集中访问对服务器的影响较大，爬虫可以短时间增大服务器负载。
---

#####一、设置下载等待时间/下载频率

大规模集中访问对服务器的影响较大，爬虫可以短时间增大服务器负载。这里需要注意的是：设定下载等待时间的范围控制，等待时间过长，不能满足短时间大规模抓取的要求，等待时间过短则很有可能被拒绝访问。

(1). 在之前“从url获取HTML”的方法里，对于httpGet的配置设置了socket超时和连接connect超时，其实这里的时长不是绝对的，主要取决于目标网站对爬虫的控制。

(2). 另外，在scrapy爬虫框架里，专有参数可以设置下载等待时间download_delay，这个参数可以设置在setting.py里，也可以设置在spider里。

#####二、设置cookies

cookie其实是储存在用户终端的一些被加密的数据，有些网站通过cookies来识别用户身份，如果某个访问总是高频率地发请求，很可能会被网站注意到，被嫌疑为爬虫，这时网站就可以通过cookie找到这个访问的用户而拒绝其访问。

可以自定义设置cookie策略（防止cookie rejected问题：拒绝写入cookie）或者禁止cookies。

*(1) 自定义设置cookies策略（防止cookierejected问题，拒绝写入cookie）*

在系列一那篇文章里就有自定义cookie策略设置，但更多的借鉴是官方文档的例子，设置方法其实都大同小异，因为HttpClient-4.3.1组件版本跟以前旧版本的不同，写法也有不同，另见官方文档：http://hc.apache.org/httpcomponents-client-4.3.x/tutorial/html/statemgmt.html#d5e553

*(2) 禁止cookies*

通过禁止cookie，这是客户端主动阻止服务器写入。禁止cookie可以防止可能使用cookies识别爬虫的网站来ban掉我们。

在scrapy爬虫中可以设置COOKIES_ENABLES= FALSE，即不启用cookies middleware，不向web server发送cookies。
 
#####三、修改User-Agent

最常见的就是伪装浏览器，修改User-Agent（用户代理）。

User-Agent是指包含浏览器信息、操作系统信息等的一个字符串，也称之为一种特殊的网络协议。服务器通过它判断当前访问对象是浏览器、邮件客户端还是网络爬虫。在request.headers里可以查看user-agent，关于怎么分析数据包、查看其User-Agent等信息，这个在前面的文章里提到过。

具体方法可以把User-Agent的值改为浏览器的方式，甚至可以设置一个User-Agent池（list，数组，字典都可以），存放多个“浏览器”，每次爬取的时候随机取一个来设置request的User-Agent，这样User-Agent会一直在变化，防止被墙。
 
#####四、修改IP

其实微博识别的是IP，不是账号。也就是说，当需要连续抓取很多数据的时候，模拟登录没有意义。只要是同一个IP，不管怎么换账号也没有用，主要的是换IP。

web server应对爬虫的策略之一就是直接将IP或者整个IP段都封掉禁止访问，当IP被禁封后，转换到其他IP继续访问即可。方法：代理IP、本地IP数据库（使用IP池）。

*(1) 从代理IP网站获取大量IP*

如果总是请求代理IP站点也未免有些麻烦，况且某些代理IP站点有时还可能被禁封，当然再换一个代理IP站点也可以，如果你不嫌麻烦的话。

*(2) 使用IP地址库*

网上也有很多现成可用的IP地址库，可以存放到本地，如果本地有IP数据库就方便很多，至少不用每次都去请求代理IP站点了（当然可以一次性把站点内所有代理IP先爬下来存储好，形成本地IP数据库），总之获取IP的方法有很多，不一定非要是通过代理IP站点。
 
#####五、分布式爬取

分布式爬取的也有很多Github repo。原理主要是维护一个所有集群机器能够有效分享的分布式队列。

使用分布式爬取还有另外一个目的：大规模抓取，单台机器的负荷很大，况且速度很慢，多台机器可以设置一个master管理多台slave去同时爬取。

另外关于网页判重问题，可以用Bloom Filter。