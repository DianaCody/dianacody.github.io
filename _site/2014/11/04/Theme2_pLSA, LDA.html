<!DOCTYPE html>
<html>
<head>
  <!--
    **
    *
    * Author:         DianaCody
    * Contact:        dianacodyleaf@gmail.com
    * Theme Name:     Block Rude
    * Version:        1.0
    *
    **
  -->
  <meta charset="utf-8" />
  <title>主题模型（二）：pLSA和LDA | DianaCody's Shell</title>
  <meta name="generator" content="Jekyll" />
  <meta name="author" content="DianaCody" />
  <meta name="description" content="" />
  <link rel="stylesheet" href="/css/style.css" type="text/css" />
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <link rel="shortcut icon" href="images/favicon.jpg" type="image/x-icon" />
</head>
<body>
    <header class="head">
	<div class="header fn-clear">
		<section class="header-cnt fn-clear">
			<div class="logo">
				<div class="logo-cnt">
				 	<h1><a href="/">DianaCody's Shell</a></h1>
				 	<h2>Focus on Search Engine, ML & IR & NLP.</h2>
				</div>
			</div>
			<nav class="nav">
				<ul>
					<li><a href="/" >Home</a></li>
					<li><a href="/archives.html" >Archives</a></li>					
					<li><a href="/contact.html" >About & Contact</a></li>
					<!--<li><a href="/resume.html" >Resume</a></li>-->
					<li><a href="/atom.xml" target="_blank" >Rss</a></li>
				</ul>
			</nav>
		</section>
	</div>
</header> 
  <section id="content">
    <article id="contenter">
      <section class="content">
	<div class="content-cnt fn-clear">
		<div class="main fn-clear">
			<article class="entry">
				<h1 class="title">主题模型（二）：pLSA和LDA</h1>
               	<p class="meta"><time class="date" pubdate="2014-11-04">04 Nov 2014</time> By <a href="/contact.html" class="author" title="author">DianaCody</a><g:plusone size="small"></g:plusone></p>
               	<p class="description">pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。和pLSA不同的是LDA中假设了很多先验分布（Dirichlet），且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同。</p>
 			 	<h5>一、pLSA（概率潜在语义分析）</h5>

<p>pLSA:    <em>-------有过拟合问题，就是求D, Z, W</em></p>

<p>pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。</p>

<p>在论文《GoogleNews Personalization Scalable Online CF》一文中提级针对用户聚类，利用相似用户性信息计算喜欢的news。其中包含min-hash以及plsi，plsi是model-based 推荐算法，属于topic(aspect) model，其在NLP领域中用途很大。</p>

<p>引入：在文本挖掘时，计算文档相似性是很基础的操作，通常，对文本进行分词，构建VSM，通过jaccard或者cosin计算距离或者相似性，这是基于corpus的思路，仅仅考虑词组，并未考虑文本的语义信息。针对下面情况，基于cropus很难处理：</p>

<ul>
<li>如果时间回到2006年，马云和杨致远的手还会握在一起吗</li>
<li>阿里巴巴集团和雅虎就股权回购一事签署了最终协议</li>
</ul>


<p>如果采用基于corpus的jaccard距离等算法，那么这两个文本的完全不相关，但是事实上，马云和阿里巴巴集团，杨致远和雅虎有着密切的联系，从语义上看，两者都和“阿里巴巴"有关系。</p>

<p>此外，另一个case：</p>

<ul>
<li>富士苹果真好，赶快买</li>
<li>苹果四代真好，赶快买</li>
</ul>


<p>从corpus上来看，两者非常相似，但是事实上，2个句子从语义上来讲，没有任何关系，一个是”水果“另一个是”手机"。</p>

<p>通过上面的例子，差不多也看出来topic model是什么以及解决什么问题。</p>

<p>概念：topic model是针对文本隐含主题的建模方法，针对第一个case，马云对应的主题是阿里巴巴，阿里巴巴集团也隐含阿里巴巴主题，这样两个文本的主题匹配上，认为他们是相关的，针对第二个，分别针对水果以及手机主题，我们认为他们是不相关的。</p>

<p>究竟什么是主题？[接下来参考baidu搜索研发部官方博客中对语义主题的定义]主题就是一个概念、一个方面。它表现为一系列相关的词，能够代表这个主题。比如如果是”阿里巴巴“主题，那么”马云“”电子商务“等词会很高的频率出现，而设计到“腾讯”主题，那么“马化腾”“游戏”“QQ”会以较高的频率出现。如果用数学来描述一下的话，主题就是词汇表上词语的条件概率分布，与主题密切相关的词，条件概率p(w|z)越大。主题就像一个桶，装了出现频率很高的词语，这些词语和主题有很强的相关性，或者说这些词语定义了这个主题。同时，一个词语，可能来自于这个桶，也可能来自那个桶，比如“电子商务”可以来自“阿里巴巴”主题，也可以来自“京东“主题，所以一段文字往往包含多个主题，也就是说，一段文字不只有一个主题。</p>

<p>上面介绍了主题的概念，我们最为关心的是如何得到这些主题？这就是topic model要解决的问题。</p>

<p>define： d表示文档，w表示词语，z表示隐含的主题。</p>

<p>其中 p(w|d)表示w在文档d中出现的概率，针对训练语料，对文本进行分词，w的频度除以文档所有词语的频度和，可以求出，对于未知数据，model用来计算该value.</p>

<p>p(w|z)表示在给定主题情况下词语的出现的概率是多少，刻画词语和主题的相关程度。</p>

<p>p(z|d)表示文档中每个主题出现的概率</p>

<p>所以主题模型就是：利用大量已知的p(w|d)词语-文档信息，训练出来主题-文档p(z|d)以及词语-主题p(w|z)。</p>

<p><strong>plsa模型</strong>：</p>

<ul>
<li><p>plsa是一种topic model，它属于生成模型(不是很理解)，给定文档d后，以一定的概率选择d对应的主题z，然后以一定概率选择z中的词语w.</p></li>
<li><p>plsa提供了一种模型求解的方法，采用之前介绍的EM算法，EM算法在之前已经介绍，现在不作处理，直接利用EM信息对topic model进行求解。</p></li>
</ul>


<p><strong>主题模型的用途</strong>：</p>

<p>1.计算文本的相似性，考虑到文本语义，更好的刻画文本相似性，避免多义词，同义词的影响</p>

<p>2.文本聚类，用户聚类(RS)</p>

<p>3.去除噪音，只保留最重要的主题，更好的刻画文档</p>

<p><strong>plsa在推荐系统中的应用</strong>：</p>

<p>上面介绍的是文档和词语的关系，映射到推荐系统中，表示为用户和ITEM的关系，ITEM可以使网，视频等。</p>

<p>这样可以看出来描述的完全是同样的问题，求解p(s|u)=∑zp(s|z)p(z|u)，模型参数为p(s|z)?p(z|u)，里面上面的推导过程可以求得。</p>

<p>具体的可以参考：</p>

<p><em>Unsupervised learning by probabilisticlatent semantic analysis</em></p>

<p><em>Latent Semantic Models for collaborativefiltering</em></p>

<p><em>Google News Personalization Scalable OnlineCF</em></p>

<h5>二、LDA（潜在狄瑞雷克模型）</h5>

<p>　　和pLSA不同的是LDA中假设了很多先验分布（Dirichlet），且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同。</p>

                <nav class="pagination-link">
                    
                    <a class="prev" href="/2014/11/03/Theme1_matrix.html" rel="bookmark">&laquo;&nbsp;主题模型（一）：条件概率、矩阵分解</a>
                    
                    
                    <a class="next" href="/2014/11/05/cn_cutwords.html" rel="bookmark">中文分词：原理及分词算法&nbsp;&raquo;</a>
                    
                 </nav>
			</article><!-- .entry -->		
		</div>
		<div class="aside fn-clear">
			<div class="box fn-clear">
	<img src="/images/psb.jpg" alt="about theme"/>
	<!-- modify by dianacody-->
	<ul>
		<li><b>I'M DianaCody</b></li>
		<li><i>dianacodyleaf@gmail.com</i></li>
		<li>DianaCody, a postgraduate student from BUPT. 1/2 coder, 1/2 writer. Love coding, writing and open-source, also reading and traveling sometimes.</li>
		<li>Interest Fields：Search Engine, Data Mining, Machine Learning, NLP. </li>
	</ul>
	
</div>
			<div class="box fn-clear">
	<h3 class="title">GitHub Project</h3>
	<ul>
		<li>&#8227;<a href="https://github.com/DianaCody/RecommandSystem/">Personal Recommand System</a></li>
		<li>&#8227;<a href="https://github.com/DianaCody/Spider_SinaTweetCrawler_java/">Spider SinaWeibo_Crawler</a></li>
		<li>&#8227;<a href="https://github.com/DianaCody/Spider_python/">Tweet Crawler</a></li>
		<li>&#8227;<a href="https://github.com/DianaCody/Translator/">Translator Plugin</a></li>
	</ul>
</div>
			<div class="box fn-clear">
	<h3 class="title">CSDN Blog</h3>
	<ul>
		<li><a href="http://blog.csdn.net/dianacody">http://blog.csdn.net/dianacody</a></li>
	</ul>

	<!--
	<p>This is a jekyll blog theme.</p>
	-->	
</div>
		</div>
	</div>
</section>
    </article>
  </section>
    <footer class="foot">
	<div class="foot-cnt fn-clear">
		<div class="foot-cnt-det fn-clear">
			<article class="blogroll">
				<h4 class="title">Links</h4>
				<ul>
					<li>&#8227;<a href="https://github.com/dianacody">DianaCody's Github</a></li>
					<li>&#8227;<a href="http://blog.csdn.net/dianacody">My csdn Blog</a></li>
					<!-- <li>&#8227;<a href="http://www.jekyllrb.com">www.jekyllrb.com</a></li> -->
				</ul>
			</article>
			<article class="about">
				<h4 class="title">About Me</h4>
				<p>DianaCody, a postgraduate student from BUPT. 1/2 coder, 1/2 writer. Love coding, writing and open-source, also reading and traveling sometimes.</p>
				<p>Interest Fields：Search Engine, Data Mining, Machine Learning, NLP. </p>
			</article>
			<article class="follow">
				<h4 class="title">Follow Me</h4>
				<ul>
					<li><a href="https://github.com/dianacody/" class="ico-github" target="_blank">github</a></li>
					<li><a href="/atom.xml" class="ico-rss" target="_blank">rss</a></li>
					<li><a href="http://www.linkedin.com/" class="ico-linkedin">linkedin</a></li>
					<li><a href="http://www.google.com/" class="ico-google">google</a></li>
					<li><a href="http://www.flickr.com/" class="ico-flickr">flickr</a></li>
					<li><a href="http://www.twitter.com/dianacody/" class="ico-twitter">twitter</a></li>
					<li><a href="http://www.facebook.com/" class="ico-facebook">facebook</a></li>
					<li><a href="http://www.vimeo.com/" class="ico-vimeo">vimeo</a></li>
				</ul>
			</article>
		</div>
	</div>
	<div class="foot-btn">
		<div class="foot-btn-det">
			<p class="copyright">© 2013-2015 DianaCody. All rights reserved.</p>
			<p class="author">Powered by <a href="http://www.jekyllrb.com">Jekyll</a>. Design by <a href="http://dianacody.github.io">DianaCody</a>.</p>
		</div>
	</div>
</footer>
</body>
</html>