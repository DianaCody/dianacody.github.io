<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 
   <title>DianaCody's Shell</title>
   <link href="http://www.dianacody.com/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://www.dianacody.com" rel="alternate" type="text/html" />
   <updated>2015-11-06T20:11:37+08:00</updated>
   <id>http://www.dianacody.com</id>
   <author>
     <name>DianaCody</name>
     <email>dianacodyleaf@gmail.com</email>
   </author>

   
   <entry>
     <title>最大匹配算法扩展</title>
     <link href="http://www.dianacody.com/2014/11/16/HMM2.html"/>
     <updated>2014-11-16T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/11/16/HMM2</id>
     <content type="html">&lt;p&gt;基于简单的中文分词匹配法做了扩展，其中比较有名的就是台湾蔡志浩老师1996年写的“MMSEG: A Word Identification System for Mandarin Chinese Text Based on Two Variants of the Maximum Matching Algorithm”，在这篇文章的页面中，不仅介绍了相关的中文分词算法，并且提供了一个C版本的mmseg供研究使用，目前根据该文及其代码移植的mmseg程序版本包括C++版、Java版、Python版及Ruby版，影响甚广。&lt;/p&gt;

&lt;p&gt;此文是英文版本，建议有条件的读者直接读原文。不过国内也有该文的简介文章：《MMSeg分词算法简述》，原文似乎出自www.solol.org。&lt;/p&gt;

&lt;p&gt;MMSEG中文分词系统的可以由一句话总结：The system consisted of a lexicon, two matching algorithms, and four ambiguity resolution rules（该系统包括一个词典，两种匹配算法，以及四种歧义消解规则）：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1、词典（The Lexicon）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;分两种形式，对于单个汉字的汉语词，除了汉字本身外，还包括其统计频率（这个频率属于先验知识，可以来自于已经人工分好词的训练语料库），而对于二字长及以上的汉语词，只要词条本身就可以了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2、匹配算法（Matching Algorithm）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;a) 简单匹配:对于字符串中的汉字Cn，用词典匹配以Cn开头的子串并查找所有可能的匹配；&lt;/p&gt;

&lt;p&gt;b) 复杂匹配:对于字符串中的汉字Cn，查找所有可能以Cn开头的三词chunks，无论第一个汉语词是否有歧义。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3、歧义消解规则（Ambiguity Resolution Rules）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;规则一：最大匹配(Maximum matching)&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;a) 简单最大匹配算法,也就是我们常说的最大匹配法，不过作者采取的是正向匹配，并且按长度从小到大搜索词典：假设C1,C2,….代表一个字符串中的汉字，首先搜索词典，看 &lt;em&gt;C1&lt;/em&gt;是否为一个单字组成的词语，然后搜索 &lt;em&gt;C1C2&lt;/em&gt;来看是否为两个汉字组成的词语，以此类推，直至找到字典中最长的匹配。&lt;/p&gt;

&lt;p&gt;b) 复杂最大匹配算法,由Chen 和Liu（1992）提出，其核心的假设是：The most plausible segmentation is the three-word chunk with maximum length. 请注意three-word chunk，可以将其翻译为“三词语块”，这也是MMSEG中比较核心的一个概念，这个最大匹配规则考虑问题比较全面，在对句子中的某个词进行切分时，如果有歧义拿不定主意，就再向后展望两个汉语词，并且找出所有可能的“三词语块”。例如，对于如下的“三词语块”，请注意括号中是注明的语块长度（以汉语单字为基本单位）：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;C1&lt;/em&gt; &lt;em&gt;C2&lt;/em&gt; &lt;em&gt;C3C4&lt;/em&gt;（4）&lt;/li&gt;
&lt;li&gt;&lt;em&gt;C1C2&lt;/em&gt; &lt;em&gt;C3C4&lt;/em&gt; &lt;em&gt;C5&lt;/em&gt;（5）&lt;/li&gt;
&lt;li&gt;&lt;em&gt;C1C2&lt;/em&gt; &lt;em&gt;C3C4&lt;/em&gt; &lt;em&gt;C5C6&lt;/em&gt;（6）&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;最大长度的“三词语块”是第3个，所以其第一汉语词&lt;em&gt;C1C2&lt;/em&gt;将被作为正确的分词形式。以此类推，接下来我们从C3开始，找出所有可能的“三词语块”，重复上述规则，直到句子的最后一个词被划分。直观一点，对于以“眼”开头的如下5个“三词语块”,利用该规则，则“眼看”是正确的词语划分：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;眼看 就要 来了（6）&lt;/li&gt;
&lt;li&gt;眼看 就要 来（5）&lt;/li&gt;
&lt;li&gt;眼看 就 要(4)&lt;/li&gt;
&lt;li&gt;眼 看 就要(4)&lt;/li&gt;
&lt;li&gt;眼 看 就(3)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;规则二：最大平均词长（Largest average word length）&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在句子的末尾，很可能得到的“三词语块”只有一个或两个词（其他位置补空），例如，对于如下两个“三词语块”，他们拥有同样的长度：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;C1&lt;/em&gt; &lt;em&gt;C2&lt;/em&gt; &lt;em&gt;C3&lt;/em&gt;（平均词长=1）&lt;/li&gt;
&lt;li&gt;&lt;em&gt;C1C2C3&lt;/em&gt;（平均词长=3）&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;这时规则1就无法解决其歧义消解问题，因此引入规则2：最大平均词长，也就是从这些语块中找出平均词长最大的语块，并选取其第一词语作为正确的词语切分形式。这个规则的前提假设是：It is more likely to encounter multi-character words than one-character words（在句子中遇到多字-词语的情况比单字-词语更有可能）.&lt;/p&gt;

&lt;p&gt;因此，上述两个“三词语块”中第二个&lt;em&gt;C1C2C3&lt;/em&gt;就是最佳候选。直观一点，对于如下位于句尾三种形式的“三词语块”：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;国际化（平均词长=3）&lt;/li&gt;
&lt;li&gt;国际 化（平均词长=1.5）&lt;/li&gt;
&lt;li&gt;国 际 化（平均词长=1）&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;在规则1无法求解的情况下，根据规则2，则“国际化”为最佳候选语块，因此该语块的第一个词“国际化”就是最佳的分词形式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;规则三：最小词长方差（Smallest variance of word lengths）&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;还有一些歧义是规则一和规则二无法解决的，例如，如下的两个“三词语块”拥有同样的长度和同样的平均词长：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;C1C2&lt;/em&gt; &lt;em&gt;C3C4&lt;/em&gt; &lt;em&gt;C5C6&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;C1C2C3&lt;/em&gt; &lt;em&gt;C4&lt;/em&gt; &lt;em&gt;C5C6&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;因此引入规则三：最小词长方差，也就是找出词长方差最小的语块，并选取其第一个词语作为正确的词语切分形式。在概率论和统计学中，一个随机变量的方差（Variance）描述的是它的离散程度，也就是该变量离其期望值的距离。因此该规则的前提假设是：Word lengths are usually evenly distributed（句子中的词语长度经常是均匀分布的）。直观来说，对于如下两个“三词语块”：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;研究 生命 起源&lt;/li&gt;
&lt;li&gt;研究生 命 起源&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;其长度为6，平均词长为2，规则一和规则二无能无力，利用规则三：
* 语块1的方差 = ((2-2)&lt;sup&gt;2&lt;/sup&gt;+(2-2)&lt;sup&gt;2&lt;/sup&gt;+(2-2)&lt;sup&gt;2&lt;/sup&gt;)/3 = 0
* 语块2的方差 = ((3-2)&lt;sup&gt;2&lt;/sup&gt;+(1-2)&lt;sup&gt;2&lt;/sup&gt;+(2-2)&lt;sup&gt;2&lt;/sup&gt;)/3 = 2/3&lt;/p&gt;

&lt;p&gt;则语块1为最佳候选，因此该语块的第一个词“研究”为最佳的分词形式。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;规则四：最大单字词语语素自由度之和（Largest sum of degree of morphemic freedom of one-character words）&lt;/em&gt;&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;如下所示，例子中的两个“三词语块”拥有同样的长度、平均词长及方差，因此上述三个规则都无法解决其歧义消解问题：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;C1&lt;/em&gt; &lt;em&gt;C2&lt;/em&gt; &lt;em&gt;C3C4&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;C1&lt;/em&gt; &lt;em&gt;C2C3&lt;/em&gt; &lt;em&gt;C4&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;这两个语块都包括了两个单字（one-character）词语和一个两字（two-character）词语，规则四主要关注其中的单字词语。直观来看，有些汉字很少作为词语出现，而另一些汉字则常常作为词语出现，从统计角度来看，在语料库中出现频率高的汉字就很可能是一个单字词语，反之可能性就小。计算单词词语语素自由度之和的公式是对“三词语块”中的单字词语频率取对数并求和（The formula used to calculate the sum of degree of morphemic freedom is to sum log(frequency) of all one-character word(s) in a chunk.）规则四则选取其中和最大的语块，并将该语块的第一词语作为最佳的词语切分形式。&lt;/p&gt;

&lt;p&gt;关于MMSEG中文分词系统的框架就介绍到此，需要指出的是：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“It has to be noted that MMSEG was not designed to be a “professional level” system whose goal is 100% correct identification. Rather, MMSEG should be viewed as a general platform on which new ambiguity resolution algorithms can be tested.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;所以，不要认为有了MMSEG就可以解决中文分词的问题，更应该将MMSEG视为一个基本的平台，在该平台的基础上，可以尝试添加新的歧义消解算法以解决中文分词中的难点问题。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>中文分词-最大匹配算法</title>
     <link href="http://www.dianacody.com/2014/11/07/HMM.html"/>
     <updated>2014-11-07T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/11/07/HMM</id>
     <content type="html">&lt;p&gt;正向最大匹配法算法如下所示:&lt;/p&gt;

&lt;p&gt;逆向匹配法思想与正向一样，只是从右向左切分，这里举一个例子：&lt;/p&gt;

&lt;p&gt;输入例句：S1=”计算语言学课程有意思” ；&lt;/p&gt;

&lt;p&gt;定义：最大词长MaxLen = 5；S2= ” “；分隔符 = “/”；&lt;/p&gt;

&lt;p&gt;假设存在词表：…，计算语言学，课程，意思，…；&lt;/p&gt;

&lt;p&gt;最大逆向匹配分词算法过程如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;S2=””；S1不为空，从S1右边取出候选子串W=”课程有意思”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有意思”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”有意思”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”意思”&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，“意思”在词表中，将W加入到S2中，S2=” 意思/”，并将W从S1中去掉，此时S1=”计算语言学课程有”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;S1不为空，于是从S1左边取出候选子串W=”言学课程有”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程有”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程有”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”有”，这W是单字，将W加入到S2中，S2=“ /有 /意思”，并将W从S1中去掉，此时S1=”计算语言学课程”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;S1不为空，于是从S1左边取出候选子串W=”语言学课程”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”言学课程”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，“意思”在词表中，将W加入到S2中，S2=“ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=”计算语言学”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;S1不为空，于是从S1左边取出候选子串W=”计算语言学”；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;查词表，“计算语言学”在词表中，将W加入到S2中，S2=“计算语言学/ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=””；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;S1为空，输出S2作为分词结果，分词过程结束。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;相应程序示例：&lt;/p&gt;

&lt;p&gt;准备文件：建立一个词表文件wordlexicon，格式如下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算语言学&lt;/li&gt;
&lt;li&gt;课程&lt;/li&gt;
&lt;li&gt;意思&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;输入文件：test,格式如下&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算语言学课程有意思&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;编译后执行如下：SegWord.exe test，输出分词结果文件：SegmentResult.txt&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;源代码如下：&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// Dictionary.h
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;string&amp;gt;
#include &amp;lt;fstream&amp;gt;
#include &amp;lt;sstream&amp;gt;
#include &amp;lt;hash_map&amp;gt;
using namespace std;
using namespace stdext;
class CDictionary
{
    public:
        CDictionary(); //将词典文件读入并构造为一个哈希词典
        ~CDictionary();
        int FindWord(string w); //在哈希词典中查找词
    private:
        string strtmp; //读取词典的每一行
        string word; //保存每个词
        hash_map&amp;lt;string, int&amp;gt; wordhash; // 用于读取词典后的哈希
        hash_map&amp;lt;string, int &amp;gt;::iterator worditer; //
        typedef pair&amp;lt;string, int&amp;gt; sipair;
};
//将词典文件读入并构造为一个哈希词典
CDictionary::CDictionary()
{
    ifstream infile(“wordlexicon”); // 打开词典
    if (!infile.is_open()) // 打开词典失败则退出程序
    {
        cerr &amp;lt;&amp;lt; &quot;Unable to open input file: &quot; &amp;lt;&amp;lt; &quot;wordlexicon&quot;
&amp;lt;&amp;lt; &quot; -- bailing out!&quot; &amp;lt;&amp;lt; endl;
        exit(-1);
    }
    while (getline(infile, strtmp, &#39;\\n&#39;)) // 读入词典的每一行并将其添加入哈希中
    {
        istringstream istr(strtmp);
        istr &amp;gt;&amp;gt; word; //读入每行第一个词
        wordhash.insert(sipair(word, 1)); //插入到哈希中
    }
}

CDictionary::~CDictionary()
{
}

//在哈希词典中查找词，若找到，则返回，否则返回
int CDictionary::FindWord(string w)
{
    if (wordhash.find(w) != wordhash.end())
    {
        return 1;
    }
    else
    {
        return 0;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;主程序main.cpp&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#include “Dictionary.h”
# define MaxWordLength 10 // 最大词长为个字节（即个汉字）
# define Separator “/ ” // 词界标记
CDictionary WordDic; //初始化一个词典
//对字符串用最大匹配法（正向或逆向）处理
string SegmentSentence(string s1)
{
    string s2 = “”; //用s2存放分词结果
    while(!s1.empty())
    {
        int len =(int) s1.length(); // 取输入串长度
        if (len &amp;gt; MaxWordLength) // 如果输入串长度大于最大词长
        {
            len = MaxWordLength; // 只在最大词长范围内进行处理
        }
        //string w = s1.substr(0, len); // （正向用）将输入串左边等于最大词长长度串取出作为候选词
        string w = s1.substr(s1.length() – len, len); //逆向用
        int n = WordDic.FindWord(w); // 在词典中查找相应的词
        while(len &amp;gt; 2 &amp;amp;&amp;amp; n == 0) // 如果不是词
        {
            len -= 2; // 从候选词右边减掉一个汉字，将剩下的部分作为候选词
            //w = w.substr(0, len); //正向用
            w = s1.substr(s1.length() – len, len); //逆向用
            n = WordDic.FindWord(w);
        }
        //s2 += w + Separator; // (正向用）将匹配得到的词连同词界标记加到输出串末尾
        w = w + Separator; // (逆向用)
        s2 = w + s2 ; // (逆向用)
        //s1 = s1.substr(w.length(), s1.length()); //(正向用)从s1-w处开始
        s1 = s1.substr(0, s1.length() – len); // (逆向用)
    }
    return s2;
}
//对句子进行最大匹配法处理，包含对特殊字符的处理
string SegmentSentenceMM (string s1)
{
    string s2 = “”; //用s2存放分词结果
    int i;
    int dd;
    while(!s1.empty() )
    {
        unsigned char ch = (unsigned char)s1[0];
        if (ch &amp;lt; 128) // 处理西文字符
        {
            i = 1;
            dd = (int)s1.length();
            while (i &amp;lt; dd &amp;amp;&amp;amp; ((unsigned char)s1[i] &amp;lt; 128) &amp;amp;&amp;amp; (s1[i] != 10) &amp;amp;&amp;amp; (s1[i] != 13)) // s1[i]不能是换行符或回车符
            {
                i++;
            }
            if ((ch != 32) &amp;amp;&amp;amp; (ch != 10) &amp;amp;&amp;amp; (ch != 13)) // 如果不是西文空格或换行或回车符
            {
                s2 += s1.substr(0,i) + Separator;
            }
            else
            {
                //if (ch == 10 || ch == 13) // 如果是换行或回车符，将它拷贝给s2输出
                if (ch == 10 || ch == 13 || ch == 32) //谢谢读者mces89的指正
                {
                    s2 += s1.substr(0, i);
                }
            }
            s1 = s1.substr(i,dd);
            continue;
        }
        else
        {
            if (ch &amp;lt; 176) // 中文标点等非汉字字符
            {
                i = 0;
                dd = (int)s1.length();
                while(i &amp;lt; dd &amp;amp;&amp;amp; ((unsigned char)s1[i] &amp;lt; 176) &amp;amp;&amp;amp; ((unsigned char)s1[i] &amp;gt;= 161)
                &amp;amp;&amp;amp; (!((unsigned char)s1[i] == 161 &amp;amp;&amp;amp; ((unsigned char)s1[i+1] &amp;gt;= 162 &amp;amp;&amp;amp; (unsigned char)s1[i+1] &amp;lt;= 168)))
                &amp;amp;&amp;amp; (!((unsigned char)s1[i] == 161 &amp;amp;&amp;amp; ((unsigned char)s1[i+1] &amp;gt;= 171 &amp;amp;&amp;amp; (unsigned char)s1[i+1] &amp;lt;= 191)))
                &amp;amp;&amp;amp; (!((unsigned char)s1[i] == 163 &amp;amp;&amp;amp; ((unsigned char)s1[i+1] == 172 || (unsigned char)s1[i+1] == 161)
                || (unsigned char)s1[i+1] == 168 || (unsigned char)s1[i+1] == 169 || (unsigned char)s1[i+1] == 186
                || (unsigned char)s1[i+1] == 187 || (unsigned char)s1[i+1] == 191)))
            {
                i = i + 2; // 假定没有半个汉字
            }
            if (i == 0)
            {
                i = i + 2;
            }
            if (!(ch == 161 &amp;amp;&amp;amp; (unsigned char)s1[1] == 161)) // 不处理中文空格
            {
                s2+=s1.substr(0, i) + Separator; // 其他的非汉字双字节字符可能连续输出
            }
            s1 = s1.substr(i, dd);
            continue;
            }
        }
        // 以下处理汉字串
        i = 2;
        dd = (int)s1.length();
        while(i &amp;lt; dd &amp;amp;&amp;amp; (unsigned char)s1[i] &amp;gt;= 176)
        {
            i += 2;
        }
        s2 += SegmentSentence(s1.substr(0, i));
        s1 = s1.substr(i,dd);
    }
    return s2;
}

int main(int argc, char *argv[])
{
    string strtmp; //用于保存从语料库中读入的每一行
    string line; //用于输出每一行的结果
    ifstream infile(argv[1]); // 打开输入文件
    if (!infile.is_open()) // 打开输入文件失败则退出程序
    {
        cerr &amp;lt;&amp;lt; &quot;Unable to open input file: &quot; &amp;lt;&amp;lt; argv[1]
&amp;lt;&amp;lt; &quot; -- bailing out!&quot; &amp;lt;&amp;lt; endl;
        exit(-1);
}
    ofstream outfile1(&quot;SegmentResult.txt&quot;); //确定输出文件
    if (!outfile1.is_open())
    {
        cerr &amp;lt;&amp;lt; &quot;Unable to open file：SegmentResult.txt&quot;
&amp;lt;&amp;lt; &quot;--bailing out!&quot; &amp;lt;&amp;lt; endl;
        exit(-1);
    }
    while (getline(infile, strtmp, &#39;n&#39;)) //读入语料库中的每一行并用最大匹配法处理
    {
        line = strtmp;
        line = SegmentSentenceMM(line); // 调用分词函数进行分词处理
        outfile1 &amp;lt;&amp;lt; line &amp;lt;&amp;lt; endl; // 将分词结果写入目标文件
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;补充说明：如果使用正向匹配法，请将源代码中的相关注释 “//&quot;互换。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>中文分词：原理及分词算法</title>
     <link href="http://www.dianacody.com/2014/11/05/cn_cutwords.html"/>
     <updated>2014-11-05T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/11/05/cn_cutwords</id>
     <content type="html">&lt;h5&gt;一、中文分词&lt;/h5&gt;

&lt;p&gt;词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文信息处理的基础与关键。&lt;/p&gt;

&lt;p&gt;Lucene中对中文的处理是基于自动切分的单字切分，或者二元切分。除此之外，还有最大切分（包括向前、向后、以及前后相结合）、最少切分、全切分等等。&lt;/p&gt;

&lt;h5&gt;二、中文分词技术分类&lt;/h5&gt;

&lt;p&gt;我们讨论的分词算法可分为三大类：&lt;/p&gt;

&lt;p&gt;1.&lt;strong&gt;基于词典&lt;/strong&gt;：基于字典、词库匹配的分词方法；（字符串匹配、机械分词法）&lt;/p&gt;

&lt;p&gt;2.&lt;strong&gt;基于统计&lt;/strong&gt;：基于词频度统计的分词方法；&lt;/p&gt;

&lt;p&gt;3.&lt;strong&gt;基于规则&lt;/strong&gt;：基于知识理解的分词方法。&lt;/p&gt;

&lt;p&gt;第一类方法应用词典匹配、汉语词法或其它汉语语言知识进行分词，如：最大匹配法、最小分词方法等。这类方法简单、分词效率较高,但汉语语言现象复杂丰富，词典的完备性、规则的一致性等问题使其难以适应开放的大规模文本的分词处理。第二类基于统计的分词方法则基于字和词的统计信息，如把相邻字间的信息、词频及相应的共现信息等应用于分词，由于这些信息是通过调查真实语料而取得的，因而基于统计的分词方法具有较好的实用性。&lt;/p&gt;

&lt;p&gt;下面简要介绍几种常用方法：&lt;/p&gt;

&lt;h5&gt;三、基于词典分词&lt;/h5&gt;

&lt;h6&gt;0. 逐词遍历法&lt;/h6&gt;

&lt;p&gt;逐词遍历法将词典中的所有词按由长到短的顺序在文章中逐字搜索,直至文章结束。也就是说，不管文章有多短，词典有多大，都要将词典遍历一遍。这种方法效率比较低，大一点的系统一般都不使用。&lt;/p&gt;

&lt;h6&gt;1. 基于字典、词库匹配的分词方法（机械分词法）&lt;/h6&gt;

&lt;p&gt;这种方法按照一定策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。识别出一个词，根据扫描方向的不同分为正向匹配和逆向匹配。根据不同长度优先匹配的情况，分为最大（最长）匹配和最小（最短）匹配。根据与词性标注过程是否相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的方法如下：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(1)最大正向匹配法(MM, MaximumMatching Method)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;通常简称为MM法。其基本思想为：假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理……  如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。&lt;/p&gt;

&lt;p&gt;其算法描述如下：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;step1&lt;/strong&gt;: 从左向右取待切分汉语句的m个字符作为匹配字段，m为大机器词典中最长词条个数。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;step2&lt;/strong&gt;: 查找大机器词典并进行匹配。若匹配成功，则将这个匹配字段作为一个词切分出来。若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配，重复以上过程，直到切分出所有词为止。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(2)逆向最大匹配法(ReverseMaximum Matching Method)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;通常简称为&lt;strong&gt;RMM法&lt;/strong&gt;。RMM法的基本原理与MM法相同 ,不同的是分词切分的方向与MM法相反，而且使用的分词辞典也不同。逆向最大匹配法从被处理文档的末端开始匹配扫描，每次取最末端的2i个字符（i字字串）作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。相应地，它使用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行倒排处理，生成逆序文档。然后，根据逆序词典，对逆序文档用正向最大匹配法处理即可。&lt;/p&gt;

&lt;p&gt;由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。所以，逆向最大匹配法比正向最大匹配法的误差要小。统计结果表明 ,单纯使用正向最大匹配的错误率为 1/16 9,单纯使用逆向最大匹配的错误率为 1/245。例如切分字段“硕士研究生产”，正向最大匹配法的结果会是“硕士研究生 / 产”，而逆向最大匹配法利用逆向扫描，可得到正确的分词结果“硕士 / 研究 / 生产”。&lt;/p&gt;

&lt;p&gt;当然，最大匹配算法是一种基于分词词典的机械分词法，不能根据文档上下文的语义特征来切分词语，对词典的依赖性较大，所以在实际使用时，难免会造成一些分词错误，为了提高系统分词的准确度，可以采用正向最大匹配法和逆向最大匹配法相结合的分词方案（见“双向匹配法”）&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(3)最少切分法&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;使每一句中切出的词数最小。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(4)双向匹配法&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;将正向最大匹配法与逆向最大匹配法组合。先根据标点对文档进行粗切分，把文档分解成若干个句子，然后再对这些句子用正向最大匹配法和逆向最大匹配法进行扫描切分。如果两种分词方法得到的匹配结果相同，则认为分词正确，否则，按最小集处理。&lt;/p&gt;

&lt;h5&gt;四、 全切分、基于词的频度统计（无字典分词）&lt;/h5&gt;

&lt;p&gt;基于词的频度统计的分词方法是一种全切分方法。在讨论这个方法之前我们先要明白有关全切分的相关内容。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(1) 全切分&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;全切分要求获得输入序列的所有可接受的切分形式，而部分切分只取得一种或几种可接受的切分形式，由于部分切分忽略了可能的其他切分形式，所以建立在部分切分基础上的分词方法不管采取何种歧义纠正策略，都可能会遗漏正确的切分，造成分词错误或失败。而建立在全切分基础上的分词方法，由于全切分取得了所有可能的切分形式，因而从根本上避免了可能切分形式的遗漏，克服了部分切分方法的缺陷。&lt;/p&gt;

&lt;p&gt;全切分算法能取得所有可能的切分形式，它的句子覆盖率和分词覆盖率均为100%，但全切分分词并没有在文本处理中广泛地采用，原因有以下几点：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;全切分算法只是能获得正确分词的前提，因为全切分不具有歧义检测功能，最终分词结果的正确性和完全性依赖于独立的歧义处理方法，如果评测有误，也会造成错误的结果。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;全切分的切分结果个数随句子长度的增长呈指数增长，一方面将导致庞大的无用数据充斥于存储数据库；另一方面当句长达到一定长度后，由于切分形式过多,造成分词效率严重下降。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;em&gt;(2) 基于词的频度统计的分词方法&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;主要思想：上下文中，相邻的字同时出现的次数越多，就越可能构成一个词。因此字与字相邻出现的概率或频率能较好的反映词的可信度。&lt;/p&gt;

&lt;p&gt;主要统计模型为：&lt;strong&gt;N元文法模型（N-gram）、隐马尔科夫模型(Hidden Markov Model, HMM)&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;HMM马尔科夫假设&lt;/strong&gt;：一个词的出现仅仅依赖于它前面出现的有限的一个或者几个词。如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram。即&lt;code class=&quot;code&quot;&gt;P(T) = P(W1W2W3…Wn) = P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1) ≈ P(W1)P(W2|W1)P(W3|W2)…P(Wn|Wn-1)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram。&lt;/p&gt;

&lt;p&gt;在实践中用的最多的就是bigram和trigram了，而且效果很不错。高于四元的用的很少，因为训练它需要更庞大的语料，而且数据稀疏严重，时间复杂度高，精度却提高的不多。&lt;/p&gt;

&lt;p&gt;设w1,w2,w3,...,wn是长度为n的字符串，规定任意词wi只与它的前两个相关，得到三元概率模型。&lt;/p&gt;

&lt;p&gt;以此类推，N元模型就是假设当前词的出现概率只同它前面的N-1个词有关。&lt;/p&gt;

&lt;p&gt;这是一种全切分方法。它不依靠词典,而是将文章中任意两个字同时出现的频率进行统计,次数越高的就可能是一个词。它首先切分出与词表匹配的所有可能的词,运用统计语言模型和决策算法决定最优的切分结果。它的优点在于可以发现所有的切分歧义并且容易将新词提取出来。&lt;/p&gt;

&lt;h5&gt;五、基于统计分词（无字典分词）&lt;/h5&gt;

&lt;p&gt;该方法主要基于句法、语法分析，并结合语义分析，通过对上下文内容所提供信息的分析对词进行定界，它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断。这类方法试图让机器具有人类的理解能力，需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式。因此目前基于知识的分词系统还处在试验阶段。&lt;/p&gt;

&lt;h5&gt;六、并行分词法&lt;/h5&gt;

&lt;p&gt;这种分词方法借助于一个含有分词词库的管道进行 ,比较匹配过程是分步进行的 ,每一步可以对进入管道中的词同时与词库中相应的词进行比较 ,由于同时有多个词进行比较匹配 ,因而分词速度可以大幅度提高。这种方法涉及到多级内码理论和管道的词典数据结构。（详细算法可以参考吴胜远的《并行分词方法的研究》）&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>主题模型（二）：pLSA和LDA</title>
     <link href="http://www.dianacody.com/2014/11/04/Theme2_pLSA,%20LDA.html"/>
     <updated>2014-11-04T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/11/04/Theme2_pLSA, LDA</id>
     <content type="html">&lt;h5&gt;一、pLSA（概率潜在语义分析）&lt;/h5&gt;

&lt;p&gt;pLSA:    &lt;em&gt;-------有过拟合问题，就是求D, Z, W&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。&lt;/p&gt;

&lt;p&gt;在论文《GoogleNews Personalization Scalable Online CF》一文中提级针对用户聚类，利用相似用户性信息计算喜欢的news。其中包含min-hash以及plsi，plsi是model-based 推荐算法，属于topic(aspect) model，其在NLP领域中用途很大。&lt;/p&gt;

&lt;p&gt;引入：在文本挖掘时，计算文档相似性是很基础的操作，通常，对文本进行分词，构建VSM，通过jaccard或者cosin计算距离或者相似性，这是基于corpus的思路，仅仅考虑词组，并未考虑文本的语义信息。针对下面情况，基于cropus很难处理：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果时间回到2006年，马云和杨致远的手还会握在一起吗&lt;/li&gt;
&lt;li&gt;阿里巴巴集团和雅虎就股权回购一事签署了最终协议&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;如果采用基于corpus的jaccard距离等算法，那么这两个文本的完全不相关，但是事实上，马云和阿里巴巴集团，杨致远和雅虎有着密切的联系，从语义上看，两者都和“阿里巴巴&quot;有关系。&lt;/p&gt;

&lt;p&gt;此外，另一个case：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;富士苹果真好，赶快买&lt;/li&gt;
&lt;li&gt;苹果四代真好，赶快买&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;从corpus上来看，两者非常相似，但是事实上，2个句子从语义上来讲，没有任何关系，一个是”水果“另一个是”手机&quot;。&lt;/p&gt;

&lt;p&gt;通过上面的例子，差不多也看出来topic model是什么以及解决什么问题。&lt;/p&gt;

&lt;p&gt;概念：topic model是针对文本隐含主题的建模方法，针对第一个case，马云对应的主题是阿里巴巴，阿里巴巴集团也隐含阿里巴巴主题，这样两个文本的主题匹配上，认为他们是相关的，针对第二个，分别针对水果以及手机主题，我们认为他们是不相关的。&lt;/p&gt;

&lt;p&gt;究竟什么是主题？[接下来参考baidu搜索研发部官方博客中对语义主题的定义]主题就是一个概念、一个方面。它表现为一系列相关的词，能够代表这个主题。比如如果是”阿里巴巴“主题，那么”马云“”电子商务“等词会很高的频率出现，而设计到“腾讯”主题，那么“马化腾”“游戏”“QQ”会以较高的频率出现。如果用数学来描述一下的话，主题就是词汇表上词语的条件概率分布，与主题密切相关的词，条件概率p(w|z)越大。主题就像一个桶，装了出现频率很高的词语，这些词语和主题有很强的相关性，或者说这些词语定义了这个主题。同时，一个词语，可能来自于这个桶，也可能来自那个桶，比如“电子商务”可以来自“阿里巴巴”主题，也可以来自“京东“主题，所以一段文字往往包含多个主题，也就是说，一段文字不只有一个主题。&lt;/p&gt;

&lt;p&gt;上面介绍了主题的概念，我们最为关心的是如何得到这些主题？这就是topic model要解决的问题。&lt;/p&gt;

&lt;p&gt;define： d表示文档，w表示词语，z表示隐含的主题。&lt;/p&gt;

&lt;p&gt;其中 p(w|d)表示w在文档d中出现的概率，针对训练语料，对文本进行分词，w的频度除以文档所有词语的频度和，可以求出，对于未知数据，model用来计算该value.&lt;/p&gt;

&lt;p&gt;p(w|z)表示在给定主题情况下词语的出现的概率是多少，刻画词语和主题的相关程度。&lt;/p&gt;

&lt;p&gt;p(z|d)表示文档中每个主题出现的概率&lt;/p&gt;

&lt;p&gt;所以主题模型就是：利用大量已知的p(w|d)词语-文档信息，训练出来主题-文档p(z|d)以及词语-主题p(w|z)。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;plsa模型&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;plsa是一种topic model，它属于生成模型(不是很理解)，给定文档d后，以一定的概率选择d对应的主题z，然后以一定概率选择z中的词语w.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;plsa提供了一种模型求解的方法，采用之前介绍的EM算法，EM算法在之前已经介绍，现在不作处理，直接利用EM信息对topic model进行求解。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;主题模型的用途&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;1.计算文本的相似性，考虑到文本语义，更好的刻画文本相似性，避免多义词，同义词的影响&lt;/p&gt;

&lt;p&gt;2.文本聚类，用户聚类(RS)&lt;/p&gt;

&lt;p&gt;3.去除噪音，只保留最重要的主题，更好的刻画文档&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;plsa在推荐系统中的应用&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;上面介绍的是文档和词语的关系，映射到推荐系统中，表示为用户和ITEM的关系，ITEM可以使网，视频等。&lt;/p&gt;

&lt;p&gt;这样可以看出来描述的完全是同样的问题，求解p(s|u)=∑zp(s|z)p(z|u)，模型参数为p(s|z)?p(z|u)，里面上面的推导过程可以求得。&lt;/p&gt;

&lt;p&gt;具体的可以参考：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Unsupervised learning by probabilisticlatent semantic analysis&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Latent Semantic Models for collaborativefiltering&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Google News Personalization Scalable OnlineCF&lt;/em&gt;&lt;/p&gt;

&lt;h5&gt;二、LDA（潜在狄瑞雷克模型）&lt;/h5&gt;

&lt;p&gt;　　和pLSA不同的是LDA中假设了很多先验分布（Dirichlet），且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>主题模型（一）：条件概率、矩阵分解</title>
     <link href="http://www.dianacody.com/2014/11/03/Theme1_matrix.html"/>
     <updated>2014-11-03T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/11/03/Theme1_matrix</id>
     <content type="html">&lt;p&gt;&lt;strong&gt;主题模型&lt;/strong&gt;训练推理方法主要有2种：&lt;/p&gt;

&lt;p&gt;(1) pLSA→EM（期望最大化）&lt;/p&gt;

&lt;p&gt;(2) LDA → Gibbs Sampling抽样方法（计算量大,单精确）、变分贝叶斯推断法（计算量小,精度弱）&lt;/p&gt;

&lt;p&gt;概率矩阵：&lt;strong&gt;p(词语|文档) =∑p(词语|主题)× p(主题|文档)&lt;/strong&gt;
                          &lt;strong&gt; C       =         Φ      ×       Θ&lt;/strong&gt;
在EM（最大期望）过程中：&lt;/p&gt;

&lt;p&gt;(1) E过程：由贝叶斯可从Φ算到Θ&lt;/p&gt;

&lt;p&gt;(2) M过程：由贝叶斯可从Θ算到Φ&lt;/p&gt;

&lt;p&gt;两者迭代，最终收敛（矩阵趋于均分）&lt;/p&gt;

&lt;p&gt;设有两个句子，想知道它们之间是否相关联：&lt;/p&gt;

&lt;p&gt;第一个是：“乔布斯离我们而去了。”&lt;/p&gt;

&lt;p&gt;第二个是：“苹果价格会不会降？”&lt;/p&gt;

&lt;p&gt;如果由人来判断，一看就知道，这两个句子之间虽然没有任何公共词语，但仍然是很相关的。因为虽然第二句中的“苹果”可能是指吃的苹果，但是由于第一句里面有了“乔布斯”，我们会很自然的把“苹果”理解为苹果公司的产品。事实上，这种文字语句之间的相关性、相似性问题在搜索引擎算法中经常遇到。例如，一个用户输入了一个query，我们要从海量的网页库中找出和它最相关的结果。这里就涉及到如何衡量query和网页之间相似度的问题。对于这类问题，人是可以通过上下文语境来判断的。但是，机器可以么？&lt;/p&gt;

&lt;p&gt;在传统信息检索领域里，实际上已经有了很多衡量文档相似性的方法，比如经典的VSM模型。然而这些方法往往基于一个基本假设：文档之间重复的词语越多越可能相似。这一点在实际中并不尽然。很多时候相关程度取决于背后的语义联系，而非表面的词语重复。&lt;/p&gt;

&lt;p&gt;那么，这种语义关系应该怎样度量呢？事实上在自然语言处理领域里已经有了很多从词、词组、句子、篇章角度进行衡量的方法。本文要介绍的是其中一个语义挖掘的利器：&lt;strong&gt;主题模型&lt;/strong&gt;。&lt;/p&gt;

&lt;h5&gt;一、主题模型定义&lt;/h5&gt;

&lt;p&gt;主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。还是上面的例子，“苹果”这个词的背后既包含是苹果公司这样一个主题，也包括了水果的主题。当我们和第一句进行比较时，苹果公司这个主题就和“乔布斯”所代表的主题匹配上了，因而我们认为它们是相关的。&lt;/p&gt;

&lt;p&gt;关于主题定义：主题就是一个概念、一个方面。它表现为一系列相关的词语。比如一个文章如果涉及到“百度”这个主题，那么“中文搜索”、“李彦宏”等词语就会以较高的频率出现，而如果涉及到“IBM”这个主题，那么“笔记本”等就会出现的很频繁。如果用数学来描述一下的话，主题就是词汇表上词语的条件概率分布 。与主题关系越密切的词语，它的条件概率越大，反之则越小。&lt;/p&gt;

&lt;p&gt;以上是从互联网新闻中摘抄下来的一段话。划分了4个桶（主题），百度（红色），微软（紫色）、谷歌（蓝色）和市场（绿色）。段落中所包含的每个主题的词语用颜色标识出来。从颜色分布上我们就可以看出，文字的大意是在讲百度和市场发展。在这里面，谷歌、微软这两个主题也出现了，但不是主要语义。值得注意的是，像“搜索引擎”这样的词语，在百度、微软、谷歌这三个主题上都是很可能出现的，可以认为一个词语放进了多个“桶”。当它在文字中出现的时候，这三个主题均有一定程度的体现。&lt;/p&gt;

&lt;p&gt;如何得到这些主题？对文章中的主题又是如何进行分析？这正是主题模型要解决的问题。主题模型如何工作？&lt;/p&gt;

&lt;h5&gt;二、主题模型工作原理&lt;/h5&gt;

&lt;p&gt;首先，用生成模型的视角来看文档和主题这两件事。所谓&lt;strong&gt;生成模型&lt;/strong&gt;就是认为一篇文章的每个词都是通过“&lt;strong&gt;以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语&lt;/strong&gt;”这样一个过程得到的。
假如有很多的文档，比如大量的网页，先对所有文档进行分词，得到一个词汇列表。这样每篇文档就可以表示为一个词语的集合。对于每个词语，可以用它在文档中出现的次数除以文档中词语的数目作为它在文档中出现的概率p(词语|文档) 。这样对任意一篇文档，左边的C矩阵是已知的，右边的两个矩阵未知。&lt;strong&gt;而主题模型就是用大量已知的“词语－文档”C矩阵 ，通过一系列的训练，推理出右边的“词语－主题”矩阵Φ 和“主题文档”矩阵Θ &lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;主题模型训练推理的方法主要有两种，一个是pLSA（Probabilistic Latent Semantic Analysis），另一个是LDA（Latent Dirichlet Allocation）。pLSA主要使用的是EM（期望最大化）算法；LDA采用的是Gibbssampling方法。由于它们都较为复杂且篇幅有限，这里就只简要地介绍一下pLSA的思想，其他具体方法和公式，读者可以查阅相关资料。&lt;/p&gt;

&lt;p&gt;pLSA采用的方法叫做EM（期望最大化）算法，它包含两个不断迭代的过程：E（期望）过程和M（最大化）过程。用一个形象的例子来说吧：比如说食堂的大师傅炒了一盘菜，要等分成两份给两个人吃，显然没有必要拿天平去一点点去精确称量，最简单的办法是先随意的把菜分到两个碗中，然后观察是否一样多，把比较多的那一份取出一点放到另一个碗中，这个过程一直重复下去，直到大家看不出两个碗里的菜有什么差别为止。&lt;/p&gt;

&lt;p&gt;对于主题模型训练来说，“计算每个主题里的词语分布”和“计算训练文档中的主题分布”就好比是在往两个人碗里分饭。在E过程中，我们通过贝叶斯公式可以由“词语－主题”矩阵计算出“主题－文档”矩阵。在M过程中，我们再用“主题－文档”矩阵重新计算“词语－主题”矩阵。这个过程一直这样迭代下去。EM算法的神奇之处就在于它可以保证这个迭代过程是收敛的。也就是说，我们在反复迭代之后，就一定可以得到趋向于真实值的Φ和Θ。&lt;/p&gt;

&lt;h5&gt;三、主题模型应用&lt;/h5&gt;

&lt;p&gt;有了主题模型，如何使用？以及优缺点？主要是以下几点：&lt;/p&gt;

&lt;p&gt;(1) 可以衡量文档之间的语义相似性。对于一篇文档，我们求出来的主题分布可以看作是对它的一个抽象表示。对于概率分布，我们可以通过一些距离公式（比如KL距离）来计算出两篇文档的语义距离，从而得到它们之间的相似度。&lt;/p&gt;

&lt;p&gt;(2) 可以解决多义词的问题。回想最开始的例子，“苹果”可能是水果，也可能指苹果公司。通过求出来的“词语－主题”概率分布，就可以知道“苹果”都属于哪些主题，就可以通过主题的匹配来计算它与其他文字之间的相似度。&lt;/p&gt;

&lt;p&gt;(3) 可以排除文档中噪音的影响。一般来说，文档中的噪音往往处于次要主题中，我们可以把它们忽略掉，只保持文档中最主要的主题。&lt;/p&gt;

&lt;p&gt;(4) 它是无监督、完全自动化。只需要提供训练文档，它就可以自动训练出各种概率，无需任何人工标注过程。&lt;/p&gt;

&lt;p&gt;(5) 跟语言无关。任何语言只要能够对它进行分词，就可以进行训练，得到它的主题分布。&lt;/p&gt;

&lt;p&gt;综上所述，主题模型是一个能够挖掘语言背后隐含信息的利器。近些年来各大搜索引擎公司都已经开始重视这方面的研发工作。语义分析的技术正在逐步深入到搜索领域的各个产品中去。以后的搜索会趋于更加智能化。&lt;/p&gt;

&lt;h5&gt;四、LSA（潜在语义分析）&lt;/h5&gt;

&lt;p&gt; 鉴于TF-IDF存在一些缺点，Deerwester等人于1990年提出潜在语义分析（LatentSemanticAnalysis）模型，用于挖掘文档与词语之间隐含的潜在语义关联。LSA的理论基础是数学中的奇异值矩阵分解（SVD）技术。&lt;/p&gt;

&lt;p&gt; &lt;strong&gt;PLSA（基于概率的潜在语义分析）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt; 鉴于LSA存在一些缺点，Hofmann等人于1999年提出一种基于概率的潜在语义分析（Probabilistic Latent SemanticAnalysis）模型。PLSA继承了“潜在语义”的概念，通过“统一的潜在语义空间”（也就是Blei等人于2003年正式提出Topic概念）来关联词与文档；通过引入概率统计的思想，避免了SVD的复杂计算。在PLSA中，各个因素（文档、潜在语义空间、词）之间的概率分布求解是最重要的，EM算法是常用的方法。PLSA也存在一些缺点：概率模型不够完备；随着文档和词的个数的增加，模型变得越来越庞大；在文档层面没有一个统计模型；EM算法需要反复迭代，计算量也很大。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LDA（潜在狄瑞雷克模型）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;鉴于PLSA的缺点，Blei等人于2003年进一步提出新的主题模型LDA（LatentDirichletAllocation），它是一个层次贝叶斯模型，把模型的参数也看作随机变量，从而可以引入控制参数的参数，实现彻底的“概率化”。&lt;/p&gt;

&lt;p&gt;是LDA模型的Dirichlet的先验分布，表示整个文档集上主题的分布；表示文档d上主题的多项式分布；Z表示文档d的第n个词的主题；W表示文档d的第n个词；N表示文档d所包含词的个数；D表示文档集；K表示主题集；表示主题k上词语的多项式分布；表示所有主题上次的先验分布。事实上，去掉和 ，LDA就变成了PLSA。目前，参数估计是LDA最重要的任务，主要有两种方法：Gibbs抽样法（计算量大，但相对简单和精确）和变分贝叶斯推断法（计算量小，精度度弱）。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;其他基于topic model的演变&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;a）&lt;strong&gt;考虑上下文信息&lt;/strong&gt;：例如，“上下文相关的概率潜在语义分析模型（ContextualProbabilistic LatentSemantic Analysis，CPLSA）”将词语上下文信息引入PLSA；也有研究人员考虑“地理位置”上下文信息，从地理位置相关的文档中发现地理位置关联的Topic。&lt;/p&gt;

&lt;p&gt;b）&lt;strong&gt;主题模型演化&lt;/strong&gt;：引入文本语料的时间信息，研究主题随时间的演化，例如DTM、CTDTM、DMM、OLDA等模型。&lt;/p&gt;

&lt;p&gt;c）&lt;strong&gt;并行主题模型&lt;/strong&gt;：在大规模数据处理的需求下，基于并行计算的主题模型也开始得到关注。现有的解决方案有：Mallet、GPU-LDA、Async-LDA、N.C.L、pLDA、Y!LDA、Mahout、Mr.LDA等；其中pLDA、Y!LDA、Mahout、Mr.LDA等基于Hadoop/MapReduce框架，其他方案则基于传统的并行编程模型；参数估算方面，Mallet、Async-LDA、pLDA、Y!LDA等使用Gibbs抽样方法，Mr.LDA、Mahout、N.C.L等使用变分贝叶斯推断法，GPU-LDA同时支持两种方法。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>文本分类：朴素贝叶斯Bayes</title>
     <link href="http://www.dianacody.com/2014/11/02/Bayes.html"/>
     <updated>2014-11-02T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/11/02/Bayes</id>
     <content type="html">&lt;h5&gt;一、贝叶斯定理&lt;/h5&gt;

&lt;p&gt;贝叶斯公式思想：利用已知值来估计未知概率。已知某条件概率，如何得到两个事件交换后的概率，也就是已知P(A|B)的情况下如何求得P(B|A)。      &lt;br/&gt;
条件概率：P(A|B)表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。&lt;/p&gt;

&lt;h5&gt;二、贝叶斯原理、流程&lt;/h5&gt;

&lt;p&gt;朴素贝叶斯思想基础：对于待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗地讲，好比你在街上看到一个黑人，我问你他是从哪里来的，你十有八九会说从非洲。为什么呢？因为黑人中非洲人比例最高，当然别人也有可能是美洲人或者拉丁人，但在没有其他可用信息下，我们会选择条件概率最大类别，这就是朴素贝叶斯思想基础。&lt;/p&gt;

&lt;p&gt;贝叶斯分类的定义：&lt;/p&gt;

&lt;p&gt;1.设x={a1, a2, …,am}为一个待分类项，而每个a为x的一个特征属性；&lt;/p&gt;

&lt;p&gt;2.有类别集合C = {y1,y2, …, yn}&lt;/p&gt;

&lt;p&gt;3.计算P(y1|x), P(y2|x),…, P(yn|x)&lt;/p&gt;

&lt;p&gt;4.如果P(yk|x)= max{ P(y1|x), P(y2|x), …, P(yn|x) }，则x∈yk.&lt;/p&gt;

&lt;p&gt;关键是如何计算第3步中的&lt;strong&gt;各个条件概率&lt;/strong&gt;。步骤：&lt;/p&gt;

&lt;p&gt;1.找到一个&lt;strong&gt;已知&lt;/strong&gt;分类的待分类项集合，这个集合叫做训练样本集。&lt;/p&gt;

&lt;p&gt;2.统计得到在各类别下各个特征属性的条件概率估计。&lt;/p&gt;

&lt;p&gt;整个朴素贝叶斯分类分为三个阶段：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;第一阶段——准备工作阶段&lt;/em&gt;，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;第二阶段——分类器训练阶段&lt;/em&gt;，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;第三阶段——应用阶段&lt;/em&gt;。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成&lt;/p&gt;

&lt;h5&gt;三、注意问题&lt;/h5&gt;

&lt;p&gt;1.如果给出的特征向量长度可能不同，这是需要归一化为同长度向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。&lt;/p&gt;

&lt;p&gt;2.计算公式中其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是P(x|yi)的计算方法，而朴素贝叶斯的前提假设“独立性”可知，P(x0, x1, x2, …, xn|yi) = p(x0|yi)&lt;em&gt;p(x1|yi)&lt;/em&gt;p(x2|yi)…p(xn|yi)，因此一般有两种，一种是在类别为yi的那些样本集里，找到xj出现次数的总和，然后除以该样本的总和；第二种方法是类别为yi的那些样本集里，找到xj出现次数的总和，然后除以该样本中所有特征出现次数的总和。&lt;/p&gt;

&lt;p&gt;3.如果p(x|yi)中的某一项为0，则其联合概率乘积也可能为0，即2中公式分子为0，为了避免这种现象出现，一般情况下会将这一项初始化为1，当然为了保证概率相等，分母应该对应初始化为2（这里因为是2类，所以加2，如果是k类就需要加k，术语上叫&lt;strong&gt;Laplace光滑&lt;/strong&gt;）。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>迭代决策树GBRT（渐进梯度回归树）</title>
     <link href="http://www.dianacody.com/2014/11/01/GBRT.html"/>
     <updated>2014-11-01T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/11/01/GBRT</id>
     <content type="html">&lt;h5&gt;一、决策树模型组合&lt;/h5&gt;

&lt;p&gt;单决策树C4.5由于功能太简单，并且非常容易出现过拟合的现象，于是引申出了许多变种决策树，就是将单决策树进行&lt;strong&gt;模型组合&lt;/strong&gt;，形成多决策树，比较典型的就是迭代决策树GBRT和随机森林RF。
在最近几年的paper上，如iccv这种重量级会议，iccv 09年的里面有不少文章都是与Boosting和随机森林相关的。模型组合+决策树相关算法有两种比较基本的形式：随机森林RF与GBDT，其他比较新的模型组合+决策树算法都是来自这两种算法的延伸。
&lt;strong&gt;核心思想：其实很多“渐进梯度” Gradient Boost都只是一个框架，里面可以套用很多不同的算法。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先说明一下，GBRT这个算法有很多名字，但都是同一个算法：
GBRT (Gradient BoostRegression Tree) 渐进梯度回归树
GBDT (Gradient BoostDecision Tree) 渐进梯度决策树
MART (MultipleAdditive Regression Tree) 多决策回归树
Tree Net决策树网络&lt;/p&gt;

&lt;h5&gt;二、GBRT&lt;/h5&gt;

&lt;p&gt;迭代决策树算法，在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），由多棵决策树组成，所有树的输出结果累加起来就是最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。&lt;/p&gt;

&lt;p&gt;GBRT是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。为了防止过拟合，和Adaboosting一样，也加入了boosting这一项。&lt;/p&gt;

&lt;p&gt;提起决策树（DT, DecisionTree）不要只想到C4.5单分类决策树，GBRT不是&lt;strong&gt;分类树&lt;/strong&gt;而是&lt;strong&gt;回归树&lt;/strong&gt;！
决策树分为&lt;strong&gt;回归树&lt;/strong&gt;和&lt;strong&gt;分类树&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;回归树&lt;/strong&gt;用于预测实数值，如明天温度、用户年龄&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;分类树&lt;/strong&gt;用于分类标签值，如晴天/阴天/雾/雨、用户性别&lt;/p&gt;

&lt;p&gt;注意前者结果加减是有意义的，如10岁+5岁-3岁=12岁，后者结果加减无意义，如男+女=到底是男还是女？GBRT的核心在于累加所有树的结果作为最终结果，而分类树是没有办法累加的。所以GBDT中的树都是回归树而非分类树。&lt;/p&gt;

&lt;p&gt;第一棵树是正常的，之后所有的树的决策全是由&lt;strong&gt;残差&lt;/strong&gt;（此次的值与上次的值之差）来作决策。&lt;/p&gt;

&lt;h5&gt;三、算法原理&lt;/h5&gt;

&lt;p&gt;0.给定一个初始值&lt;/p&gt;

&lt;p&gt;1.建立M棵决策树（迭代M次）&lt;/p&gt;

&lt;p&gt;2.对函数估计值F(x)进行Logistic变换&lt;/p&gt;

&lt;p&gt;3.对于K各分类进行下面的操作（其实这个for循环也可以理解为向量的操作，每个样本点xi都对应了K种可能的分类yi，所以yi，F(xi)，p(xi)都是一个K维向量）&lt;/p&gt;

&lt;p&gt;4.求得残差减少的梯度方向&lt;/p&gt;

&lt;p&gt;5.根据每个样本点x，与其残差减少的梯度方向，得到一棵由J个叶子节点组成的决策树&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6.当决策树建立完成后，通过这个公式，可以得到每个叶子节点的增益（这个增益在预测时候用的）&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;每个增益的组成其实也是一个K维向量，表示如果在决策树预测的过程中，如果某个样本点掉入了这个叶子节点，则其对应的K个分类的值是多少。比如GBDT得到了三棵决策树，一个样本点在预测的时候，也会掉入3个叶子节点上，其增益分别为（假设为3分类问题）：
(0.5, 0.8, 0.1), (0.2, 0.6, 0.3), (0.4, .0.3, 0.3)，那么这样最终得到的分类为第二个，因为选择分类2的决策树是最多的。&lt;/p&gt;

&lt;p&gt;7.将当前得到的决策树与之前的那些决策树合并起来，作为一个新的模型（跟6中的例子差不多）&lt;/p&gt;

&lt;h5&gt;四、GBRT适用范围&lt;/h5&gt;

&lt;p&gt;该版本的GBRT几乎可用于所有的回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBRT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。&lt;/p&gt;

&lt;h5&gt;五、搜索引擎排序应用RankNet&lt;/h5&gt;

&lt;p&gt;搜索排序关注各个doc的顺序而不是绝对值，所以需要一个新的cost function，而RankNet基本就是在定义这个cost function，它可以兼容不同的算法（GBDT、神经网络...）。&lt;/p&gt;

&lt;p&gt;实际的搜索排序使用的是Lambda MART算法，必须指出的是由于这里要使用排序需要的cost function，LambdaMART迭代用的并不是残差。Lambda在这里充当替代残差的计算方法，它使用了一种类似Gradient*步长模拟残差的方法。这里的MART在求解方法上和之前说的残差略有不同。&lt;/p&gt;

&lt;p&gt;搜索排序也需要训练集，但多数用人工标注实现，即对每个(query, doc)pair给定一个分值（如1, 2, 3, 4），分值越高越相关，越应该排到前面。RankNet就是基于此制定了一个学习误差衡量方法，即cost function。RankNet对任意两个文档A,B，通过它们的人工标注分差，用sigmoid函数估计两者顺序和逆序的概率P1。然后同理用机器学习到的分差计算概率P2（sigmoid的好处在于它允许机器学习得到的分值是任意实数值，只要它们的分差和标准分的分差一致，P2就趋近于P1）。这时利用P1和P2求的两者的交叉熵，该交叉熵就是cost function。&lt;/p&gt;

&lt;p&gt;有了cost function，可以求导求Gradient，Gradient即每个文档得分的一个下降方向组成的N维向量，N为文档个数（应该说是query-doc pair个数）。这里仅仅是把”求残差“的逻辑替换为”求梯度“。每个样本通过Shrinkage累加都会得到一个最终得分，直接按分数从大到小排序就可以了。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>KNN算法-python实现</title>
     <link href="http://www.dianacody.com/2014/10/29/KNN-python.html"/>
     <updated>2014-10-29T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/10/29/KNN-python</id>
     <content type="html">&lt;h5&gt;一、算法原理&lt;/h5&gt;

&lt;ol&gt;
&lt;li&gt;计算抑制类别数据集中的点与当前点的距离（欧氏距离、马氏距离等）&lt;/li&gt;
&lt;li&gt;按照距离递增依次排序&lt;/li&gt;
&lt;li&gt;选取当前点距离最小的k个点&lt;/li&gt;
&lt;li&gt;确定前k个点所在类别出现频率&lt;/li&gt;
&lt;li&gt;返回前k个点出现频率最高的类别作为当前点的预测分类&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;注意：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;关于k值个数的选择，其取决于数据。一般地，在分类时，较大k值可以减小噪声的影响，但会使类别界限变得模糊。&lt;/p&gt;

&lt;blockquote&gt;&lt;ul&gt;
&lt;li&gt;好的k值可以通过各种启发式技术来获取（eg.交叉验证）&lt;/li&gt;
&lt;li&gt;噪声和非相关性特征向量的存在会使k近邻算法的准确性减小&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;近邻算法具有较强的一致性结果，随着数据趋于无线，算法的错误率不会超过贝叶斯算法错误率的2倍。对于一些好的k值，k近邻保证错误率不会超过贝叶斯理论误差率。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;h5&gt;二、源码实现&lt;/h5&gt;

&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;from numpy import *  
import time  
import matplotlib.pyplot as plt  


# calculate Euclidean distance  
def euclDistance(vector1, vector2):  
    return sqrt(sum(power(vector2 - vector1, 2)))  

# init centroids with random samples  
def initCentroids(dataSet, k):  
    numSamples, dim = dataSet.shape  
    centroids = zeros((k, dim))  
    for i in range(k):  
        index = int(random.uniform(0, numSamples))  
        centroids[i, :] = dataSet[index, :]  
    return centroids  

# k-means cluster  
def kmeans(dataSet, k):  
    numSamples = dataSet.shape[0]  
    # first column stores which cluster this sample belongs to,  
    # second column stores the error between this sample and its centroid  
    clusterAssment = mat(zeros((numSamples, 2)))  
    clusterChanged = True  

    ## step 1: init centroids  
    centroids = initCentroids(dataSet, k)  

    while clusterChanged:  
        clusterChanged = False  
        ## for each sample  
        for i in xrange(numSamples):  
            minDist  = 100000.0  
            minIndex = 0  
            ## for each centroid  
            ## step 2: find the centroid who is closest  
            for j in range(k):  
                distance = euclDistance(centroids[j, :], dataSet[i, :])  
                if distance &amp;lt; minDist:  
                    minDist  = distance  
                    minIndex = j  

            ## step 3: update its cluster  
            if clusterAssment[i, 0] != minIndex:  
                clusterChanged = True  
                clusterAssment[i, :] = minIndex, minDist**2  

        ## step 4: update centroids  
        for j in range(k):  
            pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]]  
            centroids[j, :] = mean(pointsInCluster, axis = 0)  

    print &#39;Congratulations, cluster complete!&#39;  
    return centroids, clusterAssment  

# show your cluster only available with 2-D data  
def showCluster(dataSet, k, centroids, clusterAssment):  
    numSamples, dim = dataSet.shape  
    if dim != 2:  
        print &quot;Sorry! I can not draw because the dimension of your data is not 2!&quot;  
        return 1  

    mark = [&#39;or&#39;, &#39;ob&#39;, &#39;og&#39;, &#39;ok&#39;, &#39;^r&#39;, &#39;+r&#39;, &#39;sr&#39;, &#39;dr&#39;, &#39;&amp;lt;r&#39;, &#39;pr&#39;]  
    if k &amp;gt; len(mark):  
        print &quot;Sorry! Your k is too large! please contact Zouxy&quot;  
        return 1  

    # draw all samples  
    for i in xrange(numSamples):  
        markIndex = int(clusterAssment[i, 0])  
        plt.plot(dataSet[i, 0], dataSet[i, 1], mark[markIndex])  

    mark = [&#39;Dr&#39;, &#39;Db&#39;, &#39;Dg&#39;, &#39;Dk&#39;, &#39;^b&#39;, &#39;+b&#39;, &#39;sb&#39;, &#39;db&#39;, &#39;&amp;lt;b&#39;, &#39;pb&#39;]  
    # draw the centroids  
    for i in range(k):  
        plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize = 12)  

plt.show() 
&lt;/code&gt;&lt;/pre&gt;

&lt;h5&gt;三、测试代码&lt;/h5&gt;

&lt;pre&gt;&lt;code&gt;from numpy import *  
import time  
import matplotlib.pyplot as plt  

## step 1: load data  
print &quot;step 1: load data...&quot;  
dataSet = []  
fileIn = open(&#39;E:/Python/Machine Learning in Action/testSet.txt&#39;)  
for line in fileIn.readlines():  
    lineArr = line.strip().split(&#39;\t&#39;)  
    dataSet.append([float(lineArr[0]), float(lineArr[1])])  

## step 2: clustering...  
print &quot;step 2: clustering...&quot;  
dataSet = mat(dataSet)  
k = 4  
centroids, clusterAssment = kmeans(dataSet, k)  

## step 3: show the result  
print &quot;step 3: show the result...&quot;  
showCluster(dataSet, k, centroids, clusterAssment)  
&lt;/code&gt;&lt;/pre&gt;
</content>
   </entry>
   
   <entry>
     <title>防止爬虫被墙的方法总结</title>
     <link href="http://www.dianacody.com/2014/10/01/spider_5.html"/>
     <updated>2014-10-01T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/10/01/spider_5</id>
     <content type="html">&lt;h5&gt;一、设置下载等待时间/下载频率&lt;/h5&gt;

&lt;p&gt;大规模集中访问对服务器的影响较大，爬虫可以短时间增大服务器负载。这里需要注意的是：设定下载等待时间的范围控制，等待时间过长，不能满足短时间大规模抓取的要求，等待时间过短则很有可能被拒绝访问。&lt;/p&gt;

&lt;p&gt;(1). 在之前“从url获取HTML”的方法里，对于httpGet的配置设置了socket超时和连接connect超时，其实这里的时长不是绝对的，主要取决于目标网站对爬虫的控制。&lt;/p&gt;

&lt;p&gt;(2). 另外，在scrapy爬虫框架里，专有参数可以设置下载等待时间download_delay，这个参数可以设置在setting.py里，也可以设置在spider里。&lt;/p&gt;

&lt;h5&gt;二、设置cookies&lt;/h5&gt;

&lt;p&gt;cookie其实是储存在用户终端的一些被加密的数据，有些网站通过cookies来识别用户身份，如果某个访问总是高频率地发请求，很可能会被网站注意到，被嫌疑为爬虫，这时网站就可以通过cookie找到这个访问的用户而拒绝其访问。&lt;/p&gt;

&lt;p&gt;可以自定义设置cookie策略（防止cookie rejected问题：拒绝写入cookie）或者禁止cookies。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(1) 自定义设置cookies策略（防止cookierejected问题，拒绝写入cookie）&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;在系列一那篇文章里就有自定义cookie策略设置，但更多的借鉴是官方文档的例子，设置方法其实都大同小异，因为HttpClient-4.3.1组件版本跟以前旧版本的不同，写法也有不同，另见官方文档：http://hc.apache.org/httpcomponents-client-4.3.x/tutorial/html/statemgmt.html#d5e553&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(2) 禁止cookies&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;通过禁止cookie，这是客户端主动阻止服务器写入。禁止cookie可以防止可能使用cookies识别爬虫的网站来ban掉我们。&lt;/p&gt;

&lt;p&gt;在scrapy爬虫中可以设置COOKIES_ENABLES= FALSE，即不启用cookies middleware，不向web server发送cookies。&lt;/p&gt;

&lt;h5&gt;三、修改User-Agent&lt;/h5&gt;

&lt;p&gt;最常见的就是伪装浏览器，修改User-Agent（用户代理）。&lt;/p&gt;

&lt;p&gt;User-Agent是指包含浏览器信息、操作系统信息等的一个字符串，也称之为一种特殊的网络协议。服务器通过它判断当前访问对象是浏览器、邮件客户端还是网络爬虫。在request.headers里可以查看user-agent，关于怎么分析数据包、查看其User-Agent等信息，这个在前面的文章里提到过。&lt;/p&gt;

&lt;p&gt;具体方法可以把User-Agent的值改为浏览器的方式，甚至可以设置一个User-Agent池（list，数组，字典都可以），存放多个“浏览器”，每次爬取的时候随机取一个来设置request的User-Agent，这样User-Agent会一直在变化，防止被墙。&lt;/p&gt;

&lt;h5&gt;四、修改IP&lt;/h5&gt;

&lt;p&gt;其实微博识别的是IP，不是账号。也就是说，当需要连续抓取很多数据的时候，模拟登录没有意义。只要是同一个IP，不管怎么换账号也没有用，主要的是换IP。&lt;/p&gt;

&lt;p&gt;web server应对爬虫的策略之一就是直接将IP或者整个IP段都封掉禁止访问，当IP被禁封后，转换到其他IP继续访问即可。方法：代理IP、本地IP数据库（使用IP池）。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(1) 从代理IP网站获取大量IP&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;如果总是请求代理IP站点也未免有些麻烦，况且某些代理IP站点有时还可能被禁封，当然再换一个代理IP站点也可以，如果你不嫌麻烦的话。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(2) 使用IP地址库&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;网上也有很多现成可用的IP地址库，可以存放到本地，如果本地有IP数据库就方便很多，至少不用每次都去请求代理IP站点了（当然可以一次性把站点内所有代理IP先爬下来存储好，形成本地IP数据库），总之获取IP的方法有很多，不一定非要是通过代理IP站点。&lt;/p&gt;

&lt;h5&gt;五、分布式爬取&lt;/h5&gt;

&lt;p&gt;分布式爬取的也有很多Github repo。原理主要是维护一个所有集群机器能够有效分享的分布式队列。&lt;/p&gt;

&lt;p&gt;使用分布式爬取还有另外一个目的：大规模抓取，单台机器的负荷很大，况且速度很慢，多台机器可以设置一个master管理多台slave去同时爬取。&lt;/p&gt;

&lt;p&gt;另外关于网页判重问题，可以用Bloom Filter。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>User-Based CF和Item-Based CF协同过滤推荐</title>
     <link href="http://www.dianacody.com/2014/09/25/CF.html"/>
     <updated>2014-09-25T00:00:00+08:00</updated>
     <id>http://www.dianacody.com/2014/09/25/CF</id>
     <content type="html">&lt;p&gt;（转载）之前一直觉得user-based和item-based差别不大，算法的差异小的我每次和别人说起都解释一下。后来慢慢的才发现丢脸了，其实从物理意义上2者差别大着呢，想想自己以前一直喜欢从物理角度给别人解说算法我就脸红。&lt;/p&gt;

&lt;p&gt;UserCF和ItemCF是协同过滤中最为古老的两种算法，早在20多年前就有学着提出来，由于简单，很多网站都应用。以我现在的阅读论文经验来说，YouTube等各大网站应用的算法，概括点说就是在数据清洗阶段不同，数据组织成矩阵存储之后，差不多都是有user-based和item-based的影子。我忘了和谁聊天了，似乎是面试官还是朋友说，数据挖掘，推荐引擎什么的其实也就那样，只不过外行的看起来很高深而已，做了才知道，弄来弄去不过如此。EMC一个博士在面试我的时候也提及了类似的观点，我实现的观点也觉得，只要把如果计算相似度的逻辑搞清楚了，其余剩下的没什么难度。User- based认为一个人会喜欢和他有相同爱好的人喜欢的东西，即人以群分，我在豆瓣上关注的人都是我喜欢的人，他们喜欢的东西我也喜欢，而Item-based认为一个人会喜欢和他以前喜欢的东西相似的东西，我喜欢文艺片，豆瓣会给我推荐文艺片。这两个假设都有其合理性。在网上看网友的博客指出，根据网友的测试，用UserCF和ItemCF做出的推荐列表中，只有50%是一样的，还有50%完全不同。但是这两个算法确有相似的精度。所以说，这两个算法是很互补的。这句话在很多书中也见过，但是没有做过测试检验。一下是网友的见解，粘贴过来：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;我一直认为这两个算法是推荐系统的根本，因为无论我们是用矩阵，还是用概率模型，我们都非常的依赖于前面说的两种假设。如果用户的行为不符合那两种假设，推荐系统就没必要存在了。因此我一直希望能够找出这两种算法的本质区别。他们有相似的精度，但是coverage相差很大，ItemCF coverage很大而UserCF很小。我还测试了很多其他指标，不过要从这些表象的指标差异找出这两个算法的本质区别还是非常困难。不过上周我基本发现了这两个算法推荐机理的本质区别。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;我们做如下假设。每个用户兴趣爱好都是广泛的，他们可能喜欢好几个领域的东西。不过每个用户肯定也有一个主要的领域，对这个领域会比其他领域更加关心。给定一个用户，假设他喜欢3个领域A,B,C，同时A是他喜欢的主要领域。这个时候我们来看UserCF和ItemCF倾向于做出什么推荐。&lt;/p&gt;

&lt;p&gt;结果如下，如果用UserCF, 它会将A,B,C三个领域中比较热门的东西推荐给用户【这个可以理解，算法会寻找同是喜欢这3个领域的用户，然后将这3个领域中最相似的物品进行推荐】。而如果用ItemCF，它会基本上只推荐A领域的东西给用户【A领域在用户偏好中占大部分，对应item-item相似度占比率大，被推荐概率大】。因为UserCF只推荐热门的，所以UserCF在推荐长尾上能力不足。而ItemCF只推荐A领域给用户，这样他有限的推荐列表中就可能包含了一定数量的不热门item，所以ItemCF推荐长尾的能力比较强。不过ItemCF的推荐对某一个用户而言，显然多样性不足。但是对整个系统而言，因为不同的用户的主要兴趣点不同，所以系统的coverage会很大。【终于明白了覆盖率大的含义】&lt;/p&gt;

&lt;p&gt;显然上面的两种推荐都有其合理性，但都不是最好的选择，因此他们的精度也会有损失。最好的选择是，如果我们给这个用户推荐30个item，我们既不是每个领域挑选10个最热门的给他，也不是推荐30个A领域的给他，而是比如推荐15个A领域的给他，剩下的15个从B,C中选择。【这个在实际应用中就不是很容易做到了，如何将用户兴趣分类？使用图论，连接矩阵将用户兴趣偏好识别？与其这样，还不如先将数据进行社区聚类，将数据根据图划分为几个小团体，针对小团体做推荐，这样就比如将只有上面提及的3中兴趣爱好的人组成的社区里面进行推荐。至于算法，小团体里大家都差不多，对应偏好矩阵稠密度较高，2个算法差距应该不大吧，具体有待考究】&lt;/p&gt;

&lt;p&gt;认识到这一点，可以给我们设计高精度的算法指明一个方向。就是当一个系统对个人推荐的多样性不足时，我们增加个人推荐的多样性可以提高精度。而当一个系统的整体多样性不足（比如只推荐popular的)，我们增加整体的多样性同样可以提高精度。&lt;/p&gt;
</content>
   </entry>
   
 
</feed>