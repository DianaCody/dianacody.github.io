<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DianaCody</title>
  <subtitle>subtitle,subtitle,subtitle</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.dianacody.com/"/>
  <updated>2019-02-26T11:27:18.000Z</updated>
  <id>http://www.dianacody.com/</id>
  
  <author>
    <name>DianaCody</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>推荐数据查询系统（RDQS）</title>
    <link href="http://www.dianacody.com/2018/01/05/2018-01-05-RDQS/"/>
    <id>http://www.dianacody.com/2018/01/05/2018-01-05-RDQS/</id>
    <published>2018-01-04T16:00:00.000Z</published>
    <updated>2019-02-26T11:27:18.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、项目源码"><a href="#一、项目源码" class="headerlink" title="一、项目源码"></a>一、项目源码</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目源码地址：（此项目暂不开放源码）</p>
<h5 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;离线画像数据主要包括用户画像、商品画像、召回源数据信息、用户账户信息等几大类。然而这些数据存储在不同的数据库服务中，比如HBase、Redis、Hive表、MemoryCache，以及基于这些底层数据库封装好的中间层数据服务中。因此查询整合不同的数据并观测调试数据变得相当麻烦，即离线画像数据需要可视化界面接口。于是基于Spring的mvc结构，开发出了这套java-web的推荐数据查询系统RDQS。</p>
<h5 id="三、架构设计"><a href="#三、架构设计" class="headerlink" title="三、架构设计"></a>三、架构设计</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据逻辑分层框架设计，如下图所示：<br><img src="/resources/rdqs-architecture.jpg" alt="figure-rdqs-architecture"><br>(1) <strong>数据库层</strong>：访问数据库及数据服务，将取回的数据进行初步处理。<br>(2) <strong>数据逻辑层</strong>：独立于数据库的数据结构，按照业务逻辑整合数据。<br>(3) <strong>前端界面层</strong>：产品用户体验设计(UED)，优化视觉及操作体验效果。</p>
<h5 id="四、后端"><a href="#四、后端" class="headerlink" title="四、后端"></a>四、后端</h5><h6 id="4-1-数据库层"><a href="#4-1-数据库层" class="headerlink" title="4.1 数据库层"></a>4.1 数据库层</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一、数据截断。为了防止一次性读取过多数据，返回超时，所以一般地需要将取回的数据截断为200条以内，同时用户在界面上也无需查看更多的数据；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    第二、并发取数。当用户选择同时查询多个画像数据，请求数据量较大，通常可以达到几万条数据，需要在0.1s内（人视觉观测不出现明显延迟）将数据全部加载完成，所以在取数逻辑上需要做并行优化；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第三、数据预加载。预先加载用户之前读取过的数据、经常访问的数据到缓存，而非每次到数据库读取，可以加快查询速度；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第四、数据滚动加载。为了减小数据访问延迟，采用后端分页的方式，滚动请求加载数据.用户在第一页停留时，实际只请求第一页的数据，当用户点击第二页时，再次请求后端数据库取回第二页的数据；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第五、数据格式整理。主要是为了方便前端逻辑展示；</p>
<h6 id="4-2-数据逻辑层"><a href="#4-2-数据逻辑层" class="headerlink" title="4.2 数据逻辑层"></a>4.2 数据逻辑层</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一，底层数据的封装。主要按业务逻辑划分为用户画像、商品画像、召回源数据、用户账户信息四大类数据，按照这四类做成检索目录，用户查询需要选择目录，增加索引模块，分目录索引查询；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二，支持不同平台账号查询userId/uuid/skuId。此处增加userId-uuid的映射表，定时更新，直接读取表中数据；</p>
<h6 id="4-3-数据展示层"><a href="#4-3-数据展示层" class="headerlink" title="4.3 数据展示层"></a>4.3 数据展示层</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提供前后端交互接口，前后端相互发送http请求进行数据交互。</p>
<h5 id="五、前端"><a href="#五、前端" class="headerlink" title="五、前端"></a>五、前端</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于javascript (jQuery)/Bootstrap。由于此系统未达到大型系统规模，所以目前尚未分离前端架构作为前端系统。事实上一些成熟的前端框架例如AngularJS、React等都不特别符合当前需求，所以直接采用javascript做些组件足矣。</p>
<h6 id="5-1-前端重构接口"><a href="#5-1-前端重构接口" class="headerlink" title="5.1 前端重构接口"></a>5.1 前端重构接口</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;接口的重构是为了更好地适应后台数据。</p>
<h6 id="5-2-前端组件化（前端工程）"><a href="#5-2-前端组件化（前端工程）" class="headerlink" title="5.2 前端组件化（前端工程）"></a>5.2 前端组件化（前端工程）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 前端工程是一个复杂的课题，从最简单的层面上讲，即是前端组件化。一个页面由不同的部分构成，那么这些部分就是各个独立互不干扰的组件，可以抽象出来以作后续复用。依照功能模块为主导，划分开发结构、甚至团队人员分工，这样利于后续维护，而不是依照文件目录划分开发结构。有关前端工程课题，在我的另外系列博客里面有详述。</p>
<h6 id="5-3-搜索功能"><a href="#5-3-搜索功能" class="headerlink" title="5.3 搜索功能"></a>5.3 搜索功能</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 支持用户交互式选择不同账号查询数据，用户可以从下拉菜单中选择是用userId、uuid、skuId查询。支持键盘上下选择与鼠标点击选择，请求后台返回数据，输入内容后，自动弹出下拉层，里面有若干匹配输入内容的建议项，按下键盘的上下键可以在下面的输入提示项中进行选择，选中的项自动将文字填补到搜索输入框中（如图），也可以用鼠标点击来选中项。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;交互式选择方案如下图所示：<br><img src="/resources/rdqs-search.jpg" alt="figure-rdqs-search"></p>
<h6 id="5-4-多级目录展示"><a href="#5-4-多级目录展示" class="headerlink" title="5.4 多级目录展示"></a>5.4 多级目录展示</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多级目录分层展示，分为三级菜单。选中之后只能查询当前目录对应的数据。此处与一般信息检索系统目录结构类似，不再赘述。</p>
<h5 id="六、其他"><a href="#六、其他" class="headerlink" title="六、其他"></a>六、其他</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;后期扩大为统一数据服务平台，对接所有离线数据。</p>
<h5 id="七、参考文献"><a href="#七、参考文献" class="headerlink" title="七、参考文献"></a>七、参考文献</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      数据可视化系统，基于java-web。
    
    </summary>
    
      <category term="Web" scheme="http://www.dianacody.com/categories/Web/"/>
    
    
      <category term="Web" scheme="http://www.dianacody.com/tags/Web/"/>
    
      <category term="Spring" scheme="http://www.dianacody.com/tags/Spring/"/>
    
      <category term="javascript" scheme="http://www.dianacody.com/tags/javascript/"/>
    
      <category term="jQuery" scheme="http://www.dianacody.com/tags/jQuery/"/>
    
      <category term="Bootstrap" scheme="http://www.dianacody.com/tags/Bootstrap/"/>
    
      <category term="Freemarker" scheme="http://www.dianacody.com/tags/Freemarker/"/>
    
  </entry>
  
  <entry>
    <title>场景推荐</title>
    <link href="http://www.dianacody.com/2017/06/26/2017-06-26-SceneRecommendation/"/>
    <id>http://www.dianacody.com/2017/06/26/2017-06-26-SceneRecommendation/</id>
    <published>2017-06-25T16:00:00.000Z</published>
    <updated>2019-02-26T12:03:06.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、项目源码"><a href="#一、项目源码" class="headerlink" title="一、项目源码"></a>一、项目源码</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目源码地址：（此项目暂不开放源码）</p>
<h5 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般地，商品分类体系都是按照类目进行划分的，最多分为三个不同级别层次类目。但是，如此分类难以满足特殊情形下用户购买需求。比如“旅行背包、运动鞋、运动衣、徒步手杖、自拍杆”这些商品不属于一个商品类目体系，但可能属于一个特殊场景“户外徒步”，在这种情况下，可以给用户推荐“自拍杆”，以达到场景推荐的目的。像这种人工总结的场景是一般地、实际地存在，并且有可以转化的价值。类似地，还可以提炼出其他场景主题。为了丰富召回数据体系，我们采用改进LDA主题模型来实现场景推荐。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最终的目的是，跨不同商品分类进行场景主题聚合。基于用户近期点击和购买行为，将多类商品各自组成pair对/n元组作为场景，同一场景下数据集互为推荐集，pig+python处理数据。<br>规定：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)场景为单一细粒度的明确主题；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)数据组不全在一个三级分类cid3里(否则与三级类重复)，粒度小于二级类；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)场景之间有足够向量距离；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4)覆盖率达99%以上，且防止数据元组分桶倾斜严重<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前期初始化人工定义场景主题数2k；后期数据挖掘自动化模型，迭代生成场景主题数10k。</p>
<h5 id="三、基于频繁项集FP-Growth方案"><a href="#三、基于频繁项集FP-Growth方案" class="headerlink" title="三、基于频繁项集FP-Growth方案"></a>三、基于频繁项集FP-Growth方案</h5><h6 id="3-1-数据获取"><a href="#3-1-数据获取" class="headerlink" title="3.1 数据获取"></a>3.1 数据获取</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户最近的click数据和order数据来自于session。抽取用户最近数据，比如点击、搜索、加购物车、订单，将这些数据组合为2~4元组，比如(click, order), (order, order), (search-word + cart, order)。然后，将每种方法抽取出来的排序较高的组合作为置信度较高的组合。收集30天数据，然后把出现频率较高的topN作为候选集。用3-4个月的候选集，然后对每个月数据取交集，得到一些集合。</p>
<h6 id="3-2-数据评测"><a href="#3-2-数据评测" class="headerlink" title="3.2 数据评测"></a>3.2 数据评测</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;【方法一】从每种方法取出的组合中，随机挑选出10%的组合，人工评测组合存在场景的比例。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;【方法二】选一个月订单和sample一定比例的click序列作为评测数据。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用这些组合来预测评测的数据，观测能匹配到多少数据，即每种方法的召回率。</p>
<h6 id="3-3-召回聚类、打标"><a href="#3-3-召回聚类、打标" class="headerlink" title="3.3 召回聚类、打标"></a>3.3 召回聚类、打标</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对这些规则方法计算相似度，以进行聚类，比如可以通过计算组合中每个元素pw\cid3\cid2\cid1重叠度，然后人工打标。</p>
<h6 id="3-4-结论"><a href="#3-4-结论" class="headerlink" title="3.4 结论"></a>3.4 结论</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过分析数据可以发现，按频数排序的话，选频数大于10的产品词组合，发现组合都集中在销量较高的日百商品类别，如”湿巾”、”洗发水”等，经过聚类后发现只有400多个组合，而且分布不能覆盖大部分商品类别。为此考虑另外一种方案来挖掘场景数据。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前基于频率的自动挖掘的方法会导致数据集中在高频购买的商品中，为了保证场景的覆盖率，我们人工定义出了一些场景，然后再把自动挖掘的数据添加到这些场景中，丰富这些场景。</p>
<h5 id="四、基于LDA主题模型方案"><a href="#四、基于LDA主题模型方案" class="headerlink" title="四、基于LDA主题模型方案"></a>四、基于LDA主题模型方案</h5><h6 id="4-1-主题分类体系"><a href="#4-1-主题分类体系" class="headerlink" title="4.1 主题分类体系"></a>4.1 主题分类体系</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从之前基于频繁项集方式自动挖掘的数据发现，自动挖掘的场景扎堆严重，且一些场景之间“距离”过近。实际上是用户行为太少、太集中，没有“基于场景的冷启动数据”而导致的数据分桶倾斜严重。<br><img src="/resources/scene-histogram.jpg" alt="figure-scene-histogram"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以需要通过从顶层设计一个场景分类体系来统领自动挖掘出的场景数据。目前的三级品类分类体系不能完全适合现在的数据。通过建立分类体系，一方面可以掌握场景在整个全站商品中的分布，对场景有一个直观认识；另一方面，可以保证以后挖掘的场景对所有商品覆盖率更高。目前的分类体系考虑二级分类，即“主类目-topic”。有12个主类目，大概有200多个场景主题(topic)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前采用两种方法相结合：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;【方法一】人工建立场景分类体系框架，人工填入产品词；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;【方法二】数据自动挖掘，找用户购买的商品id，对应分到人工定义好的框架的桶里；</p>
<h6 id="4-2-自动数据挖掘"><a href="#4-2-自动数据挖掘" class="headerlink" title="4.2 自动数据挖掘"></a>4.2 自动数据挖掘</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将每种方法抽取出来的组合选取topN作为可信度较高的组合作为候选组合。例如“沐浴露、洗发水、牙刷、牙膏”可以提炼出场景主题模型为“日常清洁”。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但是存在下面几个问题：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) 4个产品词的场景，重复的较多，需要合并，目前合并之后又不能构成一个场景；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) sku级别的场景的sku数量太少，同样需要泛化；</p>
<h6 id="4-3-结构流程图"><a href="#4-3-结构流程图" class="headerlink" title="4.3 结构流程图"></a>4.3 结构流程图</h6><p><img src="/resources/scene-process.jpg" alt="figure-scene-process"><br><strong> 第一阶段：达到2000个场景。 </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将候选组合放到现有的一级类中，人工把这些数据映射到topic上，定义出场景。根据场景得到sku。目前人工整理了一个场景列表，把场景对应的产品词找到，再映射到sku。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;流程：场景scene-&gt;产品词pwd-&gt;商品sku；<br><strong> 第二阶段：达到10000个场景。 </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采用第一阶段的方法，人力成本太高，考虑另一种方法。解决办法：可以考虑先对”产品词pwd”、”商品sku” 进行聚类。<br><strong> ① 计算词与词之间的距离 </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) 用word-embedding的方式计算<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) 用互信息和卡方统计量计算<br><strong> ② 聚类：k-means++ </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) 保证一个类里有5-7个产品词;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) 在得到聚类的情况下，去召回3天购买的sku。<br><strong>【自动挖掘场景】</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可以考虑先对满足threshold1的四元组产品词\sku进行聚类：<br><strong> ① K-means </strong><br>计算词与词之间的距离，用word-embedding和cos-similarity的方式计算。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优点：已完成一些结果，pw的效果还可以，值得尝试；sku的效果不行。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;缺点：理论上不太合理，找到的pw会存在相似的情况。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解决办法：人工review产品词结果，然后映射成sku。<br><strong> ② LDA </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优点：把场景看成一个topic，设置好topic数目即可得到pwd或sku。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;问题：如果把4元组看成一个序列，LDA不太适合短文本；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解决方法：可以用更长序列，如用户7天点击序列看成一个doc，这样保证一个doc有多个重复的item，而且没有停用词的干扰；<br><strong> ③ Brown-cluster </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;优点：属于层次聚类，完成聚类后也可以把控聚类的堆大小；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;问题：对序列比较有效，对元组的数据不敢保证。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解决方法：用更长序列，通过控制一些阈值来保证足够多（N）的类里有5-7个产品词；在得到聚类的情况下，去召回一个月的，窗口为3/7天购买的sku，每个类选取top300的sku作为召回数据。</p>
<h6 id="4-4-调参-amp-评测"><a href="#4-4-调参-amp-评测" class="headerlink" title="4.4 调参 &amp; 评测"></a>4.4 调参 &amp; 评测</h6><p><strong>(1) 以场景集(Scene Set)作为评价对象。</strong> 按整个场景集的大小、覆盖分类体系的情况、场景之间最小“距离”、topic下平均场景数，定义如下评测指标：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) 总场景数、分品类场景数；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) 场景品类覆盖率、topic整体覆盖率、分品类topic覆盖率；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) topic下平均场景数、分品类topic下平均场景数；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(d) 整个场景集中，场景间最小“距离”、分品类场景间最小“距离”；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(e) 场景平均产品词/SKU数、分品类场景平均产品词/SKU数；<br><strong>(2) 以单个场景作为评价对象。</strong> 按场景准确度、场景流量、销量、价格段，定义如下评测指标。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) 场景准确度；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) 场景一个月内产品涉及流量占比；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) 场景一个月内产品涉及销量占比；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(d) 场景产品整体价格段；</p>
<h6 id="4-5-线上接口"><a href="#4-5-线上接口" class="headerlink" title="4.5 线上接口"></a>4.5 线上接口</h6><p><strong>(1) 离线数据 </strong>：存储为3个HBase表</p>
<table>
<thead>
<tr>
<th>表名称</th>
<th>字段1</th>
<th>字段2</th>
<th>字段3</th>
<th>字段4</th>
<th>字段5</th>
</tr>
</thead>
<tbody>
<tr>
<td>场景表</td>
<td>scene_id</td>
<td>scene_name</td>
<td>scene_sku</td>
<td>scene_image</td>
<td>scene_info</td>
</tr>
<tr>
<td>清单表</td>
<td>scene_id</td>
<td>list_id</td>
<td>list_sku</td>
<td>/</td>
<td>/</td>
</tr>
<tr>
<td>文章表</td>
<td>scene_id</td>
<td>article_id</td>
<td>article_text</td>
<td>/</td>
<td>/</td>
</tr>
</tbody>
</table>
<p><strong>(2) 线上接口 </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>(a) 请求入参</strong>：推荐位、pin，type=0，<strong>返回结果</strong>：10个场景id、场景名、场景图片、场景说明；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>(b) 请求入参</strong>：推荐位、pin、type=1，<strong>返回结果</strong>：场景id、场景名、场景图片；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>(c) 请求入参</strong>：推荐位、pin、场景id，<strong>返回结果</strong>：sku、清单id、清单sku、文章id、文章文本；</p>
<h6 id="4-6-召回结果的排序方案"><a href="#4-6-召回结果的排序方案" class="headerlink" title="4.6 召回结果的排序方案"></a>4.6 召回结果的排序方案</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;排序涉及两个部分的排序：场景模块间排序，场景内sku的排序。场景内sku排序可以参考传统推荐位排序方案。同时，可以从两个角度来考虑排序，一是从匹配用户与场景相似度的角度，二是从场景触发的角度来考虑。</p>
<h6 id="4-6-1-匹配用户与场景相似度"><a href="#4-6-1-匹配用户与场景相似度" class="headerlink" title="4.6.1 匹配用户与场景相似度"></a>4.6.1 匹配用户与场景相似度</h6><p><strong> (a) 场景单边特征 </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在一周内，场景浏览占比、场景浏览用户数占比、场景下sku销量占比、下单用户数占比、场景复购周期。<br><strong> (b) 用户-场景双边特征 </strong></p>
<table>
<thead>
<tr>
<th>行为</th>
<th>窗口</th>
<th>粒度/属性</th>
<th>统计量</th>
</tr>
</thead>
<tbody>
<tr>
<td>browse</td>
<td>最近点击:1次、6次、10次，3分钟、10分钟，1小时，1天、2天、3天</td>
<td>scene</td>
<td>number, ratio</td>
</tr>
<tr>
<td>order</td>
<td>1天、2天，1周、2周，1月、2月、3月</td>
<td>scene</td>
<td>number, ratio</td>
</tr>
<tr>
<td>add-cart</td>
<td>1天、2天、1周</td>
<td>scene</td>
<td>number, ratio</td>
</tr>
</tbody>
</table>
<p><strong> (c) 用户-场景-产品词多边特征 </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;考虑冷启的问题，用户没有场景数据的时候，利用用户最近行为和场景的产品词的重合度，得到重合度为top5的场景ID和得分。</p>
<table>
<thead>
<tr>
<th>行为</th>
<th>窗口</th>
<th>粒度/属性</th>
<th>统计量</th>
</tr>
</thead>
<tbody>
<tr>
<td>browse</td>
<td>最近点击：50次</td>
<td>场景置信度</td>
<td>score</td>
</tr>
<tr>
<td>order</td>
<td>3天，2周，1月、2月、3月</td>
<td>场景置信度</td>
<td>score</td>
</tr>
<tr>
<td>add-cart</td>
<td>1天，1周</td>
<td>场景置信度</td>
<td>score</td>
</tr>
</tbody>
</table>
<h6 id="4-6-2-场景触发"><a href="#4-6-2-场景触发" class="headerlink" title="4.6.2 场景触发"></a>4.6.2 场景触发</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;场景触发可以考虑对场景内产品词的weight。</p>
<h5 id="五、工作成果"><a href="#五、工作成果" class="headerlink" title="五、工作成果"></a>五、工作成果</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;专利论文《基于主题模型的场景推荐》。</p>
<h5 id="六、参考文献"><a href="#六、参考文献" class="headerlink" title="六、参考文献"></a>六、参考文献</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      改进的LDA主题模型，跨不同商品分类进行场景主题聚合。
    
    </summary>
    
      <category term="Algorithm" scheme="http://www.dianacody.com/categories/Algorithm/"/>
    
    
      <category term="LDA" scheme="http://www.dianacody.com/tags/LDA/"/>
    
      <category term="pig" scheme="http://www.dianacody.com/tags/pig/"/>
    
      <category term="Topic Modeling" scheme="http://www.dianacody.com/tags/Topic-Modeling/"/>
    
      <category term="Data Mining" scheme="http://www.dianacody.com/tags/Data-Mining/"/>
    
      <category term="N-Gram" scheme="http://www.dianacody.com/tags/N-Gram/"/>
    
      <category term="K-means" scheme="http://www.dianacody.com/tags/K-means/"/>
    
      <category term="word-embedding" scheme="http://www.dianacody.com/tags/word-embedding/"/>
    
      <category term="Word to Vector" scheme="http://www.dianacody.com/tags/Word-to-Vector/"/>
    
      <category term="Brown Cluster" scheme="http://www.dianacody.com/tags/Brown-Cluster/"/>
    
  </entry>
  
  <entry>
    <title>商品画像</title>
    <link href="http://www.dianacody.com/2017/06/18/2017-06-18-ItemProfile/"/>
    <id>http://www.dianacody.com/2017/06/18/2017-06-18-ItemProfile/</id>
    <published>2017-06-17T16:00:00.000Z</published>
    <updated>2019-02-26T12:09:33.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、项目源码"><a href="#一、项目源码" class="headerlink" title="一、项目源码"></a>一、项目源码</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目源码地址：（此项目暂不开放源码）</p>
<h5 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和用户画像相对应的是，商品其实也有自己的一套独有特征。通过计算多维度商品特征：包括性别、年龄、类目级别、产品词、品牌词、热销、价格、购买力等级，便可以建立这样的体系：商品基础画像。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;商品之间也有一定联系，在各个类目下，对商品分别按照品牌词、产品词、扩展属性等不同维度进行关联、主题模型聚类、层次化抽象，可以建立另外的商品维度体系，也属于商品画像的范畴。最典型的应用就是可以聚合成不同场景的商品，以保证个性化推荐。</p>
<h5 id="三、商品基础画像"><a href="#三、商品基础画像" class="headerlink" title="三、商品基础画像"></a>三、商品基础画像</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>商品性别</strong>：一部分商品通常有性别之分，如手提包可以分为男士手提包和女士手提包，能反映出性别偏好。还有一部分商品没有性别区分。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>商品年龄</strong>：一部分商品有年龄适用范围，如鞋子分为婴儿鞋、青少年鞋、成人鞋、老人鞋等，反映出年龄偏好，还有一部分商品没有年龄区分。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>商品类目</strong>：按照大类划分的级别。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>产品词和品牌词</strong>：商品的基本属性，也包含规格参数。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>价格和购买力等级</strong>：商品是否是大众消费产品还是高端消费产品，消费等级属于什么层次。</p>
<h5 id="四、扩展的商品画像"><a href="#四、扩展的商品画像" class="headerlink" title="四、扩展的商品画像"></a>四、扩展的商品画像</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>关联商品</strong>：商品之间的相似度和相关度可以根据用户日常行为得到，经常组合在一起的商品必定有相关度，相关程度越高，这个商品的相关商品的属性就可以作为该商品的附属特征。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>聚合商品集</strong>：用频繁项集或者主题模型聚类，相同类目下的商品互为推荐集。这个在基于场景推荐的应用中已经有详细叙述。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>层次抽象</strong>：商品之间的层次隶属关系。</p>
<h5 id="五、参考文献"><a href="#五、参考文献" class="headerlink" title="五、参考文献"></a>五、参考文献</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      商品的多维度特征。
    
    </summary>
    
      <category term="Profile" scheme="http://www.dianacody.com/categories/Profile/"/>
    
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
      <category term="Item Profile" scheme="http://www.dianacody.com/tags/Item-Profile/"/>
    
  </entry>
  
  <entry>
    <title>用户购物模式跟踪分析</title>
    <link href="http://www.dianacody.com/2017/05/29/2017-05-29-PurchaseTrace/"/>
    <id>http://www.dianacody.com/2017/05/29/2017-05-29-PurchaseTrace/</id>
    <published>2017-05-28T16:00:00.000Z</published>
    <updated>2019-02-26T12:26:06.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、项目源码"><a href="#一、项目源码" class="headerlink" title="一、项目源码"></a>一、项目源码</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目源码地址：（此项目暂不开放源码）</p>
<h5 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;统计发现用户点击路径偏好，人群分布；基于session日志数据，统计用户小时/天/周/月/季度的点击+购物车+订单路径，入口标记；汇总人群占比。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;获取用户购买途径固有模式和习惯分析，标记典型模式，用户打标和画像，其他固有行为模式挖掘及利用。</p>
<h6 id="2-1-商品类目cid3偏好属性"><a href="#2-1-商品类目cid3偏好属性" class="headerlink" title="2.1 商品类目cid3偏好属性"></a>2.1 商品类目cid3偏好属性</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;置信度反映该用户的“隐形”偏好。一个用户浏览了许多商品，比如筛选了100多次，则有99%的置信度是这个偏好，但是该用户筛选了100次，但也不能保证是99.9%，也只可能有99.5%的可能性/置信度，是说明有这个偏好。① 搜索词 、② 筛选偏好词（搜索过滤词）、③ 排序、 ④搜索结果列表页里，点击、浏览（商品详情页面浏览的列表展示，一般有共同属性）；或者反例，反例sku上停留时间特别短，注意过滤只看有效信息，噪音不看。<em>（注意：dwell-time是统计瞬时时间，一个session内的时间，主要用来判断误点击，过滤；不是用来判断长期时间的，即联合多个session的dwell-time基本没什么意义）。</em><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户画像方面，增加产品中心词提取、产品关键属性词提取算法，包括自动挖掘用户的复购，浏览，加购的产品词和产品属性的共性数据。 </p>
<h6 id="2-2-购物-下单（cart-order）路径"><a href="#2-2-购物-下单（cart-order）路径" class="headerlink" title="2.2 购物+下单（cart+order）路径"></a>2.2 购物+下单（cart+order）路径</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从哪个入口进来的，按单个用户计数；最终统计所有用户结果汇总，观测总的人群占比（宏观的调研）。</p>
<h6 id="2-3-点击-关注（click-follow）路径"><a href="#2-3-点击-关注（click-follow）路径" class="headerlink" title="2.3 点击+关注（click+follow）路径"></a>2.3 点击+关注（click+follow）路径</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这部分用户画像是用户behavior行为，和购物不相关。</p>
<h5 id="三、方案"><a href="#三、方案" class="headerlink" title="三、方案"></a>三、方案</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 以session为单位，完整（或挑选关注部分完整）记录用户访问行为以完整勾勒出访问全路径，据此数据进行分析。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;步骤:<br>(1) 通过读取Session Pruner剪辑后的输出结果数据，统计每个用户相当长一段时期（例如半年以上），全部有购物的session及其购买途径，寻找是否存在一定模式和规律。<br>(2) 两种方式<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) 对于部分用户中存在的典型模式，进行定义、标记，并按此给这部分用户打标、画像。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) 类似地，通过对每个用户时间纵向session数据的深度统计分析，寻找其中某些可能反复出现的模式，对其定义、标记；<br>(3) 根据这些固有模式，优化推荐结果和算法。</p>
<h5 id="四、数据分析"><a href="#四、数据分析" class="headerlink" title="四、数据分析"></a>四、数据分析</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;时间段：（1day, 3days, 7days）；dwell_time、avg_time、sum_time，看sku的数量。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;特征：乘以“时间窗口”（1day,3days,7days）</p>
<h6 id="4-1-实验数据"><a href="#4-1-实验数据" class="headerlink" title="4.1 实验数据"></a>4.1 实验数据</h6><table>
<thead>
<tr>
<th>数据来源</th>
<th>时间跨度</th>
<th>数据大小</th>
</tr>
</thead>
<tbody>
<tr>
<td>用户浏览加购物车数据</td>
<td>1 month</td>
<td>1.1 GB</td>
</tr>
<tr>
<td>用户购买行为数据</td>
<td>3 month</td>
<td>3.0 GB</td>
</tr>
</tbody>
</table>
<h6 id="4-2-方法与结果"><a href="#4-2-方法与结果" class="headerlink" title="4.2 方法与结果"></a>4.2 方法与结果</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) 将用户搜索加购产生的sku组成四元组，并统计四元组频数。根据”jaccard相似度”进行聚类。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) 将(1)方法中的四元组作为语料，训练”word2vec模型”，利用”kmeans方法”进行聚类。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) 收集同一用户六个月的购买商品的”产品词”作为语料，训练”word2vec模型”，利用”kmeans算法”聚类。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4) 由于kmeans方法对于初始节点的选择依赖性过强，执行多次算法产生多份数据进行合并。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5) 尝试以一份kmeans产生的聚类结果为基础，计算所有产品词词向量与每一个聚类的余弦相似度进行二次划分。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(6) 尝试将较大规模的类利用kmeans方法，二次聚类。但是效果不理想，仍然不能将大类细分。</p>
<h6 id="4-3-结果分析"><a href="#4-3-结果分析" class="headerlink" title="4.3 结果分析"></a>4.3 结果分析</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) 存在着一些cluster包含的产品词数量太多太杂，无法聚合为固定模式，数据比较稀疏。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) 存在着一些cluster内的产品词相似度过高。</p>
<h5 id="五、改进方案"><a href="#五、改进方案" class="headerlink" title="五、改进方案"></a>五、改进方案</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) 缩小order数据的时间跨度。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) 数据采样应该更多遵循用户模式规律。</p>
<h5 id="六、参考文献"><a href="#六、参考文献" class="headerlink" title="六、参考文献"></a>六、参考文献</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      依据用户日志研究用户购物行为模式。
    
    </summary>
    
      <category term="Profile" scheme="http://www.dianacody.com/categories/Profile/"/>
    
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
      <category term="User Profile" scheme="http://www.dianacody.com/tags/User-Profile/"/>
    
  </entry>
  
  <entry>
    <title>用户画像</title>
    <link href="http://www.dianacody.com/2017/03/30/2017-03-30-UserProfile/"/>
    <id>http://www.dianacody.com/2017/03/30/2017-03-30-UserProfile/</id>
    <published>2017-03-29T16:00:00.000Z</published>
    <updated>2019-02-26T12:34:10.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、项目源码"><a href="#一、项目源码" class="headerlink" title="一、项目源码"></a>一、项目源码</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目源码地址：（此项目暂不开放源码）</p>
<h5 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户的行为模式各不相同，但大多都遵循固定的模式，反映了用户的个性化特征，即用户画像。计算不同维度的用户特征，能有效利用这些数据为用户进行个性化推荐。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户画像分为长期用户画像和实时用户画像。<br><img src="/resources/user-profile.jpg" alt="figure-user-profile"></p>
<h5 id="三、长期用户画像"><a href="#三、长期用户画像" class="headerlink" title="三、长期用户画像"></a>三、长期用户画像</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;长期用户画像，其数据至少都来源于几个月及以上的用户日志，即多维度用户特征，存储为离线数据表。包括性别、年龄、地理位置、商品类目兴趣、产品词偏好、品牌词偏好、扩展属性、购买力、店铺偏好、折扣偏好、孩子性别年龄。通过对用户日志整理，完成对原始SDK埋点日志的提取，获取完整的session数据，对用户画像数据粒度进行升级。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>用户的深度行为</strong>：停留时长、看评论；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>用户的主动反馈</strong>：搜索、写评论、投诉；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>用户的跨平台行为</strong>：不同设备登录的账号其实对应同一个用户，通过统一这些账号的行为，建立起一个立体的用户画像。</p>
<h5 id="四、实时用户画像"><a href="#四、实时用户画像" class="headerlink" title="四、实时用户画像"></a>四、实时用户画像</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实时用户画像，包括实时反馈、近期行为，粒度在秒级，例如30 second, 1 hour, 6 hour, 1 day, 3 days, 1week。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户实时行为流可以作为实时特征来计算，采用storm实时计算。</p>
<h5 id="五、参考文献"><a href="#五、参考文献" class="headerlink" title="五、参考文献"></a>五、参考文献</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      用户多维度特征。
    
    </summary>
    
      <category term="Profile" scheme="http://www.dianacody.com/categories/Profile/"/>
    
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
      <category term="User Profile" scheme="http://www.dianacody.com/tags/User-Profile/"/>
    
  </entry>
  
  <entry>
    <title>Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank</title>
    <link href="http://www.dianacody.com/2017/02/23/2017-02-23-DeepVisual/"/>
    <id>http://www.dianacody.com/2017/02/23/2017-02-23-DeepVisual/</id>
    <published>2017-02-22T16:00:00.000Z</published>
    <updated>2018-11-30T09:25:38.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>One consideration to make is that large modern CNNs require large amounts of training data. The amount of examples available to an individual query model can be in the low hundreds, particularly for queries in the long tail. This makes training one deep CNN per query from scratch prone to overfitting. Transfer learning is a popular method for dealing with this problem, with many expels in computer vision.<br>Learning to rank search results has received considerable attention over the past decade, and it is at the core of modern information retrieval. </p>
<h5 id="Works"><a href="#Works" class="headerlink" title="Works"></a>Works</h5><h6 id="1-Multimodal-Listing-Embedding"><a href="#1-Multimodal-Listing-Embedding" class="headerlink" title="(1) Multimodal Listing Embedding"></a>(1) <strong>Multimodal Listing Embedding</strong></h6><p>Our listing contains text information such as descriptive title and tags, as well as an image of the item for sale. To measure the value of including image information, we embed listings in a multimodal space (consisting of high-level text …)</p>
<h6 id="2-Training-Models"><a href="#2-Training-Models" class="headerlink" title="(2) Training Models"></a>(2) <strong>Training Models</strong></h6><p>we extend our existing learning to rank models with image information. At first, we embed listings for the learning to rank task in both single modality and multimodal settings. Then, the listing embedded in both modalities are incorporated into a learning to rank framework.</p>
]]></content>
    
    <summary type="html">
    
      Search is at the heart of modern e-commerce. Traditional models optimize over a few hand-constructed features based on the item’s text. Now, there is a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural net work.
    
    </summary>
    
      <category term="Deep Visual" scheme="http://www.dianacody.com/categories/Deep-Visual/"/>
    
    
      <category term="Learning to rank" scheme="http://www.dianacody.com/tags/Learning-to-rank/"/>
    
      <category term="Computer vision" scheme="http://www.dianacody.com/tags/Computer-vision/"/>
    
      <category term="Deep learning" scheme="http://www.dianacody.com/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Targeted Content for a Real-Time Activity Feed, For First Time Visitors to Power Users</title>
    <link href="http://www.dianacody.com/2017/01/22/2017-01-22-RealTimeFeed/"/>
    <id>http://www.dianacody.com/2017/01/22/2017-01-22-RealTimeFeed/</id>
    <published>2017-01-21T16:00:00.000Z</published>
    <updated>2018-11-30T09:26:02.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>The Activity Feed is a frequently updated stream of content in the form of stories that flow from the top to the bottom of the page as time goes on. we addressed these issues in a new version of the Activity Feed that was built and deployed to all of our tens of millions of uses. These users cover the full gamut of our website engagement levels, ranging from the first-time visitor to the long-time power user. </p>
<h5 id="Activity-Feeds"><a href="#Activity-Feeds" class="headerlink" title="Activity Feeds"></a>Activity Feeds</h5><p>Activity feeds are loosely time-ordered sets of varied content, and are commonly found on popular social networks such as Twitter, Facebook, and LinkedIn. The content in feeds is drawn from a set of content products. Typically, content producers are nodes in a user’s social graph, which represents the user’s interests based on explicit user action, for example, “following” another user. We call content generated from a user’s social graph organic content. We also note that the time-ordered nature of the AF makes it ideal for keeping up with the actions of your social graph, the users with whom one shares commonalities. Because AFs are singular source for “what is new” amongst a peer group, they often serve as a primary landing page for users.</p>
]]></content>
    
    <summary type="html">
    
      The Activity Feed (AF) is our taking on the ubiquitous “web feed” - a continuous stream of aggregated content, personalized for each user. These streams have become the defacto means of serving advertisements in the context of social media.
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Data Mining" scheme="http://www.dianacody.com/tags/Data-Mining/"/>
    
      <category term="Large-scale Systems" scheme="http://www.dianacody.com/tags/Large-scale-Systems/"/>
    
  </entry>
  
  <entry>
    <title>Style in the Long Tail, Discovering Unique Interests with Latent Variable Models in Large Scale Social E-commerce</title>
    <link href="http://www.dianacody.com/2017/01/21/2017-01-21-LongTail/"/>
    <id>http://www.dianacody.com/2017/01/21/2017-01-21-LongTail/</id>
    <published>2017-01-20T16:00:00.000Z</published>
    <updated>2018-11-30T09:26:58.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>An online marketplace for handmade and vintage items, with over 30 million active users and 30 million active listings. This is a marketplace known for tis diverse and eclectic content (e.g. Figure 1); people come in order to find those unusual items that match the peculiarities of their style. Indeed, in its entirety could be considered part of the e-commerce long tail; in addition to wide ranging functions and styles, the handmade and vintage nature of the site means that most items for sale are unique.</p>
<h5 id="Works"><a href="#Works" class="headerlink" title="Works"></a>Works</h5><h6 id="1-Recommendation-Systems"><a href="#1-Recommendation-Systems" class="headerlink" title="(1)Recommendation Systems"></a>(1)<strong>Recommendation Systems</strong></h6><p>recommender systems are nothing new, with the first papers on collaborative filtering appearing in the 1990s.The range of techniques available when building recommender systems is vast, too board to cover here. For a good overview of common techniques, we urge the curious reader to read the survey of Adomavicius and Tuzhilin. Also of note is the work of Koren, Kolinsky and others describing the approaches that won the Netflix prize.</p>
<h6 id="2-Latent-Dirichlet-Allocation-LDA"><a href="#2-Latent-Dirichlet-Allocation-LDA" class="headerlink" title="(2)Latent Dirichlet Allocation (LDA)"></a>(2)<strong>Latent Dirichlet Allocation (LDA)</strong></h6><p>Latent Dirichlet Allocation (LDA) is an unsupervised, probabilistic, generative model that aims to find a low dimensional description that can summarize the contents of large document collections. </p>
<h5 id="Identifying-User-Interests"><a href="#Identifying-User-Interests" class="headerlink" title="Identifying User Interests"></a>Identifying User Interests</h5><h6 id="1-Social-E-commerce"><a href="#1-Social-E-commerce" class="headerlink" title="(1)Social E-commerce"></a>(1)<strong>Social E-commerce</strong></h6><p>There are four important entities:<br>“User”: Anyone registered on our website, including sellers<br>“Seller”: our user who own a shop<br>“Shop”: A collection of items sold by the same seller. Each shop has its own online storefront.<br>“Listing”: Products/items listed in a shop, each with its unique listing id.<br>To give an idea of scale, we currently have approximately 1 million active sellers/shops, 30 million active listings, and 30 million active members.</p>
<h6 id="2-Inferring-User-Interests"><a href="#2-Inferring-User-Interests" class="headerlink" title="(2)Inferring User Interests"></a>(2)<strong>Inferring User Interests</strong></h6><p>Our use of LDA is based on the premise that users with similar interests will act upon similar listings. We chose to user the social action of “favoriting” listings as a reliable signal for user style. This is done in lieu of more traditional user intent signals, for instance “purchasing” as is commonly done in collaborative filter development. </p>
<h5 id="References"><a href="#References" class="headerlink" title="References"></a>References</h5><p>[1] …<br>[2] …</p>
]]></content>
    
    <summary type="html">
    
      An online marketplace for handmade and vintage goods with over 30 million diverse listings, the problem of capturing the taste is particularly important - users come to the site specifically to find items that match their eclectic styles.
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Collaborative Filtering" scheme="http://www.dianacody.com/tags/Collaborative-Filtering/"/>
    
      <category term="Topic Modeling" scheme="http://www.dianacody.com/tags/Topic-Modeling/"/>
    
  </entry>
  
  <entry>
    <title>陪伴计划</title>
    <link href="http://www.dianacody.com/2016/12/25/2016-12-25-AccompanyPlan/"/>
    <id>http://www.dianacody.com/2016/12/25/2016-12-25-AccompanyPlan/</id>
    <published>2016-12-24T16:00:00.000Z</published>
    <updated>2019-02-26T12:42:52.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、项目源码"><a href="#一、项目源码" class="headerlink" title="一、项目源码"></a>一、项目源码</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目源码地址：（此项目暂不开放源码）</p>
<h5 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;新增的一个用户入口，若用户没有注册档案，则调用预测用户接口，将预测用户展现在前端，用户可以快速将预测的孩子数据转化为孩子档案。用此方法，提升孩子档案转化率，并且可以通过线上填写和线下BI结合来提升孩子模型预测精准度、扩大数据源召回范围。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据流向：用户孩子档案 -&gt; 挖掘新用户 –&gt; 通过陪伴计划将预测的孩子作为召回 –&gt; 更新用户档案 -&gt; ……（循环）<br><img src="/resources/accompany-flow.jpg" alt="figure-accompany-flow"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据逻辑：将用户填写的高质量潜在孩子档案作为种子用户，进行新用户预测。通过数据分析发现，用户随着孩子年龄增长，购买的商品有明显的序列特征。并可以通过挖掘母婴用户在时间轴的不同阶段对商品、品类、品牌、产品属性的不同喜好的行为特征。并且通过数据可以看出，母婴用户在不同年龄段购买的商品特征非常明显，可以考虑结合该特征预测更多潜在用户。</p>
<h5 id="三、主要内容"><a href="#三、主要内容" class="headerlink" title="三、主要内容"></a>三、主要内容</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一个小型的推荐系统，在原有基础上进行升级开发，包括线上接口升级、离线模型升级。</p>
<h6 id="3-1-线上接口升级"><a href="#3-1-线上接口升级" class="headerlink" title="3.1 线上接口升级"></a>3.1 线上接口升级</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;包括预测用户年龄、孩子模型排序接口，并按照用户特定模式进行不同过滤排序逻辑展示。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;标签推荐：一级标签排序之后，再以二级标签逻辑排序（二级标签为点击一级标签之后展示的子级别标签）。判断用户是否有订阅标签，若是则优先按时间排序展示订阅标签，其余按照用户默认顺序排列。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据流向：商品类目 -&gt; 用户账号 –&gt; 相关类目质量分 -&gt; 通过标签汇总。</p>
<h6 id="3-2-离线用户特征维度升级"><a href="#3-2-离线用户特征维度升级" class="headerlink" title="3.2 离线用户特征维度升级"></a>3.2 离线用户特征维度升级</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;增加200个三级品类cid3，按照用户年龄性别季节不同扩展属性制定算法规则，计算候选集质量分，作为召回推荐商品依据。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;形成每个商品会有以下维度，所在三级类、年龄（学龄）标签，性别标签、属性（tag）标签，sku id，排序质量分，如下表所示：</p>
<table>
<thead>
<tr>
<th>三级类目</th>
<th>年龄</th>
<th>性别</th>
<th>季节</th>
<th>扩展类型（学龄）</th>
<th>sku-id</th>
<th>排序质量分</th>
</tr>
</thead>
<tbody>
<tr>
<td>12345</td>
<td>72~83 months (6-year-old)</td>
<td>male</td>
<td>summer</td>
<td>1</td>
<td>1234567890</td>
<td>4.19615</td>
</tr>
</tbody>
</table>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于cid3确定品类年龄、季节、性别区分度进行数据过滤，以及sku的质量分和计算标签（或cid3）的排序。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据召回：根据前端请求参数中的小孩年龄、性别、季节，找到对应年龄、性别、季节下的sku，对这些sku按其绑定的不同标签属性进行分组展示。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在线过滤：主要是一些库存过滤。</p>
<h5 id="四、提升价值"><a href="#四、提升价值" class="headerlink" title="四、提升价值"></a>四、提升价值</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>用户标签场景化</strong>：扩大召回源品类数据集。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>商品推荐精准化</strong>：精细化各品类在年龄性别季节，提升商品推荐准确度。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>用户画像档案拉新</strong>：将预测行为转化为档案数据，让更多用户参与迭代预测孩子模型。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;截止项目完成，App该入口已达成100w活跃建档用户数，PV/UV价值提升。</p>
<h5 id="五、参考文献"><a href="#五、参考文献" class="headerlink" title="五、参考文献"></a>五、参考文献</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      一个小型推荐系统。
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Recommender System" scheme="http://www.dianacody.com/tags/Recommender-System/"/>
    
      <category term="Online" scheme="http://www.dianacody.com/tags/Online/"/>
    
      <category term="Java" scheme="http://www.dianacody.com/tags/Java/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐系统（离线）</title>
    <link href="http://www.dianacody.com/2016/09/15/2016-09-15-RecOffline/"/>
    <id>http://www.dianacody.com/2016/09/15/2016-09-15-RecOffline/</id>
    <published>2016-09-14T16:00:00.000Z</published>
    <updated>2019-02-26T12:56:40.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、项目源码"><a href="#一、项目源码" class="headerlink" title="一、项目源码"></a>一、项目源码</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目源码地址：（此项目暂不开放源码）</p>
<h5 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;特征学习是在原始数据中自动识别和使用特征。现代深度学习方法在特征学习领域中有很多成功的案例，比如自编码器和受限玻尔兹曼机，它们以无监督或半监督的方式实现自动地学习的特征表示，以压缩形式。其结果用于支撑例如语音识别、图像分类、物体识别和其他领域的先进成果。抽象的特征表达可以自动获取，但是仅通过这个是无法理解和利用这些学习得到的结果，只有黑盒的方式才使用这些特征。同时，如何创造和那些效果很好的特征相似或相异的特征，变得比较困难。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常情况下，特征工程都要经过以下几个步骤：收集数据、数据预处理、数据建模、特征设计、特征选择、评估模型、模型迭代。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;个性化系统中的模型训练主要是针对用户之前的行为，比如搜索、浏览、点击、加购物车、下单，选择合适特征，训练模型，预测用户未来的行为。</p>
<h5 id="三、数据收集"><a href="#三、数据收集" class="headerlink" title="三、数据收集"></a>三、数据收集</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过参数服务器parameter server收集用户特征日志feature logs，包括用户行为位置、时间、次数、频率、偏好等基本特征的记录，时间窗口一般为多天、多月，从7天到30天不等，这个要依据不同实验位置的模型需求而定。</p>
<h5 id="四、数据预处理"><a href="#四、数据预处理" class="headerlink" title="四、数据预处理"></a>四、数据预处理</h5><h6 id="4-1-数据清洗格式化"><a href="#4-1-数据清洗格式化" class="headerlink" title="4.1 数据清洗格式化"></a>4.1 数据清洗格式化</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据清洗是为了去除数据中的噪音，比如一些异常数据、误记录数据、漏报数据等。数据格式化是为了将数据整理成模型训练框架需要的输入格式，同时也便于观测。</p>
<h6 id="4-2-数据采样"><a href="#4-2-数据采样" class="headerlink" title="4.2 数据采样"></a>4.2 数据采样</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依据各推荐位埋点情况进行各比例的点击、订单数据采样，不同试验位置的模型，采样比例也不同。一般地，将点击、点击后订单、直接订单作为正样本，标记为action≠0，而这其中又可以细分为action=1,2,3等等，而未点击、未浏览的作为负样本，标记为action=0。同时正样本需要不同的采样策略，比如点击样本，一部分可以作为正样本，而一部分可以作为负样本，取决于训练时将采用什么样的策略。样本就是一个标记加一堆特征，一个样本要么被标记为order，要么被标记为click。</p>
<h6 id="4-3-数据转换"><a href="#4-3-数据转换" class="headerlink" title="4.3 数据转换"></a>4.3 数据转换</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上文已述。</p>
<h5 id="五、模型训练"><a href="#五、模型训练" class="headerlink" title="五、模型训练"></a>五、模型训练</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采用人工特征和自动训练提取特征相结合的方法。</p>
<h6 id="5-1-人工特征"><a href="#5-1-人工特征" class="headerlink" title="5.1 人工特征"></a>5.1 人工特征</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用作初始化特征，特别针对某些行为不多的欠活跃用户。比如统计时间窗口为一周或一个月用户的浏览、点击、加购物车、订单情况作为特征。如下所示，每个维度相乘得到大约几十维基本特征。</p>
<table>
<thead>
<tr>
<th>sku</th>
<th>cid3</th>
<th>brand</th>
<th>productWord</th>
</tr>
</thead>
<tbody>
<tr>
<td>sku-7day-browse-total</td>
<td>cid3-7day-browse-total</td>
<td>brand-7day-browse-total</td>
<td>pwd-7day-browse-total</td>
</tr>
<tr>
<td>sku-7day-browse-avg</td>
<td>cid3-7day-browse-avg</td>
<td>brand-7day-browse-avg</td>
<td>pwd-7day-browse-avg</td>
</tr>
<tr>
<td>sku-7day-browse-ratio-all-station</td>
<td>cid3-7day-browse-ratio-all_station</td>
<td>brand-7day-browse-ratio-all-station</td>
<td>pwd-7day-browse-ratio-all-station</td>
</tr>
<tr>
<td>sku-7day-browse-ratio-category</td>
<td>cid3-7day-browse-ratio-category</td>
<td>brand-7day-browse-ratio-category</td>
<td>pwd-7day-browse-ratio-category</td>
</tr>
<tr>
<td>sku-7day-browse-users-total</td>
<td>cid3-7day-browse-users-total</td>
<td>brand-7day-browse-users-total</td>
<td>pwd-7day-browse-users-total</td>
</tr>
<tr>
<td>sku-7day-browse-users-avg</td>
<td>cid3-7day-browse-users-avg</td>
<td>brand-7day-browse-users-avg</td>
<td>pwd-7day-browse-users-avg</td>
</tr>
<tr>
<td>sku-7day-browse-users-ratio-all-station</td>
<td>cid3-7day-browse-users-ratio-all-station</td>
<td>brand-7day-browse-users-ratio-all-station</td>
<td>pwd-7day-browse-users-ratio-all-station</td>
</tr>
<tr>
<td>sku-7day-browse-users-ratio-category</td>
<td>cid3-7day-browse-users-ratio-category</td>
<td>brand-7day-browse-users-ratio-category</td>
<td>pwd-7day-browse-users-ratio-category</td>
</tr>
<tr>
<td>sku-7day-sale-total</td>
<td>cid3-7day-sale-total</td>
<td>brand-7day-sale-total</td>
<td>pwd-7day-sale-total</td>
</tr>
<tr>
<td>sku-7day-sale-avg</td>
<td>cid3-7day-sale-avg</td>
<td>brand-7day-sale-avg</td>
<td>pwd-7day-sale-avg</td>
</tr>
<tr>
<td>sku-7day-sale-ratio-all-station</td>
<td>cid3-7day-sale-ratio-all-station</td>
<td>brand-7day-sale-ratio-all-station</td>
<td>pwd-7day-sale-ratio-all-station</td>
</tr>
<tr>
<td>sku-7day-sale-ratio-category</td>
<td>cid3-7day-sale-ratio-category</td>
<td>brand-7day-sale-ratio-category</td>
<td>pwd-7day-sale-ratio-category</td>
</tr>
<tr>
<td>sku-7day-order-total</td>
<td>cid3-7day-order-total</td>
<td>brand-7day-order-total</td>
<td>pwd-7day-order-total</td>
</tr>
<tr>
<td>sku-7day-order-avg</td>
<td>cid3-7day-order-avg</td>
<td>brand-7day-order-avg</td>
<td>pwd-7day-order-avg</td>
</tr>
<tr>
<td>sku-7day-order-ratio-all-station</td>
<td>cid3-7day-order-ratio-all-station</td>
<td>brand-7day-order-ratio-all-station</td>
<td>pwd-7day-order-ratio-all-station</td>
</tr>
<tr>
<td>sku-7day-order-ratio-category</td>
<td>cid3-7day-order-ratio-category</td>
<td>brand-7day-order-ratio-category</td>
<td>pwd-7day-order-ratio-category</td>
</tr>
<tr>
<td>sku-7day-conversion-rate-total</td>
<td>cid3-7day-conversion-rate-total</td>
<td>brand-7day-conversion-rate-total</td>
<td>pwd-7day-conversion-rate-total</td>
</tr>
<tr>
<td>sku-7day-conversion-rate-avg</td>
<td>cid3-7day-conversion-rate-avg</td>
<td>brand-7day-conversion-rate-avg</td>
<td>pwd-7day-conversion-rate-avg</td>
</tr>
</tbody>
</table>
<h6 id="5-2-自动提取特征"><a href="#5-2-自动提取特征" class="headerlink" title="5.2 自动提取特征"></a>5.2 自动提取特征</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于活跃度较高的用户样本，特征也比较多，直接采用训练框架Xgboost/TensorFlow训练模型，自动提取特征。</p>
<h5 id="六、模型评估"><a href="#六、模型评估" class="headerlink" title="六、模型评估"></a>六、模型评估</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;特征重要性(featureImportance)；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;预测命中率(click@4/20, order@4/20)；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型上线评估(CTR/CVR)；<br>下面给出示例表示不同模型的对比：</p>
<table>
<thead>
<tr>
<th>model</th>
<th>Experiments</th>
<th>trainSet</th>
<th>testSet</th>
<th>weight</th>
<th>click/order@N</th>
</tr>
</thead>
<tbody>
<tr>
<td>model_v1</td>
<td>101</td>
<td>7 days</td>
<td>2 days</td>
<td>50:5:1</td>
<td>0.23695</td>
</tr>
<tr>
<td>model_v2</td>
<td>102</td>
<td>30 days</td>
<td>2 days</td>
<td>50:5:1</td>
<td>0.26951</td>
</tr>
<tr>
<td>model_v3</td>
<td>103</td>
<td>30 days</td>
<td>2 days</td>
<td>50:10:1</td>
<td>0.25782</td>
</tr>
</tbody>
</table>
<h5 id="七、模型迭代"><a href="#七、模型迭代" class="headerlink" title="七、模型迭代"></a>七、模型迭代</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;迭代调整模型，不同试验位置的模型调整策略也不一样。同时从上面的示例相互之间的比较可以看出：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) 增加训练集数量，离线指标有提升；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) 调整抽样比例，相对来说，抽样严格点比较好，可以减小噪声。前提是训练数据样本足够大；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) 权重设置：训练的正负样本权重设置，权重比例应调整为大致和样本中各个action占比正相关，并非绝对地增大/减小 ord/clk权重，符合样本比例即可。</p>
<h5 id="八、参考文献"><a href="#八、参考文献" class="headerlink" title="八、参考文献"></a>八、参考文献</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      离线特征工程和个性化推荐系统的离线推荐算法逻辑。
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Recommender System" scheme="http://www.dianacody.com/tags/Recommender-System/"/>
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
      <category term="pig" scheme="http://www.dianacody.com/tags/pig/"/>
    
  </entry>
  
  <entry>
    <title>个性化推荐系统（在线）</title>
    <link href="http://www.dianacody.com/2016/07/29/2016-07-29-RecOnline/"/>
    <id>http://www.dianacody.com/2016/07/29/2016-07-29-RecOnline/</id>
    <published>2016-07-28T16:00:00.000Z</published>
    <updated>2019-02-26T12:59:35.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、项目源码"><a href="#一、项目源码" class="headerlink" title="一、项目源码"></a>一、项目源码</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;项目源码地址：（此项目暂不开放源码）</p>
<h5 id="二、简介"><a href="#二、简介" class="headerlink" title="二、简介"></a>二、简介</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;获取个性化用户行为、冷启动数据模型，依据不同算法打分策略（线上+离线打分），从离线召回源选择合适数据，过滤排序召回集，作为线上推荐结果，上线AB-Test，观测实验数据bucket。</p>
<h5 id="三、线上逻辑方案"><a href="#三、线上逻辑方案" class="headerlink" title="三、线上逻辑方案"></a>三、线上逻辑方案</h5><p>流程结构大致如下所示：<br><img src="/resources/rec-online.jpg" alt="figure-rec-online"></p>
<h6 id="3-1-参数配置解析"><a href="#3-1-参数配置解析" class="headerlink" title="3.1 参数配置解析"></a>3.1 参数配置解析</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;包括配置文件参数解析，例如用户账号、用户配置文件、实验配置参数解析。</p>
<h6 id="3-2-用户模型提取"><a href="#3-2-用户模型提取" class="headerlink" title="3.2 用户模型提取"></a>3.2 用户模型提取</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从各个用户行为模型服务中获取模型，比如实时行为模型、长期模型例如用户画像等等，此外还有一些离线特征训练的模型。</p>
<h6 id="3-3-数据召回"><a href="#3-3-数据召回" class="headerlink" title="3.3 数据召回"></a>3.3 数据召回</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下各个步骤都是相互独立而互不干扰，因此可以并行运行。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于协同过滤，获取相似用户、相似商品的最近结果集，然后进行规则过滤，得到候选集数据列表；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于冷启动数据，从用户画像和商品画像中读取候选集，然后进行规则过滤，得到候选集数据列表；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于特定算法（此处略）的召回结果，从数据库读取，然后进行规则过滤，得到候选集数据列表。</p>
<h6 id="3-4-数据融合与排序"><a href="#3-4-数据融合与排序" class="headerlink" title="3.4 数据融合与排序"></a>3.4 数据融合与排序</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多模型融合，然后进行加权打分，然后根据不同集合的分数进行排序。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;加权质量分也可以通过机器学习的方式进行离线特征获取，同时机器学习可以参与在线特征计算。</p>
<h6 id="3-5-数据过滤"><a href="#3-5-数据过滤" class="headerlink" title="3.5 数据过滤"></a>3.5 数据过滤</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依据一些规则对排序后的集合进行过滤。</p>
<h6 id="3-6-数据穿插"><a href="#3-6-数据穿插" class="headerlink" title="3.6 数据穿插"></a>3.6 数据穿插</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依照不同分桶策略，进行类目穿插、品牌词穿插、产品词穿插等等，保证结果展示的多样性。</p>
<h6 id="3-7-结果展示"><a href="#3-7-结果展示" class="headerlink" title="3.7 结果展示"></a>3.7 结果展示</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;获取结果进行展示。</p>
<h5 id="四、参考文献"><a href="#四、参考文献" class="headerlink" title="四、参考文献"></a>四、参考文献</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      个性化推荐系统的离线推荐算法逻辑。
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Recommender System" scheme="http://www.dianacody.com/tags/Recommender-System/"/>
    
      <category term="Online" scheme="http://www.dianacody.com/tags/Online/"/>
    
      <category term="Java" scheme="http://www.dianacody.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>最大匹配算法扩展</title>
    <link href="http://www.dianacody.com/2014/11/16/2014-11-16-HMM2/"/>
    <id>http://www.dianacody.com/2014/11/16/2014-11-16-HMM2/</id>
    <published>2014-11-15T16:00:00.000Z</published>
    <updated>2018-12-03T04:14:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于简单的中文分词匹配法做了扩展，其中比较有名的就是台湾蔡志浩老师1996年写的“MMSEG: A Word Identification System for Mandarin Chinese Text Based on Two Variants of the Maximum Matching Algorithm”，在这篇文章的页面中，不仅介绍了相关的中文分词算法，并且提供了一个C版本的mmseg供研究使用，目前根据该文及其代码移植的mmseg程序版本包括C++版、Java版、Python版及Ruby版，影响甚广。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此文是英文版本，建议有条件的读者直接读原文。不过国内也有该文的简介文章：《MMSeg分词算法简述》，原文似乎出自www.solol.org。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MMSEG中文分词系统的可以由一句话总结：The system consisted of a lexicon, two matching algorithms, and four ambiguity resolution rules（该系统包括一个词典，两种匹配算法，以及四种歧义消解规则）：</p>
<h5 id="1-词典（The-Lexicon）"><a href="#1-词典（The-Lexicon）" class="headerlink" title="1.词典（The Lexicon）"></a>1.词典（The Lexicon）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分两种形式，对于单个汉字的汉语词，除了汉字本身外，还包括其统计频率（这个频率属于先验知识，可以来自于已经人工分好词的训练语料库），而对于二字长及以上的汉语词，只要词条本身就可以了。</p>
<h5 id="2-匹配算法（Matching-Algorithm）"><a href="#2-匹配算法（Matching-Algorithm）" class="headerlink" title="2.匹配算法（Matching Algorithm）"></a>2.匹配算法（Matching Algorithm）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) 简单匹配:对于字符串中的汉字Cn，用词典匹配以Cn开头的子串并查找所有可能的匹配；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) 复杂匹配:对于字符串中的汉字Cn，查找所有可能以Cn开头的三词chunks，无论第一个汉语词是否有歧义。</p>
<h5 id="3-歧义消解规则（Ambiguity-Resolution-Rules）"><a href="#3-歧义消解规则（Ambiguity-Resolution-Rules）" class="headerlink" title="3.歧义消解规则（Ambiguity Resolution Rules）"></a>3.歧义消解规则（Ambiguity Resolution Rules）</h5><h6 id="规则一：最大匹配-Maximum-matching"><a href="#规则一：最大匹配-Maximum-matching" class="headerlink" title="规则一：最大匹配(Maximum matching)"></a>规则一：最大匹配(Maximum matching)</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) 简单最大匹配算法,也就是我们常说的最大匹配法，不过作者采取的是正向匹配，并且按长度从小到大搜索词典：假设C1,C2,….代表一个字符串中的汉字，首先搜索词典，看 <em>C1</em>是否为一个单字组成的词语，然后搜索 <em>C1C2</em>来看是否为两个汉字组成的词语，以此类推，直至找到字典中最长的匹配。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) 复杂最大匹配算法,由Chen 和Liu（1992）提出，其核心的假设是：The most plausible segmentation is the three-word chunk with maximum length. 请注意three-word chunk，可以将其翻译为“三词语块”，这也是MMSEG中比较核心的一个概念，这个最大匹配规则考虑问题比较全面，在对句子中的某个词进行切分时，如果有歧义拿不定主意，就再向后展望两个汉语词，并且找出所有可能的“三词语块”。例如，对于如下的“三词语块”，请注意括号中是注明的语块长度（以汉语单字为基本单位）：</p>
<ul>
<li><em>C1</em> <em>C2</em> <em>C3C4</em>（4）</li>
<li><em>C1C2</em> <em>C3C4</em> <em>C5</em>（5）</li>
<li><em>C1C2</em> <em>C3C4</em> <em>C5C6</em>（6）</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最大长度的“三词语块”是第3个，所以其第一汉语词<em>C1C2</em>将被作为正确的分词形式。以此类推，接下来我们从C3开始，找出所有可能的“三词语块”，重复上述规则，直到句子的最后一个词被划分。直观一点，对于以“眼”开头的如下5个“三词语块”,利用该规则，则“眼看”是正确的词语划分：</p>
<ul>
<li>眼看 就要 来了（6）</li>
<li>眼看 就要 来（5）</li>
<li>眼看 就 要(4)</li>
<li>眼 看 就要(4)</li>
<li>眼 看 就(3)</li>
</ul>
<h6 id="规则二：最大平均词长（Largest-average-word-length）"><a href="#规则二：最大平均词长（Largest-average-word-length）" class="headerlink" title="规则二：最大平均词长（Largest average word length）"></a>规则二：最大平均词长（Largest average word length）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在句子的末尾，很可能得到的“三词语块”只有一个或两个词（其他位置补空），例如，对于如下两个“三词语块”，他们拥有同样的长度：</p>
<ul>
<li><em>C1</em> <em>C2</em> <em>C3</em>（平均词长=1）</li>
<li><em>C1C2C3</em>（平均词长=3）</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时规则1就无法解决其歧义消解问题，因此引入规则2：最大平均词长，也就是从这些语块中找出平均词长最大的语块，并选取其第一词语作为正确的词语切分形式。这个规则的前提假设是：It is more likely to encounter multi-character words than one-character words（在句子中遇到多字-词语的情况比单字-词语更有可能）.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，上述两个“三词语块”中第二个<em>C1C2C3</em>就是最佳候选。直观一点，对于如下位于句尾三种形式的“三词语块”：</p>
<ul>
<li>国际化（平均词长=3）</li>
<li>国际 化（平均词长=1.5）</li>
<li>国 际 化（平均词长=1）</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在规则1无法求解的情况下，根据规则2，则“国际化”为最佳候选语块，因此该语块的第一个词“国际化”就是最佳的分词形式。</p>
<h6 id="规则三：最小词长方差（Smallest-variance-of-word-lengths）"><a href="#规则三：最小词长方差（Smallest-variance-of-word-lengths）" class="headerlink" title="规则三：最小词长方差（Smallest variance of word lengths）"></a>规则三：最小词长方差（Smallest variance of word lengths）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;还有一些歧义是规则一和规则二无法解决的，例如，如下的两个“三词语块”拥有同样的长度和同样的平均词长：</p>
<ul>
<li><em>C1C2</em> <em>C3C4</em> <em>C5C6</em></li>
<li><em>C1C2C3</em> <em>C4</em> <em>C5C6</em></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此引入规则三：最小词长方差，也就是找出词长方差最小的语块，并选取其第一个词语作为正确的词语切分形式。在概率论和统计学中，一个随机变量的方差（Variance）描述的是它的离散程度，也就是该变量离其期望值的距离。因此该规则的前提假设是：Word lengths are usually evenly distributed（句子中的词语长度经常是均匀分布的）。直观来说，对于如下两个“三词语块”：</p>
<ul>
<li>研究 生命 起源</li>
<li>研究生 命 起源</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其长度为6，平均词长为2，规则一和规则二无能无力，利用规则三：</p>
<ul>
<li>语块1的方差 = ((2-2)^2+(2-2)^2+(2-2)^2)/3 = 0</li>
<li>语块2的方差 = ((3-2)^2+(1-2)^2+(2-2)^2)/3 = 2/3</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则语块1为最佳候选，因此该语块的第一个词“研究”为最佳的分词形式。</p>
<h6 id="规则四：最大单字词语语素自由度之和（Largest-sum-of-degree-of-morphemic-freedom-of-one-character-words）"><a href="#规则四：最大单字词语语素自由度之和（Largest-sum-of-degree-of-morphemic-freedom-of-one-character-words）" class="headerlink" title="规则四：最大单字词语语素自由度之和（Largest sum of degree of morphemic freedom of one-character words）"></a>规则四：最大单字词语语素自由度之和（Largest sum of degree of morphemic freedom of one-character words）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如下所示，例子中的两个“三词语块”拥有同样的长度、平均词长及方差，因此上述三个规则都无法解决其歧义消解问题：</p>
<ul>
<li><em>C1</em> <em>C2</em> <em>C3C4</em></li>
<li><em>C1</em> <em>C2C3</em> <em>C4</em></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这两个语块都包括了两个单字（one-character）词语和一个两字（two-character）词语，规则四主要关注其中的单字词语。直观来看，有些汉字很少作为词语出现，而另一些汉字则常常作为词语出现，从统计角度来看，在语料库中出现频率高的汉字就很可能是一个单字词语，反之可能性就小。计算单词词语语素自由度之和的公式是对“三词语块”中的单字词语频率取对数并求和（The formula used to calculate the sum of degree of morphemic freedom is to sum log(frequency) of all one-character word(s) in a chunk.）规则四则选取其中和最大的语块，并将该语块的第一词语作为最佳的词语切分形式。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于MMSEG中文分词系统的框架就介绍到此，需要指出的是：</p>
<p>“It has to be noted that MMSEG was not designed to be a “professional level” system whose goal is 100% correct identification. Rather, MMSEG should be viewed as a general platform on which new ambiguity resolution algorithms can be tested.”</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，不要认为有了MMSEG就可以解决中文分词的问题，更应该将MMSEG视为一个基本的平台，在该平台的基础上，可以尝试添加新的歧义消解算法以解决中文分词中的难点问题。</p>
]]></content>
    
    <summary type="html">
    
      基于简单的中文分词匹配法做了扩展，中文分词算法提供了一个C版本的mmseg供研究使用。
    
    </summary>
    
      <category term="NLP" scheme="http://www.dianacody.com/categories/NLP/"/>
    
    
      <category term="HMM" scheme="http://www.dianacody.com/tags/HMM/"/>
    
  </entry>
  
  <entry>
    <title>中文分词-最大匹配算法</title>
    <link href="http://www.dianacody.com/2014/11/07/2014-11-07-HMM/"/>
    <id>http://www.dianacody.com/2014/11/07/2014-11-07-HMM/</id>
    <published>2014-11-06T16:00:00.000Z</published>
    <updated>2018-04-30T10:47:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>正向最大匹配法算法如下所示:</p>
<p>逆向匹配法思想与正向一样，只是从右向左切分，这里举一个例子：</p>
<p>输入例句：S1=”计算语言学课程有意思” ；</p>
<p>定义：最大词长MaxLen = 5；S2= ” “；分隔符 = “/”；</p>
<p>假设存在词表：…，计算语言学，课程，意思，…；</p>
<p>最大逆向匹配分词算法过程如下：</p>
<ol>
<li><p>S2=””；S1不为空，从S1右边取出候选子串W=”课程有意思”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有意思”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”有意思”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”意思”</p>
</li>
<li><p>查词表，“意思”在词表中，将W加入到S2中，S2=” 意思/”，并将W从S1中去掉，此时S1=”计算语言学课程有”；</p>
</li>
<li><p>S1不为空，于是从S1左边取出候选子串W=”言学课程有”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程有”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程有”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”有”，这W是单字，将W加入到S2中，S2=“ /有 /意思”，并将W从S1中去掉，此时S1=”计算语言学课程”；</p>
</li>
<li><p>S1不为空，于是从S1左边取出候选子串W=”语言学课程”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”言学课程”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程”；</p>
</li>
<li><p>查词表，“意思”在词表中，将W加入到S2中，S2=“ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=”计算语言学”；</p>
</li>
<li><p>S1不为空，于是从S1左边取出候选子串W=”计算语言学”；</p>
</li>
<li><p>查词表，“计算语言学”在词表中，将W加入到S2中，S2=“计算语言学/ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=””；</p>
</li>
<li><p>S1为空，输出S2作为分词结果，分词过程结束。</p>
</li>
</ol>
<p>相应程序示例：</p>
<p>准备文件：建立一个词表文件wordlexicon，格式如下</p>
<ul>
<li>计算语言学</li>
<li>课程</li>
<li>意思</li>
</ul>
<p>输入文件：test,格式如下</p>
<ul>
<li>计算语言学课程有意思</li>
</ul>
<p>编译后执行如下：SegWord.exe test，输出分词结果文件：SegmentResult.txt</p>
<p><strong>源代码如下：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">// Dictionary.h</div><div class="line">#include &lt;iostream&gt;</div><div class="line">#include &lt;string&gt;</div><div class="line">#include &lt;fstream&gt;</div><div class="line">#include &lt;sstream&gt;</div><div class="line">#include &lt;hash_map&gt;</div><div class="line">using namespace std;</div><div class="line">using namespace stdext;</div><div class="line">class CDictionary</div><div class="line">&#123;</div><div class="line">	public:</div><div class="line">		CDictionary(); //将词典文件读入并构造为一个哈希词典</div><div class="line">		~CDictionary();</div><div class="line">		int FindWord(string w); //在哈希词典中查找词</div><div class="line">	private:</div><div class="line">		string strtmp; //读取词典的每一行</div><div class="line">		string word; //保存每个词</div><div class="line">		hash_map&lt;string, int&gt; wordhash; // 用于读取词典后的哈希</div><div class="line">		hash_map&lt;string, int &gt;::iterator worditer; //</div><div class="line">		typedef pair&lt;string, int&gt; sipair;</div><div class="line">&#125;;</div><div class="line">//将词典文件读入并构造为一个哈希词典</div><div class="line">CDictionary::CDictionary()</div><div class="line">&#123;</div><div class="line">	ifstream infile(“wordlexicon”); // 打开词典</div><div class="line">	if (!infile.is_open()) // 打开词典失败则退出程序</div><div class="line">	&#123;</div><div class="line">		cerr &lt;&lt; &quot;Unable to open input file: &quot; &lt;&lt; &quot;wordlexicon&quot;</div><div class="line">&lt;&lt; &quot; -- bailing out!&quot; &lt;&lt; endl;</div><div class="line">		exit(-1);</div><div class="line">	&#125;</div><div class="line">	while (getline(infile, strtmp, &apos;\\n&apos;)) // 读入词典的每一行并将其添加入哈希中</div><div class="line">	&#123;</div><div class="line">		istringstream istr(strtmp);</div><div class="line">		istr &gt;&gt; word; //读入每行第一个词</div><div class="line">		wordhash.insert(sipair(word, 1)); //插入到哈希中</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">CDictionary::~CDictionary()</div><div class="line">&#123;</div><div class="line">&#125;</div><div class="line"></div><div class="line">//在哈希词典中查找词，若找到，则返回，否则返回</div><div class="line">int CDictionary::FindWord(string w)</div><div class="line">&#123;</div><div class="line">	if (wordhash.find(w) != wordhash.end())</div><div class="line">	&#123;</div><div class="line">		return 1;</div><div class="line">	&#125;</div><div class="line">	else</div><div class="line">	&#123;</div><div class="line">		return 0;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>主程序main.cpp</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line">#include “Dictionary.h”</div><div class="line"># define MaxWordLength 10 // 最大词长为个字节（即个汉字）</div><div class="line"># define Separator “/ ” // 词界标记</div><div class="line">CDictionary WordDic; //初始化一个词典</div><div class="line">//对字符串用最大匹配法（正向或逆向）处理</div><div class="line">string SegmentSentence(string s1)</div><div class="line">&#123;</div><div class="line">	string s2 = “”; //用s2存放分词结果</div><div class="line">	while(!s1.empty())</div><div class="line">	&#123;</div><div class="line">		int len =(int) s1.length(); // 取输入串长度</div><div class="line">		if (len &gt; MaxWordLength) // 如果输入串长度大于最大词长</div><div class="line">		&#123;</div><div class="line">			len = MaxWordLength; // 只在最大词长范围内进行处理</div><div class="line">		&#125;</div><div class="line">		//string w = s1.substr(0, len); // （正向用）将输入串左边等于最大词长长度串取出作为候选词</div><div class="line">		string w = s1.substr(s1.length() – len, len); //逆向用</div><div class="line">		int n = WordDic.FindWord(w); // 在词典中查找相应的词</div><div class="line">		while(len &gt; 2 &amp;&amp; n == 0) // 如果不是词</div><div class="line">		&#123;</div><div class="line">			len -= 2; // 从候选词右边减掉一个汉字，将剩下的部分作为候选词</div><div class="line">			//w = w.substr(0, len); //正向用</div><div class="line">			w = s1.substr(s1.length() – len, len); //逆向用</div><div class="line">			n = WordDic.FindWord(w);</div><div class="line">		&#125;</div><div class="line">		//s2 += w + Separator; // (正向用）将匹配得到的词连同词界标记加到输出串末尾</div><div class="line">		w = w + Separator; // (逆向用)</div><div class="line">		s2 = w + s2 ; // (逆向用)</div><div class="line">		//s1 = s1.substr(w.length(), s1.length()); //(正向用)从s1-w处开始</div><div class="line">		s1 = s1.substr(0, s1.length() – len); // (逆向用)</div><div class="line">	&#125;</div><div class="line">	return s2;</div><div class="line">&#125;</div><div class="line">//对句子进行最大匹配法处理，包含对特殊字符的处理</div><div class="line">string SegmentSentenceMM (string s1)</div><div class="line">&#123;</div><div class="line">	string s2 = “”; //用s2存放分词结果</div><div class="line">	int i;</div><div class="line">	int dd;</div><div class="line">	while(!s1.empty() )</div><div class="line">	&#123;</div><div class="line">		unsigned char ch = (unsigned char)s1[0];</div><div class="line">		if (ch &lt; 128) // 处理西文字符</div><div class="line">		&#123;</div><div class="line">			i = 1;</div><div class="line">			dd = (int)s1.length();</div><div class="line">			while (i &lt; dd &amp;&amp; ((unsigned char)s1[i] &lt; 128) &amp;&amp; (s1[i] != 10) &amp;&amp; (s1[i] != 13)) // s1[i]不能是换行符或回车符</div><div class="line">			&#123;</div><div class="line">				i++;</div><div class="line">			&#125;</div><div class="line">			if ((ch != 32) &amp;&amp; (ch != 10) &amp;&amp; (ch != 13)) // 如果不是西文空格或换行或回车符</div><div class="line">			&#123;</div><div class="line">				s2 += s1.substr(0,i) + Separator;</div><div class="line">			&#125;</div><div class="line">			else</div><div class="line">			&#123;</div><div class="line">				//if (ch == 10 || ch == 13) // 如果是换行或回车符，将它拷贝给s2输出</div><div class="line">				if (ch == 10 || ch == 13 || ch == 32) //谢谢读者mces89的指正</div><div class="line">				&#123;</div><div class="line">					s2 += s1.substr(0, i);</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">			s1 = s1.substr(i,dd);</div><div class="line">			continue;</div><div class="line">		&#125;</div><div class="line">		else</div><div class="line">		&#123;</div><div class="line">			if (ch &lt; 176) // 中文标点等非汉字字符</div><div class="line">			&#123;</div><div class="line">				i = 0;</div><div class="line">				dd = (int)s1.length();</div><div class="line">				while(i &lt; dd &amp;&amp; ((unsigned char)s1[i] &lt; 176) &amp;&amp; ((unsigned char)s1[i] &gt;= 161)</div><div class="line">				&amp;&amp; (!((unsigned char)s1[i] == 161 &amp;&amp; ((unsigned char)s1[i+1] &gt;= 162 &amp;&amp; (unsigned char)s1[i+1] &lt;= 168)))</div><div class="line">				&amp;&amp; (!((unsigned char)s1[i] == 161 &amp;&amp; ((unsigned char)s1[i+1] &gt;= 171 &amp;&amp; (unsigned char)s1[i+1] &lt;= 191)))</div><div class="line">				&amp;&amp; (!((unsigned char)s1[i] == 163 &amp;&amp; ((unsigned char)s1[i+1] == 172 || (unsigned char)s1[i+1] == 161)</div><div class="line">				|| (unsigned char)s1[i+1] == 168 || (unsigned char)s1[i+1] == 169 || (unsigned char)s1[i+1] == 186</div><div class="line">				|| (unsigned char)s1[i+1] == 187 || (unsigned char)s1[i+1] == 191)))</div><div class="line">			&#123;</div><div class="line">				i = i + 2; // 假定没有半个汉字</div><div class="line">			&#125;</div><div class="line">			if (i == 0)</div><div class="line">			&#123;</div><div class="line">				i = i + 2;</div><div class="line">			&#125;</div><div class="line">			if (!(ch == 161 &amp;&amp; (unsigned char)s1[1] == 161)) // 不处理中文空格</div><div class="line">			&#123;</div><div class="line">				s2+=s1.substr(0, i) + Separator; // 其他的非汉字双字节字符可能连续输出</div><div class="line">			&#125;</div><div class="line">			s1 = s1.substr(i, dd);</div><div class="line">			continue;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		// 以下处理汉字串</div><div class="line">		i = 2;</div><div class="line">		dd = (int)s1.length();</div><div class="line">		while(i &lt; dd &amp;&amp; (unsigned char)s1[i] &gt;= 176)</div><div class="line">		&#123;</div><div class="line">			i += 2;</div><div class="line">		&#125;</div><div class="line">		s2 += SegmentSentence(s1.substr(0, i));</div><div class="line">		s1 = s1.substr(i,dd);</div><div class="line">	&#125;</div><div class="line">	return s2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">int main(int argc, char *argv[])</div><div class="line">&#123;</div><div class="line">	string strtmp; //用于保存从语料库中读入的每一行</div><div class="line">	string line; //用于输出每一行的结果</div><div class="line">	ifstream infile(argv[1]); // 打开输入文件</div><div class="line">	if (!infile.is_open()) // 打开输入文件失败则退出程序</div><div class="line">	&#123;</div><div class="line">		cerr &lt;&lt; &quot;Unable to open input file: &quot; &lt;&lt; argv[1]</div><div class="line">&lt;&lt; &quot; -- bailing out!&quot; &lt;&lt; endl;</div><div class="line">		exit(-1);</div><div class="line">&#125;</div><div class="line">	ofstream outfile1(&quot;SegmentResult.txt&quot;); //确定输出文件</div><div class="line">	if (!outfile1.is_open())</div><div class="line">	&#123;</div><div class="line">		cerr &lt;&lt; &quot;Unable to open file：SegmentResult.txt&quot;</div><div class="line">&lt;&lt; &quot;--bailing out!&quot; &lt;&lt; endl;</div><div class="line">		exit(-1);</div><div class="line">	&#125;</div><div class="line">	while (getline(infile, strtmp, &apos;n&apos;)) //读入语料库中的每一行并用最大匹配法处理</div><div class="line">	&#123;</div><div class="line">		line = strtmp;</div><div class="line">		line = SegmentSentenceMM(line); // 调用分词函数进行分词处理</div><div class="line">		outfile1 &lt;&lt; line &lt;&lt; endl; // 将分词结果写入目标文件</div><div class="line">	&#125;</div><div class="line">	return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>补充说明：如果使用正向匹配法，请将源代码中的相关注释 “//“互换。</p>
]]></content>
    
    <summary type="html">
    
      中文分词在中文信息处理中是最最基础的，无论机器翻译亦或信息检索还是其他相关应用，如果涉及中文，都离不开中文分词，因此中文分词具有极高的地位。中文分词入门最简单应该是最大匹配法了。
    
    </summary>
    
      <category term="NLP" scheme="http://www.dianacody.com/categories/NLP/"/>
    
    
      <category term="HMM" scheme="http://www.dianacody.com/tags/HMM/"/>
    
  </entry>
  
  <entry>
    <title>中文分词：原理及分词算法</title>
    <link href="http://www.dianacody.com/2014/11/05/2014-11-05-cn_cutwords/"/>
    <id>http://www.dianacody.com/2014/11/05/2014-11-05-cn_cutwords/</id>
    <published>2014-11-04T16:00:00.000Z</published>
    <updated>2018-12-03T03:35:28.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、中文分词"><a href="#一、中文分词" class="headerlink" title="一、中文分词"></a>一、中文分词</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文信息处理的基础与关键。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lucene中对中文的处理是基于自动切分的单字切分，或者二元切分。除此之外，还有最大切分（包括向前、向后、以及前后相结合）、最少切分、全切分等等。</p>
<h5 id="二、中文分词技术分类"><a href="#二、中文分词技术分类" class="headerlink" title="二、中文分词技术分类"></a>二、中文分词技术分类</h5><p>我们讨论的分词算法可分为三大类：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.<strong> 基于词典 </strong>：基于字典、词库匹配的分词方法；（字符串匹配、机械分词法）<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.<strong> 基于统计 </strong>：基于词频度统计的分词方法；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.<strong> 基于规则 </strong>：基于知识理解的分词方法。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一类方法应用词典匹配、汉语词法或其它汉语语言知识进行分词，如：最大匹配法、最小分词方法等。这类方法简单、分词效率较高,但汉语语言现象复杂丰富，词典的完备性、规则的一致性等问题使其难以适应开放的大规模文本的分词处理。第二类基于统计的分词方法则基于字和词的统计信息，如把相邻字间的信息、词频及相应的共现信息等应用于分词，由于这些信息是通过调查真实语料而取得的，因而基于统计的分词方法具有较好的实用性。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面简要介绍几种常用方法：</p>
<h5 id="三、基于词典分词"><a href="#三、基于词典分词" class="headerlink" title="三、基于词典分词"></a>三、基于词典分词</h5><h6 id="3-1-逐词遍历法"><a href="#3-1-逐词遍历法" class="headerlink" title="3.1 逐词遍历法"></a>3.1 逐词遍历法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;逐词遍历法将词典中的所有词按由长到短的顺序在文章中逐字搜索,直至文章结束。也就是说，不管文章有多短，词典有多大，都要将词典遍历一遍。这种方法效率比较低，大一点的系统一般都不使用。</p>
<h6 id="3-2-基于字典、词库匹配的分词方法（机械分词法）"><a href="#3-2-基于字典、词库匹配的分词方法（机械分词法）" class="headerlink" title="3.2 基于字典、词库匹配的分词方法（机械分词法）"></a>3.2 基于字典、词库匹配的分词方法（机械分词法）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种方法按照一定策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。识别出一个词，根据扫描方向的不同分为正向匹配和逆向匹配。根据不同长度优先匹配的情况，分为最大（最长）匹配和最小（最短）匹配。根据与词性标注过程是否相结合，又可以分为单纯分词方法和分词与标注相结合的一体化方法。常用的方法如下：</p>
<h6 id="1-最大正向匹配法-MM-MaximumMatching-Method"><a href="#1-最大正向匹配法-MM-MaximumMatching-Method" class="headerlink" title="(1)最大正向匹配法(MM, MaximumMatching Method)"></a>(1)最大正向匹配法(MM, MaximumMatching Method)</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常简称为MM法。其基本思想为：假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理……  如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其算法描述如下：<br><strong> step1 </strong>: 从左向右取待切分汉语句的m个字符作为匹配字段，m为大机器词典中最长词条个数。<br><strong> step2 </strong>: 查找大机器词典并进行匹配。若匹配成功，则将这个匹配字段作为一个词切分出来。若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配，重复以上过程，直到切分出所有词为止。</p>
<h6 id="2-逆向最大匹配法-ReverseMaximum-Matching-Method"><a href="#2-逆向最大匹配法-ReverseMaximum-Matching-Method" class="headerlink" title="(2)逆向最大匹配法(ReverseMaximum Matching Method)"></a>(2)逆向最大匹配法(ReverseMaximum Matching Method)</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常简称为<strong>RMM法</strong>。RMM法的基本原理与MM法相同 ,不同的是分词切分的方向与MM法相反，而且使用的分词辞典也不同。逆向最大匹配法从被处理文档的末端开始匹配扫描，每次取最末端的2i个字符（i字字串）作为匹配字段，若匹配失败，则去掉匹配字段最前面的一个字，继续匹配。相应地，它使用的分词词典是逆序词典，其中的每个词条都将按逆序方式存放。在实际处理时，先将文档进行倒排处理，生成逆序文档。然后，根据逆序词典，对逆序文档用正向最大匹配法处理即可。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。所以，逆向最大匹配法比正向最大匹配法的误差要小。统计结果表明 ,单纯使用正向最大匹配的错误率为 1/16 9,单纯使用逆向最大匹配的错误率为 1/245。例如切分字段“硕士研究生产”，正向最大匹配法的结果会是“硕士研究生 / 产”，而逆向最大匹配法利用逆向扫描，可得到正确的分词结果“硕士 / 研究 / 生产”。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然，最大匹配算法是一种基于分词词典的机械分词法，不能根据文档上下文的语义特征来切分词语，对词典的依赖性较大，所以在实际使用时，难免会造成一些分词错误，为了提高系统分词的准确度，可以采用正向最大匹配法和逆向最大匹配法相结合的分词方案（见“双向匹配法”）</p>
<h6 id="3-最少切分法"><a href="#3-最少切分法" class="headerlink" title="(3)最少切分法"></a>(3)最少切分法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使每一句中切出的词数最小。</p>
<h6 id="4-双向匹配法"><a href="#4-双向匹配法" class="headerlink" title="(4)双向匹配法"></a>(4)双向匹配法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将正向最大匹配法与逆向最大匹配法组合。先根据标点对文档进行粗切分，把文档分解成若干个句子，然后再对这些句子用正向最大匹配法和逆向最大匹配法进行扫描切分。如果两种分词方法得到的匹配结果相同，则认为分词正确，否则，按最小集处理。</p>
<h5 id="四、全切分-amp-基于词的频度统计（无字典分词）"><a href="#四、全切分-amp-基于词的频度统计（无字典分词）" class="headerlink" title="四、全切分 &amp; 基于词的频度统计（无字典分词）"></a>四、全切分 &amp; 基于词的频度统计（无字典分词）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于词的频度统计的分词方法是一种全切分方法。在讨论这个方法之前我们先要明白有关全切分的相关内容。</p>
<h6 id="4-1-全切分"><a href="#4-1-全切分" class="headerlink" title="4.1 全切分"></a>4.1 全切分</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;全切分要求获得输入序列的所有可接受的切分形式，而部分切分只取得一种或几种可接受的切分形式，由于部分切分忽略了可能的其他切分形式，所以建立在部分切分基础上的分词方法不管采取何种歧义纠正策略，都可能会遗漏正确的切分，造成分词错误或失败。而建立在全切分基础上的分词方法，由于全切分取得了所有可能的切分形式，因而从根本上避免了可能切分形式的遗漏，克服了部分切分方法的缺陷。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;全切分算法能取得所有可能的切分形式，它的句子覆盖率和分词覆盖率均为100%，但全切分分词并没有在文本处理中广泛地采用，原因有以下几点：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.全切分算法只是能获得正确分词的前提，因为全切分不具有歧义检测功能，最终分词结果的正确性和完全性依赖于独立的歧义处理方法，如果评测有误，也会造成错误的结果。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.全切分的切分结果个数随句子长度的增长呈指数增长，一方面将导致庞大的无用数据充斥于存储数据库；另一方面当句长达到一定长度后，由于切分形式过多,造成分词效率严重下降。</p>
<h6 id="4-2-基于词的频度统计的分词方法"><a href="#4-2-基于词的频度统计的分词方法" class="headerlink" title="4.2 基于词的频度统计的分词方法"></a>4.2 基于词的频度统计的分词方法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要思想：上下文中，相邻的字同时出现的次数越多，就越可能构成一个词。因此字与字相邻出现的概率或频率能较好的反映词的可信度。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要统计模型为：<strong>N元文法模型（N-gram）、隐马尔科夫模型(Hidden Markov Model, HMM)</strong>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>HMM马尔科夫假设</strong>：一个词的出现仅仅依赖于它前面出现的有限的一个或者几个词。如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram。即<code class="code">P(T) = P(W1W2W3…Wn) = P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1) ≈ P(W1)P(W2|W1)P(W3|W2)…P(Wn|Wn-1)</code><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在实践中用的最多的就是bigram和trigram了，而且效果很不错。高于四元的用的很少，因为训练它需要更庞大的语料，而且数据稀疏严重，时间复杂度高，精度却提高的不多。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设w1,w2,w3,…,wn是长度为n的字符串，规定任意词wi只与它的前两个相关，得到三元概率模型。以此类推，N元模型就是假设当前词的出现概率只同它前面的N-1个词有关。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这是一种全切分方法。它不依靠词典,而是将文章中任意两个字同时出现的频率进行统计,次数越高的就可能是一个词。它首先切分出与词表匹配的所有可能的词,运用统计语言模型和决策算法决定最优的切分结果。它的优点在于可以发现所有的切分歧义并且容易将新词提取出来。</p>
<h5 id="五、基于统计分词（无字典分词）"><a href="#五、基于统计分词（无字典分词）" class="headerlink" title="五、基于统计分词（无字典分词）"></a>五、基于统计分词（无字典分词）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该方法主要基于句法、语法分析，并结合语义分析，通过对上下文内容所提供信息的分析对词进行定界，它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断。这类方法试图让机器具有人类的理解能力，需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式。因此目前基于知识的分词系统还处在试验阶段。</p>
<h5 id="六、并行分词法"><a href="#六、并行分词法" class="headerlink" title="六、并行分词法"></a>六、并行分词法</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种分词方法借助于一个含有分词词库的管道进行 ,比较匹配过程是分步进行的 ,每一步可以对进入管道中的词同时与词库中相应的词进行比较 ,由于同时有多个词进行比较匹配 ,因而分词速度可以大幅度提高。这种方法涉及到多级内码理论和管道的词典数据结构。（详细算法可以参考吴胜远的《并行分词方法的研究》）</p>
]]></content>
    
    <summary type="html">
    
      词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，Lucene中对中文的处理是基于自动切分的单字切分，或者二元切分。除此之外，还有最大切分（包括向前、向后、以及前后相结合）、最少切分、全切分等等。
    
    </summary>
    
      <category term="NLP" scheme="http://www.dianacody.com/categories/NLP/"/>
    
    
      <category term="Chinese Segmentation" scheme="http://www.dianacody.com/tags/Chinese-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>主题模型（二）：pLSA和LDA</title>
    <link href="http://www.dianacody.com/2014/11/04/2014-11-04-Theme2_pLSA,%20LDA/"/>
    <id>http://www.dianacody.com/2014/11/04/2014-11-04-Theme2_pLSA, LDA/</id>
    <published>2014-11-03T16:00:00.000Z</published>
    <updated>2018-12-03T04:15:13.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、pLSA（概率潜在语义分析）"><a href="#一、pLSA（概率潜在语义分析）" class="headerlink" title="一、pLSA（概率潜在语义分析）"></a>一、pLSA（概率潜在语义分析）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pLSA有过拟合问题，就是求D, Z, W。pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在论文《Google News Personalization Scalable Online CF》一文中提级针对用户聚类，利用相似用户性信息计算喜欢的news。其中包含min-hash以及plsi，plsi是model-based 推荐算法，属于topic(aspect) model，其在NLP领域中用途很大。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;引入：在文本挖掘时，计算文档相似性是很基础的操作，通常，对文本进行分词，构建VSM，通过jaccard或者cosin计算距离或者相似性，这是基于corpus的思路，仅仅考虑词组，并未考虑文本的语义信息。针对下面情况，基于cropus很难处理：</p>
<ul>
<li>如果时间回到2006年，马云和杨致远的手还会握在一起吗</li>
<li>阿里巴巴集团和雅虎就股权回购一事签署了最终协议</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果采用基于corpus的jaccard距离等算法，那么这两个文本的完全不相关，但是事实上，马云和阿里巴巴集团，杨致远和雅虎有着密切的联系，从语义上看，两者都和“阿里巴巴”有关系。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此外，另一个case：</p>
<ul>
<li>富士苹果真好，赶快买</li>
<li>苹果四代真好，赶快买</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从corpus上来看，两者非常相似，但是事实上，2个句子从语义上来讲，没有任何关系，一个是”水果“另一个是”手机”。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过上面的例子，差不多也看出来topic model是什么以及解决什么问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;概念：topic model是针对文本隐含主题的建模方法，针对第一个case，马云对应的主题是阿里巴巴，阿里巴巴集团也隐含阿里巴巴主题，这样两个文本的主题匹配上，认为他们是相关的，针对第二个，分别针对水果以及手机主题，我们认为他们是不相关的。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;究竟什么是主题？[接下来参考baidu搜索研发部官方博客中对语义主题的定义]主题就是一个概念、一个方面。它表现为一系列相关的词，能够代表这个主题。比如如果是”阿里巴巴“主题，那么”马云“”电子商务“等词会很高的频率出现，而设计到“腾讯”主题，那么“马化腾”“游戏”“QQ”会以较高的频率出现。如果用数学来描述一下的话，主题就是词汇表上词语的条件概率分布，与主题密切相关的词，条件概率p(w|z)越大。主题就像一个桶，装了出现频率很高的词语，这些词语和主题有很强的相关性，或者说这些词语定义了这个主题。同时，一个词语，可能来自于这个桶，也可能来自那个桶，比如“电子商务”可以来自“阿里巴巴”主题，也可以来自“京东“主题，所以一段文字往往包含多个主题，也就是说，一段文字不只有一个主题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面介绍了主题的概念，我们最为关心的是如何得到这些主题？这就是topic model要解决的问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;define： d表示文档，w表示词语，z表示隐含的主题。其中 p(w|d)表示w在文档d中出现的概率，针对训练语料，对文本进行分词，w的频度除以文档所有词语的频度和，可以求出，对于未知数据，model用来计算该value.p(w|z)表示在给定主题情况下词语的出现的概率是多少，刻画词语和主题的相关程度。p(z|d)表示文档中每个主题出现的概率。所以主题模型就是：利用大量已知的p(w|d)词语-文档信息，训练出来主题-文档p(z|d)以及词语-主题p(w|z)。</p>
<h6 id="1-1-plsa模型"><a href="#1-1-plsa模型" class="headerlink" title="1.1 plsa模型"></a>1.1 plsa模型</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;plsa是一种topic model，它属于生成模型(不是很理解)，给定文档d后，以一定的概率选择d对应的主题z，然后以一定概率选择z中的词语w.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;plsa提供了一种模型求解的方法，采用之前介绍的EM算法，EM算法在之前已经介绍，现在不作处理，直接利用EM信息对topic model进行求解。</p>
<h6 id="1-2-主题模型的用途"><a href="#1-2-主题模型的用途" class="headerlink" title="1.2 主题模型的用途"></a>1.2 主题模型的用途</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.计算文本的相似性，考虑到文本语义，更好的刻画文本相似性，避免多义词，同义词的影响。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.文本聚类，用户聚类(RS)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.去除噪音，只保留最重要的主题，更好的刻画文档</p>
<h6 id="1-3-plsa在推荐系统中的应用"><a href="#1-3-plsa在推荐系统中的应用" class="headerlink" title="1.3 plsa在推荐系统中的应用"></a>1.3 plsa在推荐系统中的应用</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面介绍的是文档和词语的关系，映射到推荐系统中，表示为用户和ITEM的关系，ITEM可以使网，视频等。这样可以看出来描述的完全是同样的问题，求解p(s|u)=∑zp(s|z)p(z|u)，模型参数为p(s|z)?p(z|u)，里面上面的推导过程可以求得。<br>具体的可以参考：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unsupervised learning by probabilisticlatent semantic analysis<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Latent Semantic Models for collaborativefiltering<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Google News Personalization Scalable Online CF</p>
<h5 id="二、LDA（潜在狄瑞雷克模型）"><a href="#二、LDA（潜在狄瑞雷克模型）" class="headerlink" title="二、LDA（潜在狄瑞雷克模型）"></a>二、LDA（潜在狄瑞雷克模型）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和pLSA不同的是LDA中假设了很多先验分布（Dirichlet），且一般参数的先验分布都假设为Dirichlet分布，其原因是共轭分布时先验概率和后验概率的形式相同。</p>
]]></content>
    
    <summary type="html">
    
      pLSA由LSA发展过来，而早期LSA的实现主要是通过SVD分解。和pLSA不同的是LDA中假设了很多先验分布（Dirichlet），且一般参数的先验分布都假设为Dirichlet分布。
    
    </summary>
    
      <category term="NLP" scheme="http://www.dianacody.com/categories/NLP/"/>
    
    
      <category term="pLSA" scheme="http://www.dianacody.com/tags/pLSA/"/>
    
      <category term="LDA" scheme="http://www.dianacody.com/tags/LDA/"/>
    
  </entry>
  
  <entry>
    <title>主题模型（一）：条件概率、矩阵分解</title>
    <link href="http://www.dianacody.com/2014/11/03/2014-11-03-Theme1_matrix/"/>
    <id>http://www.dianacody.com/2014/11/03/2014-11-03-Theme1_matrix/</id>
    <published>2014-11-02T16:00:00.000Z</published>
    <updated>2018-12-03T03:48:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>主题模型</strong>训练推理方法主要有2种：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) pLSA→EM（期望最大化）<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) LDA → Gibbs Sampling抽样方法（计算量大,单精确）、变分贝叶斯推断法（计算量小,精度弱）<br>概率矩阵：<strong>p(词语|文档) =∑p(词语|主题)× p(主题|文档)</strong>，<strong> C = Φ × Θ </strong><br>在EM（最大期望）过程中：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) E过程：由贝叶斯可从Φ算到Θ<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) M过程：由贝叶斯可从Θ算到Φ<br>两者迭代，最终收敛（矩阵趋于均分）</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;设有两个句子，想知道它们之间是否相关联：第一个是：“乔布斯离我们而去了。”第二个是：“苹果价格会不会降？”如果由人来判断，一看就知道，这两个句子之间虽然没有任何公共词语，但仍然是很相关的。因为虽然第二句中的“苹果”可能是指吃的苹果，但是由于第一句里面有了“乔布斯”，我们会很自然的把“苹果”理解为苹果公司的产品。事实上，这种文字语句之间的相关性、相似性问题在搜索引擎算法中经常遇到。例如，一个用户输入了一个query，我们要从海量的网页库中找出和它最相关的结果。这里就涉及到如何衡量query和网页之间相似度的问题。对于这类问题，人是可以通过上下文语境来判断的。但是，机器可以么？<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在传统信息检索领域里，实际上已经有了很多衡量文档相似性的方法，比如经典的VSM模型。然而这些方法往往基于一个基本假设：文档之间重复的词语越多越可能相似。这一点在实际中并不尽然。很多时候相关程度取决于背后的语义联系，而非表面的词语重复。那么，这种语义关系应该怎样度量呢？事实上在自然语言处理领域里已经有了很多从词、词组、句子、篇章角度进行衡量的方法。本文要介绍的是其中一个语义挖掘的利器：<strong>主题模型</strong>。</p>
<h5 id="一、主题模型定义"><a href="#一、主题模型定义" class="headerlink" title="一、主题模型定义"></a>一、主题模型定义</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主题模型，顾名思义，就是对文字中隐含主题的一种建模方法。还是上面的例子，“苹果”这个词的背后既包含是苹果公司这样一个主题，也包括了水果的主题。当我们和第一句进行比较时，苹果公司这个主题就和“乔布斯”所代表的主题匹配上了，因而我们认为它们是相关的。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于主题定义：主题就是一个概念、一个方面。它表现为一系列相关的词语。比如一个文章如果涉及到“百度”这个主题，那么“中文搜索”、“李彦宏”等词语就会以较高的频率出现，而如果涉及到“IBM”这个主题，那么“笔记本”等就会出现的很频繁。如果用数学来描述一下的话，主题就是词汇表上词语的条件概率分布 。与主题关系越密切的词语，它的条件概率越大，反之则越小。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上是从互联网新闻中摘抄下来的一段话。划分了4个桶（主题），百度（红色），微软（紫色）、谷歌（蓝色）和市场（绿色）。段落中所包含的每个主题的词语用颜色标识出来。从颜色分布上我们就可以看出，文字的大意是在讲百度和市场发展。在这里面，谷歌、微软这两个主题也出现了，但不是主要语义。值得注意的是，像“搜索引擎”这样的词语，在百度、微软、谷歌这三个主题上都是很可能出现的，可以认为一个词语放进了多个“桶”。当它在文字中出现的时候，这三个主题均有一定程度的体现。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如何得到这些主题？对文章中的主题又是如何进行分析？这正是主题模型要解决的问题。主题模型如何工作？</p>
<h5 id="二、主题模型工作原理"><a href="#二、主题模型工作原理" class="headerlink" title="二、主题模型工作原理"></a>二、主题模型工作原理</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先，用生成模型的视角来看文档和主题这两件事。所谓<strong>生成模型</strong>就是认为一篇文章的每个词都是通过“<strong>以一定概率选择了某个主题，并从这个主题中以一定概率选择某个词语</strong>”这样一个过程得到的。<br>假如有很多的文档，比如大量的网页，先对所有文档进行分词，得到一个词汇列表。这样每篇文档就可以表示为一个词语的集合。对于每个词语，可以用它在文档中出现的次数除以文档中词语的数目作为它在文档中出现的概率p(词语|文档) 。这样对任意一篇文档，左边的C矩阵是已知的，右边的两个矩阵未知。<strong>而主题模型就是用大量已知的“词语－文档”C矩阵 ，通过一系列的训练，推理出右边的“词语－主题”矩阵Φ 和“主题文档”矩阵Θ </strong>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主题模型训练推理的方法主要有两种，一个是pLSA（Probabilistic Latent Semantic Analysis），另一个是LDA（Latent Dirichlet Allocation）。pLSA主要使用的是EM（期望最大化）算法；LDA采用的是Gibbssampling方法。由于它们都较为复杂且篇幅有限，这里就只简要地介绍一下pLSA的思想，其他具体方法和公式，读者可以查阅相关资料。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pLSA采用的方法叫做EM（期望最大化）算法，它包含两个不断迭代的过程：E（期望）过程和M（最大化）过程。用一个形象的例子来说吧：比如说食堂的大师傅炒了一盘菜，要等分成两份给两个人吃，显然没有必要拿天平去一点点去精确称量，最简单的办法是先随意的把菜分到两个碗中，然后观察是否一样多，把比较多的那一份取出一点放到另一个碗中，这个过程一直重复下去，直到大家看不出两个碗里的菜有什么差别为止。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于主题模型训练来说，“计算每个主题里的词语分布”和“计算训练文档中的主题分布”就好比是在往两个人碗里分饭。在E过程中，我们通过贝叶斯公式可以由“词语－主题”矩阵计算出“主题－文档”矩阵。在M过程中，我们再用“主题－文档”矩阵重新计算“词语－主题”矩阵。这个过程一直这样迭代下去。EM算法的神奇之处就在于它可以保证这个迭代过程是收敛的。也就是说，我们在反复迭代之后，就一定可以得到趋向于真实值的Φ和Θ。</p>
<h5 id="三、主题模型应用"><a href="#三、主题模型应用" class="headerlink" title="三、主题模型应用"></a>三、主题模型应用</h5><p>有了主题模型，如何使用？以及优缺点？主要是以下几点：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) 可以衡量文档之间的语义相似性。对于一篇文档，我们求出来的主题分布可以看作是对它的一个抽象表示。对于概率分布，我们可以通过一些距离公式（比如KL距离）来计算出两篇文档的语义距离，从而得到它们之间的相似度。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) 可以解决多义词的问题。回想最开始的例子，“苹果”可能是水果，也可能指苹果公司。通过求出来的“词语－主题”概率分布，就可以知道“苹果”都属于哪些主题，就可以通过主题的匹配来计算它与其他文字之间的相似度。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) 可以排除文档中噪音的影响。一般来说，文档中的噪音往往处于次要主题中，我们可以把它们忽略掉，只保持文档中最主要的主题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4) 它是无监督、完全自动化。只需要提供训练文档，它就可以自动训练出各种概率，无需任何人工标注过程。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5) 跟语言无关。任何语言只要能够对它进行分词，就可以进行训练，得到它的主题分布。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;综上所述，主题模型是一个能够挖掘语言背后隐含信息的利器。近些年来各大搜索引擎公司都已经开始重视这方面的研发工作。语义分析的技术正在逐步深入到搜索领域的各个产品中去。以后的搜索会趋于更加智能化。</p>
<h5 id="四、LSA（潜在语义分析）"><a href="#四、LSA（潜在语义分析）" class="headerlink" title="四、LSA（潜在语义分析）"></a>四、LSA（潜在语义分析）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;鉴于TF-IDF存在一些缺点，Deerwester等人于1990年提出潜在语义分析（LatentSemanticAnalysis）模型，用于挖掘文档与词语之间隐含的潜在语义关联。LSA的理论基础是数学中的奇异值矩阵分解（SVD）技术。</p>
<h6 id="4-1-PLSA（基于概率的潜在语义分析）"><a href="#4-1-PLSA（基于概率的潜在语义分析）" class="headerlink" title="4.1 PLSA（基于概率的潜在语义分析）"></a>4.1 PLSA（基于概率的潜在语义分析）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;鉴于LSA存在一些缺点，Hofmann等人于1999年提出一种基于概率的潜在语义分析（Probabilistic Latent SemanticAnalysis）模型。PLSA继承了“潜在语义”的概念，通过“统一的潜在语义空间”（也就是Blei等人于2003年正式提出Topic概念）来关联词与文档；通过引入概率统计的思想，避免了SVD的复杂计算。在PLSA中，各个因素（文档、潜在语义空间、词）之间的概率分布求解是最重要的，EM算法是常用的方法。PLSA也存在一些缺点：概率模型不够完备；随着文档和词的个数的增加，模型变得越来越庞大；在文档层面没有一个统计模型；EM算法需要反复迭代，计算量也很大。</p>
<h6 id="4-2-LDA（潜在狄瑞雷克模型）"><a href="#4-2-LDA（潜在狄瑞雷克模型）" class="headerlink" title="4.2 LDA（潜在狄瑞雷克模型）"></a>4.2 LDA（潜在狄瑞雷克模型）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;鉴于PLSA的缺点，Blei等人于2003年进一步提出新的主题模型LDA（LatentDirichletAllocation），它是一个层次贝叶斯模型，把模型的参数也看作随机变量，从而可以引入控制参数的参数，实现彻底的“概率化”。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;是LDA模型的Dirichlet的先验分布，表示整个文档集上主题的分布；表示文档d上主题的多项式分布；Z表示文档d的第n个词的主题；W表示文档d的第n个词；N表示文档d所包含词的个数；D表示文档集；K表示主题集；表示主题k上词语的多项式分布；表示所有主题上次的先验分布。事实上，去掉和 ，LDA就变成了PLSA。目前，参数估计是LDA最重要的任务，主要有两种方法：Gibbs抽样法（计算量大，但相对简单和精确）和变分贝叶斯推断法（计算量小，精度度弱）。</p>
<h6 id="4-3-其他基于topic-model的演变"><a href="#4-3-其他基于topic-model的演变" class="headerlink" title="4.3 其他基于topic model的演变"></a>4.3 其他基于topic model的演变</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a）<strong>考虑上下文信息</strong>：例如，“上下文相关的概率潜在语义分析模型（ContextualProbabilistic LatentSemantic Analysis，CPLSA）”将词语上下文信息引入PLSA；也有研究人员考虑“地理位置”上下文信息，从地理位置相关的文档中发现地理位置关联的Topic。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b）<strong>主题模型演化</strong>：引入文本语料的时间信息，研究主题随时间的演化，例如DTM、CTDTM、DMM、OLDA等模型。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c）<strong>并行主题模型</strong>：在大规模数据处理的需求下，基于并行计算的主题模型也开始得到关注。现有的解决方案有：Mallet、GPU-LDA、Async-LDA、N.C.L、pLDA、Y!LDA、Mahout、Mr.LDA等；其中pLDA、Y!LDA、Mahout、Mr.LDA等基于Hadoop/MapReduce框架，其他方案则基于传统的并行编程模型；参数估算方面，Mallet、Async-LDA、pLDA、Y!LDA等使用Gibbs抽样方法，Mr.LDA、Mahout、N.C.L等使用变分贝叶斯推断法，GPU-LDA同时支持两种方法。</p>
]]></content>
    
    <summary type="html">
    
      两篇文档是否相关往往不只决定于字面上的词语重复，还取决于文字背后的语义关联。对语义关联的挖掘，可以让搜索更加智能化。
    
    </summary>
    
      <category term="Algorithm" scheme="http://www.dianacody.com/categories/Algorithm/"/>
    
    
      <category term="Conditional Probability" scheme="http://www.dianacody.com/tags/Conditional-Probability/"/>
    
      <category term="matrix" scheme="http://www.dianacody.com/tags/matrix/"/>
    
  </entry>
  
  <entry>
    <title>文本分类：朴素贝叶斯Bayes</title>
    <link href="http://www.dianacody.com/2014/11/02/2014-11-02-Bayes/"/>
    <id>http://www.dianacody.com/2014/11/02/2014-11-02-Bayes/</id>
    <published>2014-11-01T16:00:00.000Z</published>
    <updated>2018-12-03T03:51:00.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、贝叶斯定理"><a href="#一、贝叶斯定理" class="headerlink" title="一、贝叶斯定理"></a>一、贝叶斯定理</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;贝叶斯公式思想：利用已知值来估计未知概率。已知某条件概率，如何得到两个事件交换后的概率，也就是已知P(A|B)的情况下如何求得P(B|A)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;条件概率：P(A|B)表示事件B已经发生的前提下，事件A发生的概率，叫做事件B发生下事件A的条件概率。</p>
<h5 id="二、贝叶斯原理、流程"><a href="#二、贝叶斯原理、流程" class="headerlink" title="二、贝叶斯原理、流程"></a>二、贝叶斯原理、流程</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;朴素贝叶斯思想基础：对于待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗地讲，好比你在街上看到一个黑人，我问你他是从哪里来的，你十有八九会说从非洲。为什么呢？因为黑人中非洲人比例最高，当然别人也有可能是美洲人或者拉丁人，但在没有其他可用信息下，我们会选择条件概率最大类别，这就是朴素贝叶斯思想基础。<br>贝叶斯分类的定义：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.设x={a1, a2, …,am}为一个待分类项，而每个a为x的一个特征属性；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.有类别集合C = {y1,y2, …, yn}<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.计算P(y1|x), P(y2|x),…, P(yn|x)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.如果P(yk|x)= max{ P(y1|x), P(y2|x), …, P(yn|x) }，则x∈yk.<br>关键是如何计算第3步中的<strong>各个条件概率</strong>。步骤：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.找到一个<strong>已知</strong>分类的待分类项集合，这个集合叫做训练样本集。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.统计得到在各类别下各个特征属性的条件概率估计。</p>
<p>整个朴素贝叶斯分类分为三个阶段：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em> 第一阶段——准备工作阶段</em>，这个阶段的任务是为朴素贝叶斯分类做必要的准备，主要工作是根据具体情况确定特征属性，并对每个特征属性进行适当划分，然后由人工对一部分待分类项进行分类，形成训练样本集合。这一阶段的输入是所有待分类数据，输出是特征属性和训练样本。这一阶段是整个朴素贝叶斯分类中唯一需要人工完成的阶段，其质量对整个过程将有重要影响，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em> 第二阶段——分类器训练阶段</em>，这个阶段的任务就是生成分类器，主要工作是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是特征属性和训练样本，输出是分类器。这一阶段是机械性阶段，根据前面讨论的公式可以由程序自动计算完成。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em> 第三阶段——应用阶段</em>。这个阶段的任务是使用分类器对待分类项进行分类，其输入是分类器和待分类项，输出是待分类项与类别的映射关系。这一阶段也是机械性阶段，由程序完成</p>
<h5 id="三、注意问题"><a href="#三、注意问题" class="headerlink" title="三、注意问题"></a>三、注意问题</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.如果给出的特征向量长度可能不同，这是需要归一化为同长度向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.计算公式中其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是P(x|yi)的计算方法，而朴素贝叶斯的前提假设“独立性”可知，P(x0, x1, x2, …, xn|yi) = p(x0|yi)<em>p(x1|yi)</em>p(x2|yi)…p(xn|yi)，因此一般有两种，一种是在类别为yi的那些样本集里，找到xj出现次数的总和，然后除以该样本的总和；第二种方法是类别为yi的那些样本集里，找到xj出现次数的总和，然后除以该样本中所有特征出现次数的总和。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.如果p(x|yi)中的某一项为0，则其联合概率乘积也可能为0，即2中公式分子为0，为了避免这种现象出现，一般情况下会将这一项初始化为1，当然为了保证概率相等，分母应该对应初始化为2（这里因为是2类，所以加2，如果是k类就需要加k，术语上叫<strong>Laplace光滑</strong>）。</p>
]]></content>
    
    <summary type="html">
    
      朴素贝叶斯主要用于文本分类。文本分类常见三大算法：KNN、朴素贝叶斯、支持向量机SVM。
    
    </summary>
    
      <category term="Algorithm" scheme="http://www.dianacody.com/categories/Algorithm/"/>
    
    
      <category term="Bayes" scheme="http://www.dianacody.com/tags/Bayes/"/>
    
  </entry>
  
  <entry>
    <title>迭代决策树GBRT（渐进梯度回归树）</title>
    <link href="http://www.dianacody.com/2014/11/01/2014-11-01-GBRT/"/>
    <id>http://www.dianacody.com/2014/11/01/2014-11-01-GBRT/</id>
    <published>2014-10-31T16:00:00.000Z</published>
    <updated>2018-12-03T03:55:36.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、决策树模型组合"><a href="#一、决策树模型组合" class="headerlink" title="一、决策树模型组合"></a>一、决策树模型组合</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;单决策树C4.5由于功能太简单，并且非常容易出现过拟合的现象，于是引申出了许多变种决策树，就是将单决策树进行<strong>模型组合</strong>，形成多决策树，比较典型的就是迭代决策树GBRT和随机森林RF。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在最近几年的paper上，如iccv这种重量级会议，iccv 09年的里面有不少文章都是与Boosting和随机森林相关的。模型组合+决策树相关算法有两种比较基本的形式：随机森林RF与GBDT，其他比较新的模型组合+决策树算法都是来自这两种算法的延伸。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>核心思想：其实很多“渐进梯度” Gradient Boost都只是一个框架，里面可以套用很多不同的算法。</strong><br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先说明一下，GBRT这个算法有很多名字，但都是同一个算法：</p>
<ul>
<li>GBRT (Gradient BoostRegression Tree) 渐进梯度回归树</li>
<li>GBDT (Gradient BoostDecision Tree) 渐进梯度决策树</li>
<li>MART (MultipleAdditive Regression Tree) 多决策回归树</li>
<li>Tree Net决策树网络</li>
</ul>
<h5 id="二、GBRT"><a href="#二、GBRT" class="headerlink" title="二、GBRT"></a>二、GBRT</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;迭代决策树算法，在阿里内部用得比较多（所以阿里算法岗位面试时可能会问到），由多棵决策树组成，所有树的输出结果累加起来就是最终答案。它在被提出之初就和SVM一起被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GBRT是回归树，不是分类树。其核心就在于，每一棵树是从之前所有树的残差中来学习的。为了防止过拟合，和Adaboosting一样，也加入了boosting这一项。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;提起决策树（DT, DecisionTree）不要只想到C4.5单分类决策树，GBRT不是<strong>分类树</strong>而是<strong>回归树</strong>！<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;决策树分为<strong>回归树</strong>和<strong>分类树</strong>：<strong>回归树</strong>用于预测实数值，如明天温度、用户年龄；<strong>分类树</strong>用于分类标签值，如晴天/阴天/雾/雨、用户性别。注意前者结果加减是有意义的，如10岁+5岁-3岁=12岁，后者结果加减无意义，如男+女=到底是男还是女？GBRT的核心在于累加所有树的结果作为最终结果，而分类树是没有办法累加的。所以GBDT中的树都是回归树而非分类树。第一棵树是正常的，之后所有的树的决策全是由<strong>残差</strong>（此次的值与上次的值之差）来作决策。</p>
<h5 id="三、算法原理"><a href="#三、算法原理" class="headerlink" title="三、算法原理"></a>三、算法原理</h5><p>0.给定一个初始值<br>1.建立M棵决策树（迭代M次）<br>2.对函数估计值F(x)进行Logistic变换<br>3.对于K各分类进行下面的操作（其实这个for循环也可以理解为向量的操作，每个样本点xi都对应了K种可能的分类yi，所以yi，F(xi)，p(xi)都是一个K维向量）<br>4.求得残差减少的梯度方向<br>5.根据每个样本点x，与其残差减少的梯度方向，得到一棵由J个叶子节点组成的决策树<br><strong>6.当决策树建立完成后，通过这个公式，可以得到每个叶子节点的增益（这个增益在预测时候用的）</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<em>每个增益的组成其实也是一个K维向量，表示如果在决策树预测的过程中，如果某个样本点掉入了这个叶子节点，则其对应的K个分类的值是多少。比如GBDT得到了三棵决策树，一个样本点在预测的时候，也会掉入3个叶子节点上，其增益分别为（假设为3分类问题）：(0.5, 0.8, 0.1), (0.2, 0.6, 0.3), (0.4, .0.3, 0.3)，那么这样最终得到的分类为第二个，因为选择分类2的决策树是最多的。</em><br>7.将当前得到的决策树与之前的那些决策树合并起来，作为一个新的模型（跟6中的例子差不多）</p>
<h5 id="四、GBRT适用范围"><a href="#四、GBRT适用范围" class="headerlink" title="四、GBRT适用范围"></a>四、GBRT适用范围</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该版本的GBRT几乎可用于所有的回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBRT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。</p>
<h5 id="五、搜索引擎排序应用RankNet"><a href="#五、搜索引擎排序应用RankNet" class="headerlink" title="五、搜索引擎排序应用RankNet"></a>五、搜索引擎排序应用RankNet</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;搜索排序关注各个doc的顺序而不是绝对值，所以需要一个新的cost function，而RankNet基本就是在定义这个cost function，它可以兼容不同的算法（GBDT、神经网络…）。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实际的搜索排序使用的是Lambda MART算法，必须指出的是由于这里要使用排序需要的cost function，LambdaMART迭代用的并不是残差。Lambda在这里充当替代残差的计算方法，它使用了一种类似Gradient*步长模拟残差的方法。这里的MART在求解方法上和之前说的残差略有不同。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;搜索排序也需要训练集，但多数用人工标注实现，即对每个(query, doc)pair给定一个分值（如1, 2, 3, 4），分值越高越相关，越应该排到前面。RankNet就是基于此制定了一个学习误差衡量方法，即cost function。RankNet对任意两个文档A,B，通过它们的人工标注分差，用sigmoid函数估计两者顺序和逆序的概率P1。然后同理用机器学习到的分差计算概率P2（sigmoid的好处在于它允许机器学习得到的分值是任意实数值，只要它们的分差和标准分的分差一致，P2就趋近于P1）。这时利用P1和P2求的两者的交叉熵，该交叉熵就是cost function。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;有了cost function，可以求导求Gradient，Gradient即每个文档得分的一个下降方向组成的N维向量，N为文档个数（应该说是query-doc pair个数）。这里仅仅是把”求残差“的逻辑替换为”求梯度“。每个样本通过Shrinkage累加都会得到一个最终得分，直接按分数从大到小排序就可以了。</p>
]]></content>
    
    <summary type="html">
    
      单决策树C4.5由于功能太简单，并且非常容易出现过拟合的现象，于是引申出了许多变种决策树，就是将单决策树进行模型组合，形成多决策树，比较典型的就是迭代决策树GBRT和随机森林RF.
    
    </summary>
    
      <category term="Algorithm" scheme="http://www.dianacody.com/categories/Algorithm/"/>
    
    
      <category term="GBRT" scheme="http://www.dianacody.com/tags/GBRT/"/>
    
  </entry>
  
  <entry>
    <title>KNN算法-python实现</title>
    <link href="http://www.dianacody.com/2014/10/29/2014-10-29-KNN-python/"/>
    <id>http://www.dianacody.com/2014/10/29/2014-10-29-KNN-python/</id>
    <published>2014-10-28T16:00:00.000Z</published>
    <updated>2018-04-30T10:45:11.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、算法原理"><a href="#一、算法原理" class="headerlink" title="一、算法原理"></a>一、算法原理</h5><ol>
<li>计算抑制类别数据集中的点与当前点的距离（欧氏距离、马氏距离等）</li>
<li>按照距离递增依次排序</li>
<li>选取当前点距离最小的k个点</li>
<li>确定前k个点所在类别出现频率</li>
<li>返回前k个点出现频率最高的类别作为当前点的预测分类</li>
</ol>
<p>注意：</p>
<ol>
<li><p>关于k值个数的选择，其取决于数据。一般地，在分类时，较大k值可以减小噪声的影响，但会使类别界限变得模糊。</p>
<blockquote>
<ul>
<li>好的k值可以通过各种启发式技术来获取（eg.交叉验证）</li>
<li>噪声和非相关性特征向量的存在会使k近邻算法的准确性减小</li>
</ul>
</blockquote>
</li>
<li><p>近邻算法具有较强的一致性结果，随着数据趋于无线，算法的错误率不会超过贝叶斯算法错误率的2倍。对于一些好的k值，k近邻保证错误率不会超过贝叶斯理论误差率。</p>
</li>
</ol>
<h5 id="二、源码实现"><a href="#二、源码实现" class="headerlink" title="二、源码实现"></a>二、源码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *  </div><div class="line"><span class="keyword">import</span> time  </div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </div><div class="line">  </div><div class="line">  </div><div class="line"><span class="comment"># calculate Euclidean distance  </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclDistance</span><span class="params">(vector1, vector2)</span>:</span>  </div><div class="line">    <span class="keyword">return</span> sqrt(sum(power(vector2 - vector1, <span class="number">2</span>)))  </div><div class="line">  </div><div class="line"><span class="comment"># init centroids with random samples  </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">initCentroids</span><span class="params">(dataSet, k)</span>:</span>  </div><div class="line">    numSamples, dim = dataSet.shape  </div><div class="line">    centroids = zeros((k, dim))  </div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):  </div><div class="line">        index = int(random.uniform(<span class="number">0</span>, numSamples))  </div><div class="line">        centroids[i, :] = dataSet[index, :]  </div><div class="line">    <span class="keyword">return</span> centroids  </div><div class="line">  </div><div class="line"><span class="comment"># k-means cluster  </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kmeans</span><span class="params">(dataSet, k)</span>:</span>  </div><div class="line">    numSamples = dataSet.shape[<span class="number">0</span>]  </div><div class="line">    <span class="comment"># first column stores which cluster this sample belongs to,  </span></div><div class="line">    <span class="comment"># second column stores the error between this sample and its centroid  </span></div><div class="line">    clusterAssment = mat(zeros((numSamples, <span class="number">2</span>)))  </div><div class="line">    clusterChanged = <span class="keyword">True</span>  </div><div class="line">  </div><div class="line">    <span class="comment">## step 1: init centroids  </span></div><div class="line">    centroids = initCentroids(dataSet, k)  </div><div class="line">  </div><div class="line">    <span class="keyword">while</span> clusterChanged:  </div><div class="line">        clusterChanged = <span class="keyword">False</span>  </div><div class="line">        <span class="comment">## for each sample  </span></div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(numSamples):  </div><div class="line">            minDist  = <span class="number">100000.0</span>  </div><div class="line">            minIndex = <span class="number">0</span>  </div><div class="line">            <span class="comment">## for each centroid  </span></div><div class="line">            <span class="comment">## step 2: find the centroid who is closest  </span></div><div class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  </div><div class="line">                distance = euclDistance(centroids[j, :], dataSet[i, :])  </div><div class="line">                <span class="keyword">if</span> distance &lt; minDist:  </div><div class="line">                    minDist  = distance  </div><div class="line">                    minIndex = j  </div><div class="line">              </div><div class="line">            <span class="comment">## step 3: update its cluster  </span></div><div class="line">            <span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:  </div><div class="line">                clusterChanged = <span class="keyword">True</span>  </div><div class="line">                clusterAssment[i, :] = minIndex, minDist**<span class="number">2</span>  </div><div class="line">  </div><div class="line">        <span class="comment">## step 4: update centroids  </span></div><div class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(k):  </div><div class="line">            pointsInCluster = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A == j)[<span class="number">0</span>]]  </div><div class="line">            centroids[j, :] = mean(pointsInCluster, axis = <span class="number">0</span>)  </div><div class="line">  </div><div class="line">    <span class="keyword">print</span> <span class="string">'Congratulations, cluster complete!'</span>  </div><div class="line">    <span class="keyword">return</span> centroids, clusterAssment  </div><div class="line">  </div><div class="line"><span class="comment"># show your cluster only available with 2-D data  </span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">showCluster</span><span class="params">(dataSet, k, centroids, clusterAssment)</span>:</span>  </div><div class="line">    numSamples, dim = dataSet.shape  </div><div class="line">    <span class="keyword">if</span> dim != <span class="number">2</span>:  </div><div class="line">        <span class="keyword">print</span> <span class="string">"Sorry! I can not draw because the dimension of your data is not 2!"</span>  </div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>  </div><div class="line">  </div><div class="line">    mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]  </div><div class="line">    <span class="keyword">if</span> k &gt; len(mark):  </div><div class="line">        <span class="keyword">print</span> <span class="string">"Sorry! Your k is too large! please contact Zouxy"</span>  </div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>  </div><div class="line">  </div><div class="line">    <span class="comment"># draw all samples  </span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(numSamples):  </div><div class="line">        markIndex = int(clusterAssment[i, <span class="number">0</span>])  </div><div class="line">        plt.plot(dataSet[i, <span class="number">0</span>], dataSet[i, <span class="number">1</span>], mark[markIndex])  </div><div class="line">  </div><div class="line">    mark = [<span class="string">'Dr'</span>, <span class="string">'Db'</span>, <span class="string">'Dg'</span>, <span class="string">'Dk'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]  </div><div class="line">    <span class="comment"># draw the centroids  </span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):  </div><div class="line">        plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize = <span class="number">12</span>)  </div><div class="line">  </div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<h5 id="三、测试代码"><a href="#三、测试代码" class="headerlink" title="三、测试代码"></a>三、测试代码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">from numpy import *  </div><div class="line">import time  </div><div class="line">import matplotlib.pyplot as plt  </div><div class="line">  </div><div class="line">## step 1: load data  </div><div class="line">print &quot;step 1: load data...&quot;  </div><div class="line">dataSet = []  </div><div class="line">fileIn = open(&apos;E:/Python/Machine Learning in Action/testSet.txt&apos;)  </div><div class="line">for line in fileIn.readlines():  </div><div class="line">    lineArr = line.strip().split(&apos;\t&apos;)  </div><div class="line">    dataSet.append([float(lineArr[0]), float(lineArr[1])])  </div><div class="line">  </div><div class="line">## step 2: clustering...  </div><div class="line">print &quot;step 2: clustering...&quot;  </div><div class="line">dataSet = mat(dataSet)  </div><div class="line">k = 4  </div><div class="line">centroids, clusterAssment = kmeans(dataSet, k)  </div><div class="line">  </div><div class="line">## step 3: show the result  </div><div class="line">print &quot;step 3: show the result...&quot;  </div><div class="line">showCluster(dataSet, k, centroids, clusterAssment)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      KNN算法的学习及实现源码。
    
    </summary>
    
      <category term="Algorithm" scheme="http://www.dianacody.com/categories/Algorithm/"/>
    
    
      <category term="KNN" scheme="http://www.dianacody.com/tags/KNN/"/>
    
      <category term="algorithm" scheme="http://www.dianacody.com/tags/algorithm/"/>
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>防止爬虫被墙的方法总结</title>
    <link href="http://www.dianacody.com/2014/10/01/2014-10-01-spider_5/"/>
    <id>http://www.dianacody.com/2014/10/01/2014-10-01-spider_5/</id>
    <published>2014-09-30T16:00:00.000Z</published>
    <updated>2018-12-03T04:01:39.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、设置下载等待时间-下载频率"><a href="#一、设置下载等待时间-下载频率" class="headerlink" title="一、设置下载等待时间/下载频率"></a>一、设置下载等待时间/下载频率</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大规模集中访问对服务器的影响较大，爬虫可以短时间增大服务器负载。这里需要注意的是：设定下载等待时间的范围控制，等待时间过长，不能满足短时间大规模抓取的要求，等待时间过短则很有可能被拒绝访问。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)在之前“从url获取HTML”的方法里，对于httpGet的配置设置了socket超时和连接connect超时，其实这里的时长不是绝对的，主要取决于目标网站对爬虫的控制。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)另外，在scrapy爬虫框架里，专有参数可以设置下载等待时间download_delay，这个参数可以设置在setting.py里，也可以设置在spider里。</p>
<h5 id="二、设置cookies"><a href="#二、设置cookies" class="headerlink" title="二、设置cookies"></a>二、设置cookies</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Cookie其实是储存在用户终端的一些被加密的数据，有些网站通过cookies来识别用户身份，如果某个访问总是高频率地发请求，很可能会被网站注意到，被嫌疑为爬虫，这时网站就可以通过cookie找到这个访问的用户而拒绝其访问。可以自定义设置cookie策略（防止cookie rejected问题：拒绝写入cookie）或者禁止cookies。</p>
<h6 id="2-1-自定义设置cookies策略（防止cookierejected问题，拒绝写入cookie）"><a href="#2-1-自定义设置cookies策略（防止cookierejected问题，拒绝写入cookie）" class="headerlink" title="2.1 自定义设置cookies策略（防止cookierejected问题，拒绝写入cookie）"></a>2.1 自定义设置cookies策略（防止cookierejected问题，拒绝写入cookie）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在系列一那篇文章里就有自定义cookie策略设置，但更多的借鉴是官方文档的例子，设置方法其实都大同小异，因为HttpClient-4.3.1组件版本跟以前旧版本的不同，写法也有不同，另见官方文档：<a href="http://hc.apache.org/httpcomponents-client-4.3.x/tutorial/html/statemgmt.html#d5e553" target="_blank" rel="external">http://hc.apache.org/httpcomponents-client-4.3.x/tutorial/html/statemgmt.html#d5e553</a></p>
<h6 id="2-2-禁止cookies"><a href="#2-2-禁止cookies" class="headerlink" title="2.2 禁止cookies"></a>2.2 禁止cookies</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过禁止cookie，这是客户端主动阻止服务器写入。禁止cookie可以防止可能使用cookies识别爬虫的网站来ban掉我们。在scrapy爬虫中可以设置COOKIES_ENABLES= FALSE，即不启用cookies middleware，不向web server发送cookies。</p>
<h5 id="三、修改User-Agent"><a href="#三、修改User-Agent" class="headerlink" title="三、修改User-Agent"></a>三、修改User-Agent</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最常见的就是伪装浏览器，修改User-Agent（用户代理）。User-Agent是指包含浏览器信息、操作系统信息等的一个字符串，也称之为一种特殊的网络协议。服务器通过它判断当前访问对象是浏览器、邮件客户端还是网络爬虫。在request.headers里可以查看user-agent，关于怎么分析数据包、查看其User-Agent等信息，这个在前面的文章里提到过。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;具体方法可以把User-Agent的值改为浏览器的方式，甚至可以设置一个User-Agent池（list，数组，字典都可以），存放多个“浏览器”，每次爬取的时候随机取一个来设置request的User-Agent，这样User-Agent会一直在变化，防止被墙。</p>
<h5 id="四、修改IP"><a href="#四、修改IP" class="headerlink" title="四、修改IP"></a>四、修改IP</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其实微博识别的是IP，不是账号。也就是说，当需要连续抓取很多数据的时候，模拟登录没有意义。只要是同一个IP，不管怎么换账号也没有用，主要的是换IP。Web server应对爬虫的策略之一就是直接将IP或者整个IP段都封掉禁止访问，当IP被禁封后，转换到其他IP继续访问即可。方法：代理IP、本地IP数据库（使用IP池）。</p>
<h6 id="4-1-从代理IP网站获取大量IP"><a href="#4-1-从代理IP网站获取大量IP" class="headerlink" title="4.1 从代理IP网站获取大量IP"></a>4.1 从代理IP网站获取大量IP</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果总是请求代理IP站点也未免有些麻烦，况且某些代理IP站点有时还可能被禁封，当然再换一个代理IP站点也可以，如果你不嫌麻烦的话。</p>
<h6 id="4-2-使用IP地址库"><a href="#4-2-使用IP地址库" class="headerlink" title="4.2 使用IP地址库"></a>4.2 使用IP地址库</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;网上也有很多现成可用的IP地址库，可以存放到本地，如果本地有IP数据库就方便很多，至少不用每次都去请求代理IP站点了（当然可以一次性把站点内所有代理IP先爬下来存储好，形成本地IP数据库），总之获取IP的方法有很多，不一定非要是通过代理IP站点。</p>
<h5 id="五、分布式爬取"><a href="#五、分布式爬取" class="headerlink" title="五、分布式爬取"></a>五、分布式爬取</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分布式爬取的也有很多Github repo。原理主要是维护一个所有集群机器能够有效分享的分布式队列。使用分布式爬取还有另外一个目的：大规模抓取，单台机器的负荷很大，况且速度很慢，多台机器可以设置一个master管理多台slave去同时爬取。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;另外关于网页判重问题，可以用Bloom Filter。</p>
]]></content>
    
    <summary type="html">
    
      爬虫的目的就是大规模地、长时间地获取数据，大规模集中对服务器访问，时间一长就有可能被拒绝。大规模集中访问对服务器的影响较大，爬虫可以短时间增大服务器负载。
    
    </summary>
    
      <category term="Crawler" scheme="http://www.dianacody.com/categories/Crawler/"/>
    
    
      <category term="crawler" scheme="http://www.dianacody.com/tags/crawler/"/>
    
      <category term="spider" scheme="http://www.dianacody.com/tags/spider/"/>
    
  </entry>
  
</feed>
