<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DianaCody</title>
  <subtitle>subtitle,subtitle,subtitle</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.dianacody.com/"/>
  <updated>2023-08-03T11:03:24.288Z</updated>
  <id>http://www.dianacody.com/</id>
  
  <author>
    <name>DianaCody</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SLAM定位建图和轨迹评估</title>
    <link href="http://www.dianacody.com/2023/01/03/2023-01-03-SLAM/"/>
    <id>http://www.dianacody.com/2023/01/03/2023-01-03-SLAM/</id>
    <published>2023-01-02T16:00:00.000Z</published>
    <updated>2023-08-03T11:03:24.288Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h5><p><strong>SLAM</strong>：<strong>即时定位与地图构建</strong>，Simultaneous Location And Mapping，也称为CML (Current Mapping and Localization)。分开来看就是两个部分：</p>
<ul>
<li>定位部分，主要是依据周围图像，提取图像特征点(或光流法)，依据图像差异计算轨迹，以确定自身位置。</li>
<li>建图部分，通过自身回环轨迹，不断更新当前地图和全局地图，最终以点云形式，计算出周边环境轮廓地图。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;开源框架Orb-Slam3是典型的以图像特征点法为主的slam计算框架，基于此可以评估不同光照条件、不同相机、不同外部环境、不同曝光、不同图像等各种因素对于传感器运动轨迹的影响。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于仅基于图像的slam定位建图算法来说，获取到的图像质量对于计算精度和准确率的影响是极为重要的（尤其在没有附加惯性测量单元(imu)或外部GPS定位的条件下）。同时也可以看到，不同相机的帧率/曝光方式/相机性质/相机标定的内外参…，所带来的差异影响也是非常大的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本文分为三个部分：<br><strong>1.卷帘曝光相机(Rolling-Shutter)和全局曝光相机(Global-Shutter)对比</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>global shutter</strong>：<strong>全局曝光</strong>，感光元件的所有像素点同时曝光一定时间，进而成像。没有Rolling Shutter的“果冻现象”。曝光时间比Rolling Shutter短，曝光整帧后输出像素数据。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>rolling shutter</strong>：<strong>卷帘曝光</strong>，感光元件的所有像素点逐行轮流曝光一定时间，进而成像。会出现“果冻现象”。曝光时间比Global Shutter长，但是曝光一行输出一行。<br><strong>2.RealSense D435i的彩色相机(rgb-camera)和红外相机(infrared-camera)对比</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RealSense 是立体视觉深度相机，集成了2个红外传感器(IR Stereo Camera)、1个红外激光发射器(IR Projector)、1个彩色相机(Color Camera)。IntelRealSense D435i提供完整深度相机模块，集成视觉处理器、立体深度模块、RGB传感器、彩色图像信号处理模块。深度模块采用立体视觉的左右成像器、可选红外激光发射器、 RGB色彩传感器。<br><strong>3.RealSense D455的彩色相机(rgb-camera)和红外相机(infrared-camera)对比</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RealSense D455和RealSense D435i其实本质差异不太大，除了相机摄像头分布位置、摄像头内外参等。</p>
<h5 id="二、Rolling-Shutter-amp-Global-Shutter相机轨迹评估"><a href="#二、Rolling-Shutter-amp-Global-Shutter相机轨迹评估" class="headerlink" title="二、Rolling-Shutter &amp; Global-Shutter相机轨迹评估"></a>二、Rolling-Shutter &amp; Global-Shutter相机轨迹评估</h5><h6 id="1-实验设备"><a href="#1-实验设备" class="headerlink" title="1.实验设备"></a>1.实验设备</h6><p>Global Shutter相机 (2M)<br>Rolling Shutter相机 (630)</p>
<h6 id="2-实验方法"><a href="#2-实验方法" class="headerlink" title="2.实验方法"></a>2.实验方法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;两个摄像头同时固定设备，沿相同轨迹移动一段时间后，得到视频。两个相机运动轨迹近乎重合，有细微差别。预处理视频后，得到帧图像数据集（室内indoor/室外outdoor/走廊slides），标注为TUM数据集格式，定位建图框架运行环境为Orb-Slam3，按关键帧提取图像特征点，得到的轨迹文件KeyFrameTrajectory.txt，为运动位姿数据，包含：时间戳timestamp、6dof位姿数据(xyz和rpy)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;参照轨迹Ground Truth为global shutter轨迹，rolling shutter相对于global shutter的。</p>
<h6 id="3-轨迹实验"><a href="#3-轨迹实验" class="headerlink" title="3.轨迹实验"></a>3.轨迹实验</h6><p><strong>3.1.openCV录制视频</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;openCV直接调起“pc摄像头”或“usb外接摄像头”录制数据。openCV读视频原理：其实就是不停地拍照，不停地写入到一个文件。视频本身也是由一张一张照片组成的。部分代码如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">string</span> video_path = <span class="string">"/home/admin/video.mp4"</span>;</div><div class="line"><span class="function">VideoCapture <span class="title">capture</span><span class="params">(<span class="number">2</span>)</span></span>;</div><div class="line">capture.<span class="built_in">set</span>(CAP_PROP_FOURCC, CV_FOURCC(<span class="string">'M'</span>,<span class="string">'J'</span>,<span class="string">'P'</span>,<span class="string">'G'</span>));</div><div class="line">capture.<span class="built_in">set</span>(CV_CAP_PROP_FRAME_WIDTH, <span class="number">640</span>);</div><div class="line">capture.<span class="built_in">set</span>(CV_CAP_PROP_FRAME_HEIGHT, <span class="number">480</span>);</div><div class="line">capture.<span class="built_in">set</span>(CV_CAP_PROP_FPS, <span class="number">30</span>);</div><div class="line">VideoWriter videoWriter;</div><div class="line">videoWriter.open(video_path, CV_FOURCC(<span class="string">'M'</span>,<span class="string">'J'</span>,<span class="string">'P'</span>,<span class="string">'G'</span>), <span class="number">30.0</span>, Size(frameWidth,frameHeight), <span class="literal">true</span>);</div><div class="line">Mat frameImg;</div><div class="line"><span class="keyword">while</span>(<span class="number">1</span>) &#123;</div><div class="line">    videoWriter &lt;&lt; frameImg;</div><div class="line">    <span class="keyword">if</span>(<span class="keyword">char</span>(waitKey(<span class="number">40</span>))==<span class="string">'q'</span> or <span class="keyword">char</span>(waitKey(<span class="number">40</span>))==<span class="number">27</span>) &#123;</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line">capture.release();</div><div class="line">frameImg.release();</div><div class="line">videoWriter.release();</div></pre></td></tr></table></figure></p>
<p><strong>3.2.实验视频</strong> (点击自动播放)<br><strong>(1) 室内(indoor)</strong></p>
<center><video height="60%" width="60%" controls preload="none" poster="封面" src="/resources/slam-indoor.mp4" type="video/mp4"></video></center>
<center><h7 style="color:grey">视频1 室内(indoor)轨迹实验</h7></center>

<p><strong>(2) 室外(outdoor)</strong></p>
<center><video height="60%" width="60%" controls preload="none" poster="封面" src="/resources/slam-outdoor.mp4" type="video/mp4"></video></center>
<center><h7 style="color:grey">视频2 室外(outdoor)轨迹实验</h7></center>

<p><strong>(3) 走廊(slides)</strong></p>
<center><video height="60%" width="60%" controls preload="none" poster="封面" src="/resources/slam-slides.mp4" type="video/mp4"></video></center>
<center><h7 style="color:grey">视频3 走廊(slides)轨迹实验</h7></center>

<p><strong>3.3.视频制作数据集</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将视频以一定帧率采样，得到每帧图像，加上时间戳(timestamp)，用作Orb-Slam3系统的输入。常见的输入格式有TUM、EuRoC，此处以TUM为例，用openCV将视频mp4文件转换为TUM格式数据集。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mp4提取图片帧，保存为TUM的“rgb文件夹”和“rgb.txt”。部分代码片如下：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">string</span> video_path = <span class="string">"/home/admin/video.mp4"</span>;</div><div class="line">Mat frame;</div><div class="line"><span class="keyword">double</span> time;</div><div class="line"><span class="function">VideoCapture <span class="title">capture</span><span class="params">(video_path)</span></span>;</div><div class="line">ofstream of;</div><div class="line">of.open(path+<span class="string">"video/rgb.txt"</span>, <span class="built_in">std</span>::ios_base::app);</div><div class="line">...</div><div class="line"><span class="keyword">while</span>(<span class="literal">true</span>) &#123;</div><div class="line">    capture &gt;&gt; frame;</div><div class="line">    time = capture.get(CV_CAP_PROP_POS_MSEC);  <span class="comment">//时间戳timestamp</span></div><div class="line">    ...</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">ostringstream</span> osf;</div><div class="line">    osf &lt;&lt; path+<span class="string">"video/rgb/"</span> &lt;&lt; <span class="built_in">std</span>::fixed &lt;&lt; time &lt;&lt; <span class="string">".png"</span>;</div><div class="line">    cv::imwrite(osf.str(), frame);</div><div class="line">    of &lt;&lt; <span class="built_in">std</span>::fixed &lt;&lt; time &lt;&lt; <span class="string">" "</span> &lt;&lt; <span class="string">"rgb/"</span> &lt;&lt; time &lt;&lt; <span class="string">".png"</span> &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">&#125;</div><div class="line">of.close();</div><div class="line">capture.release();</div></pre></td></tr></table></figure></p>
<p><strong>3.4.相机标定</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;标定得到实际相机内外参(fx,fy,cx,cy)、相机转移参数(T_c1_c0)、畸变参数(distortion_coeffs)，写入Orb-Slam3参数配置文件cam0/1-camchain.yaml。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相机标定部分内容比较多，从搭建ros标定环境、到标定工具Kalibr使用、相机内外参+imu标定，详见另外本系列另外博客《<a href="http://www.dianacody.com/2022/12/01/2022-12-01-Calib">相机标定</a>》。用到的工具：Kalibr工具imu+camera联合标定。</p>
<p><strong>3.5.Slam计算</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;修改配置文件中的相机内外参(ps:相机参数需要自行先标定好)，Orb-Slam3计算轨迹，得到轨迹文件FrameTrajectory.txt。</p>
<h6 id="4-评估指标"><a href="#4-评估指标" class="headerlink" title="4.评估指标"></a>4.评估指标</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;轨迹、绝对位姿误差、相对位姿误差。</p>
<h7 style="color:grey"><p>
Ape: 绝对位姿误差，APE w.r.t. translation part (m), (with Sim(3) Umeyama alignment)。估计轨迹全局一致性，比较被估计值和真值轨迹之间的绝对距离。针对所有时刻定义平移分量的 均方根误差(RMSE)和方差(SSE)。<br>
Rpe: 相对位姿误差，RPE w.r.t. translation part (m), (with Sim(3) Umeyama alignment), for delta = 1 (frames) using consecutive pairs。测量轨迹在固定时间区间Δt内的局部准确度。对应轨迹的漂移，计算相隔固定时间差Δ两帧位姿差。
</p></h7>

<p>各指标信息汇总对比如下：</p>
<table>
    <tr><th style="width:15%">Scene</th><th>Traj-Info</th></tr>
    <tr><td>室内(Indoor)</td><td>name: f_dataset-indoor_rolling<br>infos: 968 poses, 6.247m path length, 322333333330000.000s duration<br>name: f_dataset-indoor_global<br>infos: 986 poses, 5.679m path length, 328333333330000.000s duration</td></tr>
    <tr><td bgcolor="white">室外(Outdoor)</td><td bgcolor="white">name: f_dataset-outdoor_rolling<br>infos: 391 poses, 0.695m path length, 130000000000000.000s duration<br>name: f_dataset-outdoor_global<br>infos: 696 poses, 1.511m path length, 231666666670000.000s duration</td></tr>
    <tr><td>走廊(Slides)</td><td>name: f_dataset-slides_rolling<br>infos: 466 poses, 11.648m path length, 155000000000000.062s duration<br>name:  f_dataset-slides_global<br>infos: 1455 poses, 40.405m path length, 494333333330000.062s duration</td></tr>
</table>

<p>各误差汇总对比如下：</p>
<table>
    <tr><th style="width:10%">指标</th><th>室内(indoor)</th><th>室外(outdoor)</th><th>走廊(slides)</th></tr>
    <tr>
        <td>APE</td>
        <td>max: 0.374459<br>mean: 0.062394<br>median: 0.029402<br>min: 0.003393<br>rmse: 0.099173<br>sse: 9.520567<br>std: 0.077086</td>
        <td>max: 0.132285<br>mean: 0.008305<br>median: 0.006561<br>min: 0.000379<br>rmse: 0.012343<br>sse: 0.059569<br>std: 0.009131</td>
        <td>max: 2.565694<br>mean: 0.654031<br>median: 0.608664<br>min: 0.038379<br>rmse: 0.792056<br>sse: 292.346650<br>std: 0.446762</td>
    </tr>
    <tr>
        <td bgcolor="white">RPE</td>
        <td bgcolor="white">max: 0.104738<br>mean: 0.005775<br>median: 0.003462<br>min: 0.000161<br>rmse: 0.010518<br>sse: 0.106973<br>std: 0.008790</td>
        <td bgcolor="white">max: 0.135044<br>mean: 0.006286<br>median: 0.004057<br>min: 0.000303<br>rmse: 0.013238<br>sse: 0.068347<br>std: 0.011650</td>
        <td bgcolor="white">max: 0.679385<br>mean: 0.040339<br>median: 0.020645<br>min: 0.002755<br>rmse: 0.077982<br>sse: 2.827775<br>std: 0.066738</td>
    </tr>
</table>

<p>各轨迹图统计汇总对比如下：(点击查看大图)</p>
<table>
    <tr><th style="width:10%;">指标</th><th>室内(indoor)</th><th>室外(outdoor)</th><th>走廊(slides)</th></tr>
    <tr>
        <td>Trajectory</td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-traj-indoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-traj-outdoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-traj-slides.jpg" width="240px"></td>
    </tr>
    <tr>
        <td bgcolor="white">xyz_view</td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-xyz-indoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-xyz-outdoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-xyz-slides.jpg" width="240px"></td>
    </tr>
    <tr>
        <td>rpy_view</td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpy-indoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpy-outdoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpy-slides.jpg" width="240px"></td>
    </tr>
    <tr>
        <td bgcolor="white">APE</td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-ape-indoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-ape-outdoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-ape-slides.jpg" width="240px"></td>
    </tr>
    <tr>
        <td>APE-Map</td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-indoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-outdoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-slides.jpg" width="240px"></td>
    </tr>
    <tr>
        <td bgcolor="white">RPE</td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpe-indoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpe-outdoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpe-slides.jpg" width="240px"></td>
    </tr>
    <tr>
        <td>RPE-Map</td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-indoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-outdoor.jpg" width="240px"></td>
        <td width="240px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-slides.jpg" width="240px"></td>
    </tr>
</table>

<h6 id="5-结论"><a href="#5-结论" class="headerlink" title="5.结论"></a>5.结论</h6><p>1.Orb-Slam3完全是基于图像特征点(ORB)的方法。</p>
<ul>
<li>在特定光照、参照物明确的情景下，轨迹完整，误差也较小。</li>
<li>在图像过于相似(走廊往返)、光照过于强烈(室外)的情况，会存在轨迹跳跃不连续现象。</li>
</ul>
<p>2.图像变化过快的情形，导致特征点提取过慢或丢失部分特征点，会出现跑数据时的帧卡顿现象，对Altas地图集localMap构建和地图合并有影响。</p>
<table>
    <tr>
        <td width="330px" bgcolor="white"><img align="left" src="/resources/slam-break1.jpg" width="330px"></td>
        <td width="360px" bgcolor="white"><img align="left" src="/resources/slam-break2.jpg" width="360px"></td>
    </tr>
</table>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如上图所示，走廊实验中的现象，当沿着走廊尽头翻转180°走回来时，Orb-Slam3系统会因为图像过于相似(走廊顺向/逆向)而分辨不清，于是定位建图时直接跳回原点开始重新建图，随机选定一个方向角度开始重新绘制轨迹，即出现红色箭头所示方向。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其本质原因是基于特征点法的slam算法本身存在的缺陷，当图像质量不清晰、或者图像中能检出的特征点十分有限，导致不能slam系统很好区分图像时，就没法定位了。<br>3.rolling shutter会存在轨迹不完全的情况(丢失轨迹)，global shutter则不会存在。</p>
<ul>
<li>轨迹重叠度：室内&gt;室外&gt;走廊</li>
<li>在轨迹存在的部分，误差：走廊&gt;室外&gt;室内</li>
</ul>
<p>此处详见上述“指标信息汇总对比”、“误差汇总对比”部分。</p>
<h5 id="三、RealSense-D435i-rgb-amp-infrared-camera轨迹评估"><a href="#三、RealSense-D435i-rgb-amp-infrared-camera轨迹评估" class="headerlink" title="三、RealSense D435i: rgb &amp; infrared-camera轨迹评估"></a>三、RealSense D435i: rgb &amp; infrared-camera轨迹评估</h5><h6 id="1-实验设备-1"><a href="#1-实验设备-1" class="headerlink" title="1.实验设备"></a>1.实验设备</h6><p>RealSense D435i: RGB-camera<br>RealSense D435i: Infrared-left-camera</p>
<h6 id="2-实验方法-1"><a href="#2-实验方法-1" class="headerlink" title="2.实验方法"></a>2.实验方法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RealSense D435i的2个摄像头：rgb-camera, infrared_left-camera，分别单独沿重复轨迹，实时运行Orb-Slam3，场景为 (室内indoor/室外outdoor/走廊slides)，得到轨迹文件KeyFrameTrajectory.txt和CameraTrajectory.txt，为各自运动位姿数据，包含：时间戳timestamp、6dof位姿数据(xyz和rpy)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<h7 style="color:red">由于2个摄像头是单独实时按相同既定路线运行，不存在同一时空统一性（时间戳timestamp、运行时长during length不完全统一），故轨迹为单独各自绘制。</h7></p>
<h6 id="3-轨迹实验-1"><a href="#3-轨迹实验-1" class="headerlink" title="3.轨迹实验"></a>3.轨迹实验</h6><p><strong>3.1.相机驱动</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RealSense深度相机系列，自带驱动调起摄像头，可实时录视频+slam定位建图，不是openCV录制预存离线数据集。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于RealSense驱动安装+ros环境安装+配置&amp;校准相机内外参+imu参数，详见RealSense官网。</p>
<ul>
<li>(1)realsense-sdk-2.0驱动安装：(下载链接见“参考文献”部分)<br></li>
<li>(2)获取相机内外参数：读取配置写到自定义文件(config_D435i.yaml、config_D455.yaml)，详见sdk源码：librealsense/tools/enumerate-devices/rs-enumerate-devices.cpp，可直接编译运行。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;把Orb-Slam3配置文件中的相机参数<script type="math/tex">Cx</script>, <script type="math/tex">Cy</script>, <script type="math/tex">Fx</script>, <script type="math/tex">Fy</script>，替换为相机配置的<script type="math/tex">Px</script>, <script type="math/tex">Py</script>, <script type="math/tex">Fx</script>, <script type="math/tex">Fy</script>参数。<br><strong>3.2.实验视频</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如前所述，仍旧是：室内(indoor)、室外(outdoor)、走廊(slides)。此处不再给出实验视频。<br><strong>3.3.相机标定</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;驱动RealSense SDK可以完成，一定注意保证相机参数的准确性。<br><strong>3.4.实时slam计算</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;运行Orb-Slam3，计算轨迹+定位建图。</p>
<h6 id="4-评估指标-1"><a href="#4-评估指标-1" class="headerlink" title="4.评估指标"></a>4.评估指标</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;轨迹、绝对位姿误差、相对位姿误差。</p>
<p>各指标信息汇总对比如下：</p>
<table>
    <tr><th style="width:15%">Scene</th><th colspan="2" style="text-align:center;">Traj-Info</th></tr>
    <tr><td bgcolor="#F5F5F5"></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Mono_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Rgbd_IMU</b></td></tr>
    <tr>
        <td bgcolor="white">室内(Indoor)</td>
        <td bgcolor="white">name: kf_dataset-monoi_realsense_d435i_indoor<br>infos: 276 poses, 36.352m path length, 101989215488.000s duration<br>name: f_dataset-monoi_realsense_d435i_indoor<br>infos: 3152 poses, 42.172m path length, 105055297792.000s duration</td>
        <td bgcolor="white">name: kf_dataset-rgbd_imu_realsense_d435i_indoor<br>infos: 280 poses, 29.703m path length, 49206745088.000s duration<br>name: f_dataset-rgbd_imu_realsense_d435i_indoor<br>infos: 1587 poses, 37.269m path length, 52942709760.000s duration</td>
    </tr>
    <tr>
        <td bgcolor="white">室外(Outdoor)</td>
        <td bgcolor="white">name: kf_dataset-monoi_realsense_d435i_outdoor<br>infos: 1179 poses, 257.977m path length, 236662186496.000s duration<br>name: f_dataset-monoi_realsense_d435i_outdoor<br>infos: 7252 poses, 275.178m path length, 241763062528.000s duration</td>
        <td bgcolor="white">name: kf_dataset-rgbd_imu_realsense_d435i_outdoor<br>infos: 1230 poses, 267.810m path length, 267213810176.000s duration<br>name: f_dataset-rgbd_imu_realsense_d435i_outdoor<br>infos: 8166 poses, 381.485m path length, 272350868224.000s duration</td>
    </tr>
    <tr>
        <td bgcolor="white">走廊(Slides)</td>
        <td bgcolor="white">name: kf_dataset-monoi_realsense_d435i_slides<br>infos: 283 poses, 45.609m path length, 173703452160.000s duration<br>name: f_dataset-monoi_realsense_d435i_slides<br>infos: 2581 poses, 71.888m path length, 175361714176.000s duration</td>
        <td bgcolor="white">name: kf_dataset-rgbd_imu_realsense_d435i_slides<br>infos: 233 poses, 35.413m path length, 52588330496.000s duration<br>name: f_dataset-rgbd_imu_realsense_d435i_slides<br>infos: 1729 poses, 48.788m path length, 57625243904.000s duration</td>
    </tr>
</table>

<p>各误差汇总对比如下：</p>
<table>
    <tr><th style="width:5%">指标</th><th colspan="2" style="text-align:center;">室内(indoor)</th><th colspan="2" style="text-align:center;">室外(outdoor)</th><th colspan="2" style="text-align:center;">走廊(slides)</th></tr>
    <tr><td bgcolor="#F5F5F5"></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Mono_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Rgbd_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Mono_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Rgbd_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Mono_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Rgbd_IMU</b></td></tr>
    <tr>
        <td bgcolor="white">APE</td>
        <td bgcolor="white">max:0.000004<br>mean:0.000001<br>median:0.000001<br>min:0.000000<br>rmse:0.000001<br>sse:0.000000<br>std:0.000001</td><td bgcolor="white">max:0.000006<br>mean:0.000002<br>median:0.000001<br>min:0.000000<br>rmse:0.000002<br>sse:0.000000<br>std:0.000001</td>
        <td bgcolor="white">max:0.000067<br>mean:0.000013<br>median:0.000011<br>min:0.000000<br>rmse:0.000017<br>sse:0.000000<br>std:0.000010</td><td bgcolor="white">max:0.000094<br>mean:0.000015<br>median:0.000010<br>min:0.000001<br>rmse:0.000020<br>sse:0.000001<br>std:0.000013</td>
        <td bgcolor="white">max:0.000007<br>mean:0.000002<br>median:0.000001<br>min:0.000000<br>rmse:0.000002<br>sse:0.000000<br>std:0.000001</td><td bgcolor="white">max:0.000013<br>mean:0.000002<br>median:0.000001<br>min:0.000000<br>rmse:0.000003<br>sse:0.000000<br>std:0.000002</td>
    </tr>
    <tr>
        <td bgcolor="white">RPE</td>
        <td bgcolor="white">max:0.000006<br>mean:0.000001<br>median:0.000001<br>min:0.000000<br>rmse:0.000002<br>sse:0.000000<br>std:0.000001</td><td bgcolor="white">max:0.000010<br>mean:0.000002<br>median:0.000002<br>min:0.000000<br>rmse:0.000003<br>sse:0.000000<br>std:0.000002</td>
        <td bgcolor="white">max:0.000108<br>mean:0.000017<br>median:0.000014<br>min:0.000000<br>rmse:0.000023<br>sse:0.000001<br>std:0.000015</td><td bgcolor="white">max:0.000110<br>mean:0.000019<br>median:0.000013<br>min:0.000000<br>rmse:0.000028<br>sse:0.000001<br>std:0.000020</td>
        <td bgcolor="white">max:0.000011<br>mean:0.000002<br>median:0.000002<br>min:0.000000<br>rmse:0.000003<br>sse:0.000000<br>std:0.000002</td><td bgcolor="white">max:0.000015<br>mean:0.000003<br>median:0.000002<br>min:0.000000<br>rmse:0.000004<br>sse:0.000000<br>std:0.000003</td>
    </tr>
</table>

<p>各轨迹图统计汇总对比如下：(点击查看大图)</p>
<table>
    <tr><th style="width:10%;">指标</th><th colspan="2" style="text-align:center;">室内(indoor)</th><th colspan="2" style="text-align:center;">室外(outdoor)</th><th colspan="2" style="text-align:center;">走廊(slides)</th></tr>
    <tr><td bgcolor="#F5F5F5"></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Mono_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Rgbd_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Mono_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Rgbd_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Mono_IMU</b></td><td bgcolor="#F5F5F5" style="text-align:center;"><b>Rgbd_IMU</b></td></tr>
    <tr>
        <td bgcolor="white">Trajectory</td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-traj-indoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-traj-indoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-traj-outdoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-traj-outdoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-traj-slides-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-traj-slides-d435i-rgbd.jpg" width="120px"></td>
    </tr>
    <tr>
        <td bgcolor="white">xyz_view</td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-xyz-indoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-xyz-indoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-xyz-outdoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-xyz-outdoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-xyz-slides-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-xyz-slides-d435i-rgbd.jpg" width="120px"></td>
    </tr>
    <tr>
        <td bgcolor="white">rpy_view</td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpy-indoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpy-indoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpy-outdoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpy-outdoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpy-slides-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpy-slides-d435i-rgbd.jpg" width="120px"></td>
    </tr>
    <tr>
        <td bgcolor="white">APE</td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-indoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-indoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-outdoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-outdoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-slides-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-slides-d435i-rgbd.jpg" width="120px"></td>
    </tr>
    <tr>
        <td bgcolor="white">APE-Map</td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-indoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-indoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-outdoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-outdoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-slides-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-ape-map-slides-d435i-rgbd.jpg" width="120px"></td>
    </tr>
    <tr>
        <td bgcolor="white">RPE</td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-indoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-indoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-outdoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-outdoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-slides-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-slides-d435i-rgbd.jpg" width="120px"></td>
    </tr>
    <tr>
        <td bgcolor="white">RPE-Map</td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-indoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-indoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-outdoor-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-outdoor-d435i-rgbd.jpg" width="120px"></td>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-slides-d435i-mono.jpg" width="120px"></td><td width="120px" bgcolor="white"><img align="left" src="/resources/slam-rpe-map-slides-d435i-rgbd.jpg" width="120px"></td>
    </tr>
</table>

<h6 id="5-结论-1"><a href="#5-结论-1" class="headerlink" title="5.结论"></a>5.结论</h6><p>1.infrared-camera整体效果好于rgb-camera，其中rgb-camera运行过程中，会出现的问题：<br>(1) 跳帧(找不到当前活跃地图current active-map就跳回原点、随机初始化角度方向，重画跟踪轨迹)：存在多次。bad case触发条件: 场景移动变化过快(&gt;1m/s)、场景过于相似(走廊翻转180°)；<br>(2) 卡顿(图像帧卡住, 无法活动轨迹)：存在少量，停下重新移动camera会继续运行；而infrared-camera较流畅，不会出现上述问题。</p>
<p>2.场景对比：<br>(1) 轨迹完整度：室内&gt;室外&gt;走廊；</p>
<ul>
<li>室内、室外：整体轨迹保留比较完整，虽然在运行过程中会出现卡顿、跳帧，但最终轨迹结果是完整的；</li>
<li>走廊：infrared-camera没有丢失轨迹，rgb-camera丢失部分轨迹；</li>
</ul>
<p>(2) 轨迹总体误差：走廊&gt;室外&gt;室内；</p>
<h5 id="四、RealSense-D455-rgb-amp-infrared-camera轨迹评估"><a href="#四、RealSense-D455-rgb-amp-infrared-camera轨迹评估" class="headerlink" title="四、RealSense D455: rgb &amp; infrared-camera轨迹评估"></a>四、RealSense D455: rgb &amp; infrared-camera轨迹评估</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和RealSense D435i差异不大，此处不再赘述，注意获取准确对应的相机内参。</p>
<h5 id="五、参考文献"><a href="#五、参考文献" class="headerlink" title="五、参考文献"></a>五、参考文献</h5><p>[1] ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM, Dec, 2021.<br>[2] Orb-Slam3源码：<a href="https://github.com/UZ-SLAMLab/ORB_SLAM3" target="_blank" rel="external">https://github.com/UZ-SLAMLab/ORB_SLAM3</a><br>[3] RealSense-sdk-2.0：<a href="https://github.com/IntelRealSense/librealsense.git" target="_blank" rel="external">https://github.com/IntelRealSense/librealsense.git</a></p>
]]></content>
    
    <summary type="html">
    
      Slam定位、建图、轨迹评估。RealSense D435i、D455，Rolling-Shutter、Global-Shutter等不同相机对于slam计算影响对比。
    
    </summary>
    
      <category term="SLAM" scheme="http://www.dianacody.com/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="http://www.dianacody.com/tags/SLAM/"/>
    
      <category term="Trajectory Evaluation" scheme="http://www.dianacody.com/tags/Trajectory-Evaluation/"/>
    
  </entry>
  
  <entry>
    <title>相机标定</title>
    <link href="http://www.dianacody.com/2022/12/01/2022-12-01-Calib/"/>
    <id>http://www.dianacody.com/2022/12/01/2022-12-01-Calib/</id>
    <published>2022-11-30T16:00:00.000Z</published>
    <updated>2023-08-03T12:14:46.722Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相机标定的目标是获取双目相机参数、imu参数、几者之间的转换矩阵参数。标定的主要工作包括：<br>1.获取双目相机内参+双目转换矩阵；<br>2.获取imu参数，以测定噪声密度、随机游走；<br>3.双目+imu联合标定，获取双目到imu的转换矩阵；</p>
<h5 id="二、相机参数说明"><a href="#二、相机参数说明" class="headerlink" title="二、相机参数说明"></a>二、相机参数说明</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相机参数配置文件camchain.yaml，存储相机内外参的校准、IMU相对于相机的空间/时间校准参数。其所含的参数说明如下：<br><strong>· CAMERA_MODEL</strong>：camera_model(pin_hole/omni)，(针孔/全向)。<br><strong>· intrinsics</strong>：包含给定投影类型的内部参数的向量。要素如下：</p>
<ul>
<li>pinhone: [fu fv pu pv]</li>
<li>omni: [xi fu fv pu pv]</li>
<li>ds: [xi alpha fu fv pu pv]</li>
<li>eucm: [alpha beta fu fv pu pv]</li>
</ul>
<p><strong>· distortion_model</strong>：畸变模型，(radtan/equidistant)。<br><strong>· distortion_coeffs</strong>：畸变参数。<br><strong>· T_cn_cnm1</strong>：相机外在转换，总是相对于链中最后一个相机。（例如：cam1：T_cn_cnm1 = T_c1_c0，将cam0转换为cam1坐标）<br><strong>· T_cam_imu</strong>：imu外参，从imu到相机坐标的转换(T_c_i)。<br><strong>· timeshift_cam_imu</strong>：相机和imu时间戳之间的时间间隔，以秒为单位（t_imu = t_cam + shift）。<br><strong>· rostopic</strong>：摄像机图像流的主题。<br><strong>· resolution</strong>：相机分辨率[width,height]。</p>
<p>示例：</p>
<h6 id="1-相机chain-yaml"><a href="#1-相机chain-yaml" class="headerlink" title="1.相机chain.yaml"></a>1.相机chain.yaml</h6><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="attr">cam0:</span></div><div class="line"><span class="attr">  camera_model:</span> <span class="string">pinhole</span></div><div class="line"><span class="attr">  intrinsics:</span> <span class="string">[461.629,</span> <span class="number">460.152</span><span class="string">,</span> <span class="number">362.680</span><span class="string">,</span> <span class="number">246.049</span><span class="string">]</span></div><div class="line"><span class="attr">  distortion_model:</span> <span class="string">radtan</span></div><div class="line"><span class="attr">  distortion_coeffs:</span> <span class="string">[-0.27695497,</span> <span class="number">0.06712482</span><span class="string">,</span> <span class="number">0.00087538</span><span class="string">,</span> <span class="number">0.00011556</span><span class="string">]</span></div><div class="line"><span class="attr">  T_cam_imu:</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[0.01779318,</span> <span class="number">0.99967549</span><span class="string">,-0.01822936,</span> <span class="number">0.07008565</span><span class="string">]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[-0.9998017,</span> <span class="number">0.01795239</span><span class="string">,</span> <span class="number">0.00860714</span><span class="string">,-0.01771023]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[0.00893160,</span> <span class="number">0.01807260</span><span class="string">,</span> <span class="number">0.99979678</span><span class="string">,</span> <span class="number">0.00399246</span><span class="string">]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[0.0,</span> <span class="number">0.0</span><span class="string">,</span> <span class="number">0.0</span><span class="string">,</span> <span class="number">1.0</span><span class="string">]</span></div><div class="line"><span class="attr">  timeshift_cam_imu:</span> <span class="bullet">-8.121e-05</span></div><div class="line"><span class="attr">  rostopic:</span> <span class="string">/cam0/image_raw</span></div><div class="line"><span class="attr">  resolution:</span> <span class="string">[752,</span> <span class="number">480</span><span class="string">]</span></div><div class="line"><span class="attr">cam1:</span></div><div class="line"><span class="attr">  camera_model:</span> <span class="string">omni</span></div><div class="line"><span class="attr">  intrinsics:</span> <span class="string">[0.80065662,</span> <span class="number">833.006</span><span class="string">,</span> <span class="number">830.345</span><span class="string">,</span> <span class="number">373.850</span><span class="string">,</span> <span class="number">253.749</span><span class="string">]</span></div><div class="line"><span class="attr">  distortion_model:</span> <span class="string">radtan</span></div><div class="line"><span class="attr">  distortion_coeffs:</span> <span class="string">[-0.33518750,</span> <span class="number">0.13211436</span><span class="string">,</span> <span class="number">0.00055967</span><span class="string">,</span> <span class="number">0.00057686</span><span class="string">]</span></div><div class="line"><span class="attr">  T_cn_cnm1:</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[</span> <span class="number">0.99998854</span><span class="string">,</span> <span class="number">0.00216014</span><span class="string">,</span> <span class="number">0.00427195</span><span class="string">,-0.11003785]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[-0.00221074,</span> <span class="number">0.99992702</span><span class="string">,</span> <span class="number">0.01187697</span><span class="string">,</span> <span class="number">0.00045792</span><span class="string">]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[-0.00424598,-0.01188627,</span> <span class="number">0.99992034</span><span class="string">,-0.00064487]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[0.0,</span> <span class="number">0.0</span><span class="string">,</span> <span class="number">0.0</span><span class="string">,</span> <span class="number">1.0</span><span class="string">]</span></div><div class="line"><span class="attr">  T_cam_imu:</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[</span> <span class="number">0.01567142</span><span class="string">,</span> <span class="number">0.99978002</span><span class="string">,-0.01393948,-0.03997419]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[-0.99966203,</span> <span class="number">0.01595569</span><span class="string">,</span> <span class="number">0.02052137</span><span class="string">,-0.01735854]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[</span> <span class="number">0.02073927</span><span class="string">,</span> <span class="number">0.01361317</span><span class="string">,</span> <span class="number">0.99969223</span><span class="string">,</span> <span class="number">0.00326019</span><span class="string">]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[0.0,</span> <span class="number">0.0</span><span class="string">,</span> <span class="number">0.0</span><span class="string">,</span> <span class="number">1.0</span><span class="string">]</span></div><div class="line"><span class="attr">  timeshift_cam_imu:</span> <span class="bullet">-8.681e-05</span></div><div class="line"><span class="attr">  rostopic:</span> <span class="string">/cam1/image_raw</span></div><div class="line"><span class="attr">  resolution:</span> <span class="string">[752,</span> <span class="number">480</span><span class="string">]</span></div></pre></td></tr></table></figure>
<h6 id="2-imu配置：imu-yaml"><a href="#2-imu配置：imu-yaml" class="headerlink" title="2.imu配置：imu.yaml"></a>2.imu配置：imu.yaml</h6><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Accelerometers</span></div><div class="line"><span class="attr">accelerometer_noise_density:</span> <span class="number">1.86e-03</span>   <span class="comment">#Noise density (continuous-time)</span></div><div class="line"><span class="attr">accelerometer_random_walk:</span>   <span class="number">4.33e-04</span>   <span class="comment">#Bias random walk</span></div><div class="line"></div><div class="line"><span class="comment">#Gyroscopes</span></div><div class="line"><span class="attr">gyroscope_noise_density:</span>     <span class="number">1.87e-04</span>   <span class="comment">#Noise density (continuous-time)</span></div><div class="line"><span class="attr">gyroscope_random_walk:</span>       <span class="number">2.66e-05</span>   <span class="comment">#Bias random walk</span></div><div class="line"></div><div class="line"><span class="attr">rostopic:</span>                    <span class="string">/imu0</span>      <span class="comment">#the IMU ROS topic</span></div><div class="line"><span class="attr">update_rate:</span>                 <span class="number">200.0</span>      <span class="comment">#Hz (for discretization of the values above)</span></div></pre></td></tr></table></figure>
<h5 id="三、标定环境搭建"><a href="#三、标定环境搭建" class="headerlink" title="三、标定环境搭建"></a>三、标定环境搭建</h5><h6 id="1-ros安装"><a href="#1-ros安装" class="headerlink" title="1.ros安装"></a><strong>1.ros安装</strong></h6><p>注意当前匹配使用的ubuntu系统版本。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">1.[添加软件源=aliyun]  <span class="comment"># ubuntu 18.04 &amp; 20.04</span></div><div class="line">sudo sh -c <span class="string">'echo "deb http://packages.ros.org/ros/ubuntu $(lsb_release -sc) main" &gt; /etc/apt/sources.list.d/ros-latest.list'</span></div><div class="line">sudo sh -c <span class="string">'. /etc/lsb-release &amp;&amp; echo "deb http://mirrors.ustc.edu.cn/ros/ubuntu/ $DISTRIB_CODENAME main" &gt; /etc/apt/sources.list.d/ros-latest.list'</span></div><div class="line"></div><div class="line">2.[设置秘钥]</div><div class="line">sudo apt-key adv --keyserver <span class="string">'hkp://keyserver.ubuntu.com:80'</span> --recv-key C1CF6E31E6BADE8868B172B4F42ED6FBAB17C654</div><div class="line"></div><div class="line">3.[安装ROS文件]</div><div class="line">sudo apt update</div><div class="line">sudo apt install ros-melodic-desktop-full     <span class="comment"># ubuntu 18.04</span></div><div class="line">sudo apt install ros-noetic-desktop-full      <span class="comment"># ubuntu 20.04 </span></div><div class="line">如果提示找不到rosdep，执行sudo apt install python3-rosdep2，先安装rospack-tools包: sudo apt install rospack-tools</div><div class="line"></div><div class="line">4.[初始化rosdep] <span class="comment"># 安装过程中,执行该步骤,参考其他各种科学上网方法</span></div><div class="line">sudo rosdep init</div><div class="line">rosdep update</div><div class="line"></div><div class="line">5.[设置环境变量]</div><div class="line"><span class="built_in">echo</span> <span class="string">"source /opt/ros/melodic/setup.bash"</span> &gt;&gt; ~/.bashrc      <span class="comment"># ubuntu 18.04</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"source /opt/ros/noetic/setup.bash"</span> &gt;&gt; ~/.bashrc       <span class="comment"># ubuntu 20.04</span></div><div class="line"><span class="built_in">source</span> ~/.bashrc</div><div class="line">执行上述命令后，可以输入ros并按tab键，正常情况下会输出很多ros开头的对应所有命令，如果环境变量设置有问题，将不会出现。</div><div class="line"></div><div class="line">6.[安装rosinstall]</div><div class="line">sudo apt install python-rosinstall python-rosinstall-generator python-wstool build-essential       <span class="comment"># ubuntu 18.04</span></div><div class="line">sudo apt install python3-rosinstall python3-rosinstall-generator python3-wstool build-essential    <span class="comment"># ubuntu 20.04</span></div><div class="line"></div><div class="line">7.[安装检查]</div><div class="line">ROS的默认安装路径为/opt/ros/noetic/，可进入该路径查看安装包是否存在。</div><div class="line">使用基本命令检查ROS是否安装成功。以下三条命令分别打开一个终端进行运行。执行中很多报错依赖包没安装，参考博客原文or百度安装即可。</div><div class="line">roscore                               <span class="comment"># 启动roscore</span></div><div class="line">rosrun turtlesim turtlesim_node       <span class="comment"># 启动小海龟仿真器</span></div><div class="line">rosrun turtlesim turtle_teleop_key    <span class="comment"># 启动海龟控制节点</span></div></pre></td></tr></table></figure></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后是两个utils的安装，这之前有依赖和环境搭建：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">1.[安装依赖项]</div><div class="line">sudo apt-get install libdw-dev</div><div class="line"></div><div class="line">2.[创建ros工作空间]</div><div class="line">mkdir -p ~/Documents/catkin_ws/src</div><div class="line"><span class="built_in">cd</span> ~/Documents/catkin_ws/src</div><div class="line">catkin_init_workspace</div><div class="line"><span class="built_in">cd</span> ~/Documents/catkin_ws/</div><div class="line">catkin_make</div><div class="line"><span class="built_in">source</span> ~/Documents/catkin_ws/devel/setup.bash</div></pre></td></tr></table></figure></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以下2个包，注意：先编译code_utils，再编译imu_utils，不能放在一起编译，imu_utils依赖code_utils。先把code_utils放在工作空间src下编译。再将imu_utils放到src下编译。</p>
<h6 id="2-core-utils安装"><a href="#2-core-utils安装" class="headerlink" title="2.core_utils安装"></a><strong>2.core_utils安装</strong></h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> ~/Documents/catkin_ws/src</div><div class="line">git <span class="built_in">clone</span> https://github.com/gaowenliang/code_utils.git</div><div class="line"><span class="built_in">cd</span> ~/Documents/catkin_ws/</div><div class="line">catkin_make</div></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;报错 fatal error: backward.hpp: 没有那个文件或目录，在code_utils下面找到sumpixel_test.cpp，修改#include “backward.hpp”为#include “code_utils/backward.hpp”后再catkin_make。</p>
<h6 id="3-imu-utils安装"><a href="#3-imu-utils安装" class="headerlink" title="3.imu_utils安装"></a><strong>3.imu_utils安装</strong></h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> ~/Documents/atkin_ws/src</div><div class="line">git <span class="built_in">clone</span> https://github.com/gaowenliang/imu_utils.git</div><div class="line"><span class="built_in">cd</span> ~/Documents/catkin_ws/</div><div class="line">catkin_make</div><div class="line"><span class="built_in">source</span> ~/Documents/catkin_ws/devel/setup.bash</div></pre></td></tr></table></figure>
<h6 id="4-标定工具Kalibr"><a href="#4-标定工具Kalibr" class="headerlink" title="4.标定工具Kalibr"></a><strong>4.标定工具Kalibr</strong></h6><p>官网：<a href="https://github.com/ethz-asl/kalibr" target="_blank" rel="external">https://github.com/ethz-asl/kalibr</a><br>安装文档：<a href="https://github.com/ethz-asl/kalibr/wiki/installation" target="_blank" rel="external">https://github.com/ethz-asl/kalibr/wiki/installation</a><br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">1.[安装依赖项]</div><div class="line">sudo apt-get install -y </div><div class="line">git wget autoconf automake nano </div><div class="line">libeigen3-dev libboost-all-dev libsuitesparse-dev </div><div class="line">doxygen libopencv-dev </div><div class="line">libpoco-dev libtbb-dev libblas-dev liblapack-dev libv4l-dev</div><div class="line"></div><div class="line">2.[安装python包]</div><div class="line">sudo apt-get install -y python3-dev python3-pip python3-scipy </div><div class="line">python3-matplotlib ipython3 python3-wxgtk4.0 python3-tk python3-igraph</div><div class="line"></div><div class="line">3.[建立工作空间]</div><div class="line">mkdir -p ~/文档/catkin_ws/src</div><div class="line"><span class="built_in">cd</span> ~/文档/catkin_ws/</div><div class="line"><span class="built_in">export</span> ROS1_DISTRO=noetic     <span class="comment">#--kinetic=16.04, melodic=18.04, noetic=20.04</span></div><div class="line"><span class="built_in">source</span> /opt/ros/<span class="variable">$ROS1_DISTRO</span>/setup.bash</div><div class="line">catkin init</div><div class="line">catkin config --extend /opt/ros/<span class="variable">$ROS1_DISTRO</span></div><div class="line">catkin config --merge-devel   <span class="comment">#--Necessary for catkin_tools&gt;=0.4.</span></div><div class="line">catkin config --cmake-args -DCMAKE_BUILD_TYPE=Release</div><div class="line"></div><div class="line">4.[源码安装]</div><div class="line"><span class="built_in">cd</span> ~/文档/catkin_ws/src</div><div class="line">git <span class="built_in">clone</span> https://github.com/ethz-asl/kalibr.git</div><div class="line"><span class="built_in">cd</span> ~/文档/catkin_ws/</div><div class="line">catkin build -DCMAKE_BUILD_TYPE=Release -j4</div><div class="line"></div><div class="line">5.[运行]</div><div class="line"><span class="built_in">source</span> ~/Documents/catkin_ws/devel/setup.bash</div><div class="line">rosrun kalibr &lt;command_you_want_to_run_here&gt;</div></pre></td></tr></table></figure></p>
<h5 id="四、双目相机标定"><a href="#四、双目相机标定" class="headerlink" title="四、双目相机标定"></a>四、双目相机标定</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;利用Kalibr工具进行双目标定。</p>
<h6 id="1-创建新ros空间"><a href="#1-创建新ros空间" class="headerlink" title="1.创建新ros空间"></a>1.创建新ros空间</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;已如上述说明。</p>
<h6 id="2-标定板制作"><a href="#2-标定板制作" class="headerlink" title="2.标定板制作"></a>2.标定板制作</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/ethz-asl/kalibr/wiki/downloads" target="_blank" rel="external">下载链接</a>，包含三种类型标定板(Aprilgrid, Checkerboard, Circlegrid)，其中Aprilgrid含序号，能防止计算位姿时跳帧，故用Aprilgrid标定。</p>
<h6 id="3-示例"><a href="#3-示例" class="headerlink" title="3.示例"></a>3.示例</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下载链接同上，含多相机标定(Multiple camera calibration)、IMU+相机联合标定(IMU-camera calibration)，此处先用前者标定相机，包括：相机参数文件camchain.yaml、标定结果文件target.yaml、录制数据集.bag文件。</p>
<h6 id="4-录制bag数据集"><a href="#4-录制bag数据集" class="headerlink" title="4.录制bag数据集"></a>4.录制bag数据集</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了之后imu+相机联合标定，此处采集双目+imu的数据。若已有通过ros发布image和imu消息节点，只需用rosbag record工具将拍摄到的标定板图像制作成.bag文件即可。注意默认设备采集的频率为20~60Hz，会使标定图像过多，计算量太大。采集时最好将ros topic频率降低到4Hz左右。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ros可更改topic发布频率的节点throttle，分别打开两个终端，运行以下命令：(其中原始话题名称：/mynteye/left/image_raw、/mynteye/right/image_raw，更改频率后话题名称：/left、/right)<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">rosrun</div><div class="line">rosrun topic_tools throttle messages /mynteye/left/image_raw 4.0 /left</div><div class="line">rosrun topic_tools throttle messages /mynteye/right/image_raw 4.0 /right</div></pre></td></tr></table></figure></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后录制bag，参考：<a href="https://www.bilibili.com/video/BV1gA411w7EM/?is_story_h5=false&amp;p=1&amp;share_from=ugc&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=4cb33025-a24d-435f-adfc-7e10953ecc92&amp;share_source=WEIXIN&amp;share_tag=s_i&amp;timestamp=1671450944&amp;unique_k=vMzZHAU" target="_blank" rel="external">Kalibr标定视频教程</a>。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rosbag record -O stereo_calibra.bag /left /right /imu</div></pre></td></tr></table></figure></p>
<h6 id="5-进行标定"><a href="#5-进行标定" class="headerlink" title="5.进行标定"></a>5.进行标定</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;进入下载示例Multiple camera calibration文件夹，修改camchain.yaml文件：话题名称、频率、标定板尺寸。（详见视频教程）<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;打开终端输入命令：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">source</span> devel/setup.bash</div><div class="line">rosrun kalibr kalibr_calibrate_cameras --bag /home/admin/stereo_calibra.bag --topics /left /right --models pinhole-equi pinhole-equi --target /home/admin/april_6x6_80x80cm_A0.yaml</div></pre></td></tr></table></figure></p>
<h6 id="6-标定结果"><a href="#6-标定结果" class="headerlink" title="6.标定结果"></a>6.标定结果</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;标定完成后，生成cam-chain.yaml文件，包含：相机内参、畸变参数、基线等。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="attr">cam0:</span></div><div class="line"><span class="attr">  cam_overlaps:</span> <span class="string">[1]</span></div><div class="line"><span class="attr">  camera_model:</span> <span class="string">pinhole</span></div><div class="line"><span class="attr">  distortion_coeffs:</span> <span class="string">[0.2783471016021346,</span> <span class="number">0.6964790605940689</span><span class="string">,</span> <span class="bullet">-1.7636165117280103</span><span class="string">,</span> <span class="number">1.9185066948675424</span><span class="string">]</span></div><div class="line"><span class="attr">  distortion_model:</span> <span class="string">equidistant</span></div><div class="line"><span class="attr">  intrinsics:</span> <span class="string">[464.46660755800724,</span> <span class="number">465.36764628863784</span><span class="string">,</span> <span class="number">345.1018538803555</span><span class="string">,</span> <span class="number">229.48918054311704</span><span class="string">]</span></div><div class="line"><span class="attr">  resolution:</span> <span class="string">[640,</span> <span class="number">480</span><span class="string">]</span></div><div class="line"><span class="attr">  rostopic:</span> <span class="string">/left</span></div><div class="line"><span class="attr">cam1:</span></div><div class="line"><span class="attr">  T_cn_cnm1:</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[0.9996465213666637,</span> <span class="bullet">-0.02537787899957037</span><span class="string">,</span> <span class="number">0.007924366031799834</span><span class="string">,</span> <span class="bullet">-0.11251195025612559</span><span class="string">]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[0.02547649136646326,</span> <span class="number">0.9995959921960228</span><span class="string">,</span> <span class="bullet">-0.012601617884125324</span><span class="string">,</span> <span class="bullet">-0.0017042839059835384</span><span class="string">]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[-0.0076013621922192826,</span> <span class="number">0.012799048524251695</span><span class="string">,</span> <span class="number">0.9998891956860474</span><span class="string">,</span> <span class="bullet">-0.0005072054140188229</span><span class="string">]</span></div><div class="line"><span class="bullet">  -</span> <span class="string">[0.0,</span> <span class="number">0.0</span><span class="string">,</span> <span class="number">0.0</span><span class="string">,</span> <span class="number">1.0</span><span class="string">]</span></div><div class="line"><span class="attr">  cam_overlaps:</span> <span class="string">[0]</span></div><div class="line"><span class="attr">  camera_model:</span> <span class="string">pinhole</span></div><div class="line"><span class="attr">  distortion_coeffs:</span> <span class="string">[0.2682739040565608,</span> <span class="number">0.7579325851647005</span><span class="string">,</span> <span class="bullet">-2.0708675842774156</span><span class="string">,</span></div><div class="line">    <span class="number">2.3962970091118696</span><span class="string">]</span></div><div class="line"><span class="attr">  distortion_model:</span> <span class="string">equidistant</span></div><div class="line"><span class="attr">  intrinsics:</span> <span class="string">[461.8506067496413,</span> <span class="number">462.2467047389745</span><span class="string">,</span> <span class="number">325.2758830849641</span><span class="string">,</span> <span class="number">247.10152372631737</span><span class="string">]</span></div><div class="line"><span class="attr">  resolution:</span> <span class="string">[640,</span> <span class="number">480</span><span class="string">]</span></div><div class="line"><span class="attr">  rostopic:</span> <span class="string">/right</span></div></pre></td></tr></table></figure></p>
<p><div align="center" style="width:60%;height:60%;margin:0 auto;"><img src="/resources/calib-reprojection-error-stereo.jpg" alt="figure-stereo-reprojection-errors"></div></p>
<center><h7 style="color:grey">图1 双目标定的重投影误差</h7></center>

<h5 id="五、IMU标定"><a href="#五、IMU标定" class="headerlink" title="五、IMU标定"></a>五、IMU标定</h5><h6 id="1-录制imu-bag"><a href="#1-录制imu-bag" class="headerlink" title="1.录制imu.bag"></a>1.录制imu.bag</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此处采用imu_utils进行标定。保持imu静止不动至少2小时，录制imu数据集.bag。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rosbag record /imu -O imu</div></pre></td></tr></table></figure></p>
<h6 id="2-修改launch文件"><a href="#2-修改launch文件" class="headerlink" title="2.修改launch文件"></a>2.修改launch文件</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自定义可选，修改src/imu_utils-master/launch文件，包括名字、时长之类。(比如: mynt_imu.launch)。<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">launch</span>&gt;</span></div><div class="line">    <span class="tag">&lt;<span class="name">node</span> <span class="attr">pkg</span>=<span class="string">"imu_utils"</span> <span class="attr">type</span>=<span class="string">"imu_an"</span> <span class="attr">name</span>=<span class="string">"imu_an"</span> <span class="attr">output</span>=<span class="string">"screen"</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"imu_topic"</span> <span class="attr">type</span>=<span class="string">"string"</span> <span class="attr">value</span>=<span class="string">"/imu0"</span>/&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"imu_name"</span> <span class="attr">type</span>=<span class="string">"string"</span> <span class="attr">value</span>=<span class="string">"16448"</span>/&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"data_save_path"</span> <span class="attr">type</span>=<span class="string">"string"</span> <span class="attr">value</span>=<span class="string">"$(find imu_utils)/data/"</span>/&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"max_time_min"</span> <span class="attr">type</span>=<span class="string">"int"</span> <span class="attr">value</span>=<span class="string">"120"</span>/&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">param</span> <span class="attr">name</span>=<span class="string">"max_cluster"</span> <span class="attr">type</span>=<span class="string">"int"</span> <span class="attr">value</span>=<span class="string">"100"</span>/&gt;</span></div><div class="line">    <span class="tag">&lt;/<span class="name">node</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">launch</span>&gt;</span></div></pre></td></tr></table></figure></p>
<h6 id="3-进行标定"><a href="#3-进行标定" class="headerlink" title="3.进行标定"></a>3.进行标定</h6><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">roscore</div><div class="line">rosbag play -r 200　imu_utils/imu.bag    <span class="comment"># 播放bag文件:以200速度播放,而非实际等待时长2h</span></div></pre></td></tr></table></figure>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> imu_ws</div><div class="line"><span class="built_in">source</span> ./devel/setup.bash</div><div class="line">roslaunch imu_utils mynt_imu.launch      <span class="comment"># 运行launch文件</span></div></pre></td></tr></table></figure>
<h6 id="4-标定结果"><a href="#4-标定结果" class="headerlink" title="4.标定结果"></a>4.标定结果</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;标定结束，生成cam-imu.yaml文件，在data目录: src/imu_utils/data/m210_imu_param.yaml，包含：各方向的随机噪声、随机游走。<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="string">%YAML:1.0</span></div><div class="line"><span class="meta">---</span></div><div class="line"><span class="attr">type:</span> <span class="string">IMU</span></div><div class="line"><span class="attr">name:</span> <span class="string">m210</span></div><div class="line"><span class="attr">Gyr:</span></div><div class="line"><span class="attr">   unit:</span> <span class="string">" rad/s"</span></div><div class="line"><span class="attr">   avg-axis:</span></div><div class="line"><span class="attr">      gyr_n:</span> <span class="number">1.1016440992489866e-03</span></div><div class="line"><span class="attr">      gyr_w:</span> <span class="number">5.7968344392887427e-06</span></div><div class="line"><span class="attr">   x-axis:</span></div><div class="line"><span class="attr">      gyr_n:</span> <span class="number">1.2601308952358316e-03</span></div><div class="line"><span class="attr">      gyr_w:</span> <span class="number">6.6549144466689008e-06</span></div><div class="line"><span class="attr">   y-axis:</span></div><div class="line"><span class="attr">      gyr_n:</span> <span class="number">9.1639862355727669e-04</span></div><div class="line"><span class="attr">      gyr_w:</span> <span class="number">4.5920115983527386e-06</span></div><div class="line"><span class="attr">   z-axis:</span></div><div class="line"><span class="attr">      gyr_n:</span> <span class="number">1.1284027789538512e-03</span></div><div class="line"><span class="attr">      gyr_w:</span> <span class="number">6.1435772728445881e-06</span></div><div class="line"><span class="attr">Acc:</span></div><div class="line"><span class="attr">   unit:</span> <span class="string">" m/s^2"</span></div><div class="line"><span class="attr">   avg-axis:</span></div><div class="line"><span class="attr">      acc_n:</span> <span class="number">5.1380013730273288e-02</span></div><div class="line"><span class="attr">      acc_w:</span> <span class="number">1.7544697355051581e-03</span></div><div class="line"><span class="attr">   x-axis:</span></div><div class="line"><span class="attr">      acc_n:</span> <span class="number">3.1145378555559291e-02</span></div><div class="line"><span class="attr">      acc_w:</span> <span class="number">2.9961069437766212e-03</span></div><div class="line"><span class="attr">   y-axis:</span></div><div class="line"><span class="attr">      acc_n:</span> <span class="number">7.0380818146931270e-02</span></div><div class="line"><span class="attr">      acc_w:</span> <span class="number">1.9377334256450410e-03</span></div><div class="line"><span class="attr">   z-axis:</span></div><div class="line"><span class="attr">      acc_n:</span> <span class="number">5.2613844488329294e-02</span></div><div class="line"><span class="attr">      acc_w:</span> <span class="number">3.2956883709381275e-04</span></div></pre></td></tr></table></figure></p>
<h5 id="六、双目-IMU联合标定"><a href="#六、双目-IMU联合标定" class="headerlink" title="六、双目+IMU联合标定"></a>六、双目+IMU联合标定</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目标：获取相机内参到imu的相对转换矩阵。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与kalibr标定双目过程类似，这次在IMU-camera calibration例子基础上，标定imu与camera。首先以上两次标定结果修改.yaml参数，包括：标定结果、话题类型、频率等。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">kalibr_calibrate_imu_camera --target april_6x6.yaml --cam camchain.yaml --imu imu_adis16448.yaml --bag /home/admin/calib/stereo/imu_stereo.bag --bag-from-to 5 45</div></pre></td></tr></table></figure></p>
<h6 id="标定结果"><a href="#标定结果" class="headerlink" title="标定结果"></a>标定结果</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.txt文件中的部分结果，包含相机与imu之间的转换矩阵。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">Calibration results</div><div class="line">===================</div><div class="line">Normalized Residuals</div><div class="line">----------------------------</div><div class="line">Reprojection error (cam0):     mean 0.126389437034, median 0.117608381877, std: 0.0673478862055</div><div class="line">Reprojection error (cam1):     mean 0.132576172002, median 0.12130080264, std: 0.0739372479366</div><div class="line">Gyroscope error (imu0):        mean 0.000146638755352, median 3.16696236165e-07, std: 0.0014737840839</div><div class="line">Accelerometer error (imu0):    mean 1.14000452874e-06, median 1.02723705118e-08, std: 6.0143590628e-06</div><div class="line"></div><div class="line">Residuals</div><div class="line">----------------------------</div><div class="line">Reprojection error (cam0) [px]:     mean 0.126389437034, median 0.117608381877, std: 0.0673478862055</div><div class="line">Reprojection error (cam1) [px]:     mean 0.132576172002, median 0.12130080264, std: 0.0739372479366</div><div class="line">Gyroscope error (imu0) [rad/s]:     mean 5.18446291471e-06, median 1.11969028084e-08, std: 5.21061359864e-05</div><div class="line">Accelerometer error (imu0) [m/s^2]: mean 8.06104932854e-08, median 7.26366284774e-10, std: 4.2527940778e-07</div><div class="line"></div><div class="line">Transformation (cam0):</div><div class="line">-----------------------</div><div class="line">T_ci:  (imu0 to cam0): </div><div class="line">[[-0.02093225 -0.99971259  0.01168657 -0.00021865]</div><div class="line"> [ 0.13518962 -0.01441203 -0.99071492 -0.00086208]</div><div class="line"> [ 0.99059861 -0.01915798  0.13545244  0.00014753]</div><div class="line"> [ 0.          0.          0.          1.        ]]</div><div class="line"></div><div class="line">T_ic:  (cam0 to imu0): </div><div class="line">[[-0.02093225  0.13518962  0.99059861 -0.00003417]</div><div class="line"> [-0.99971259 -0.01441203 -0.01915798 -0.00022818]</div><div class="line"> [ 0.01168657 -0.99071492  0.13545244 -0.00087151]</div><div class="line"> [ 0.          0.          0.          1.        ]]</div><div class="line"></div><div class="line">timeshift cam0 to imu0: [s] (t_imu = t_cam + shift)</div><div class="line">0.000336885304337</div></pre></td></tr></table></figure></p>
<p><div align="center" style="width:60%;height:60%;margin:0 auto;"><img src="/resources/calib-reprojection-error-stereo-imu.jpg" alt="figure-stereo-imu-reprojection-errors"></div></p>
<center><h7 style="color:grey">图2 双目+imu联合标定的重投影误差</h7></center>

<h5 id="七、参考文献"><a href="#七、参考文献" class="headerlink" title="七、参考文献"></a>七、参考文献</h5><p>[1] Kalibr官网：<a href="https://github.com/ethz-asl/kalibr/wiki/" target="_blank" rel="external">https://github.com/ethz-asl/kalibr/wiki/</a><br>[2] Kalibr源码：<a href="https://github.com/ethz-asl/kalibr/" target="_blank" rel="external">https://github.com/ethz-asl/kalibr/</a><br>[3] Kalibr标定教程：<a href="https://www.youtube.com/watch?app=desktop&amp;v=puNXsnrYWTY/" target="_blank" rel="external">https://www.youtube.com/watch?app=desktop&amp;v=puNXsnrYWTY/</a></p>
]]></content>
    
    <summary type="html">
    
      相机标定的目标是获取相机内外参、imu参数、几者间的转换矩阵参数，标定工具Kalibr。
    
    </summary>
    
      <category term="Camera Calibration" scheme="http://www.dianacody.com/categories/Camera-Calibration/"/>
    
    
      <category term="Camera Calibration" scheme="http://www.dianacody.com/tags/Camera-Calibration/"/>
    
  </entry>
  
  <entry>
    <title>TBB并行加速优化Slam计算框架</title>
    <link href="http://www.dianacody.com/2022/10/09/2022-10-09-TBB/"/>
    <id>http://www.dianacody.com/2022/10/09/2022-10-09-TBB/</id>
    <published>2022-10-08T16:00:00.000Z</published>
    <updated>2023-08-03T09:52:30.953Z</updated>
    
    <content type="html"><![CDATA[<h5 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>TBB指令集, Threading Building Blocks(Intel TBB)</strong>：英特尔发布，使用标准 ISO C++代码支持可扩展并行编程的库。不需要特殊的语言或编译器，可扩展数据并行编程。完全支持嵌套并行性，可从较小并行组件构建较大并行组件。使用该库需要指定任务而非线程，并让库以有效的方式将任务映射到线程上。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该库在以下方面与典型的线程包/线程池不同：</p>
<ul>
<li>指定逻辑并行而不是线程。</li>
<li>以线程化为目标，以提高性能。 </li>
<li>兼容其他线程包。 </li>
<li>强调可扩展的数据并行编程。</li>
<li>依赖泛型编程。</li>
</ul>
<h5 id="二、TBB并行指令集函数"><a href="#二、TBB并行指令集函数" class="headerlink" title="二、TBB并行指令集函数"></a>二、TBB并行指令集函数</h5><p>1.并行算法；2.任务调度；3.并行容器；4.同步原语；5.内存分配器；<br><strong>TBB基础函数</strong><br><strong>1.parallel_for</strong>：并行方式遍历一个区间。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">parallel_for(<span class="number">1</span>, <span class="number">20000</span>, [](<span class="keyword">int</span> i)</div><div class="line">&#123;</div><div class="line">	<span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="built_in">endl</span>; </div><div class="line">&#125;);</div><div class="line"></div><div class="line">parallel_for(blocked_range&lt;<span class="keyword">size_t</span>&gt;(<span class="number">0</span>, <span class="number">20000</span>), [](blocked_range&lt;<span class="keyword">size_t</span>&gt;&amp; r)</div><div class="line">&#123;</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">size_t</span> i = r.begin(); i != r.end(); ++i)</div><div class="line">        <span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="built_in">endl</span>; </div><div class="line">&#125;);</div></pre></td></tr></table></figure></p>
<p><strong>2.parallel_do和parallel_for_each</strong>：将算法应用于一个区间。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">vector&lt;size_t&gt; v;</div><div class="line">parallel_do(v.begin(), v.end(), [](size_t i)&#123;cout &lt;&lt; i &lt;&lt; endl; &#125;);</div><div class="line">parallel_for_each(v.begin(), v.end(), [](size_t i)&#123;cout &lt;&lt; i &lt;&lt; endl; &#125;);</div></pre></td></tr></table></figure></p>
<p><strong>3.parallel_reduce</strong>：类似于map_reduce，先将区间自动分组，对每个分组进行聚合(accumulate)计算，每组得到一个结果，最后将各组的结果进行汇聚(reduce)。<br>这个算法稍微复杂一点，parallel_reduce(range,identity,func,reduction)，第一个参数是区间范围，第二个参数是计算的初始值，第三个参数是聚合函数，第四个参数是汇聚参数。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">ParallelSum</span><span class="params">(<span class="keyword">float</span> <span class="built_in">array</span> [], <span class="keyword">size_t</span> n)</span> </span>&#123;</div><div class="line">    <span class="keyword">return</span> parallel_reduce(</div><div class="line">        blocked_range&lt;<span class="keyword">float</span>*&gt;(<span class="built_in">array</span>, <span class="built_in">array</span> + n),</div><div class="line">        <span class="number">0.f</span>,</div><div class="line">        [](<span class="keyword">const</span> blocked_range&lt;<span class="keyword">float</span>*&gt;&amp; r, <span class="keyword">float</span> value)-&gt;<span class="keyword">float</span> &#123;</div><div class="line">            <span class="keyword">return</span> <span class="built_in">std</span>::accumulate(r.begin(), r.end(), value);</div><div class="line">    &#125;,</div><div class="line">        <span class="built_in">std</span>::plus&lt;<span class="keyword">float</span>&gt;()</div><div class="line">        );</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这数组求和例子就是先自动分组然后对各组中的元素进行聚合累加，最后将各组结果汇聚相加。<br><strong>4.parallel_pipeline</strong>：并行的管道过滤器数据流经过一个管道，在数据流动的过程中依次要经过一些过滤器的处理，其中有些过滤器可能会并行处理数据，这时就可以用到并行的管道过滤器。举一个例子，比如我要读入一个文件，先将文件中的数字提取出来，再将提取出来的数字做一个转换，最后将转换后的数字输出到另外一个文件中。其中读文件和输出文件不能并行去做，但是中间数字转换环节可并行。parallel_pipeline的原型：<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">parallel_pipeline( max_number_of_live_tokens, </div><div class="line">                   make_filter&lt;<span class="keyword">void</span>,I1&gt;(mode0,g0) &amp;</div><div class="line">                   make_filter&lt;I1,I2&gt;(mode1,g1) &amp;</div><div class="line">                   make_filter&lt;I2,I3&gt;(mode2,g2) &amp;</div><div class="line">                   ... </div><div class="line">                   make_filter&lt;In,<span class="keyword">void</span>&gt;(moden,gn) );</div></pre></td></tr></table></figure></p>
<p>第一个参数是最大的并行数，可以通过&amp;连接多个filter，这些filter是顺序执行的，前一个filter的输出是下一个filter的输入。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">RootMeanSquare</span><span class="params">( <span class="keyword">float</span>* first, <span class="keyword">float</span>* last )</span> </span>&#123;</div><div class="line">    <span class="keyword">float</span> sum=<span class="number">0</span>;</div><div class="line">    parallel_pipeline( <span class="comment">/*max_number_of_live_token=*/</span><span class="number">16</span>,       </div><div class="line">        make_filter&lt;<span class="keyword">void</span>,<span class="keyword">float</span>*&gt;(</div><div class="line">            filter::serial,</div><div class="line">            [&amp;](flow_control&amp; fc)-&gt; <span class="keyword">float</span>*&#123;</div><div class="line">                <span class="keyword">if</span>( first&lt;last ) &#123;</div><div class="line">                    <span class="keyword">return</span> first++;</div><div class="line">                 &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    fc.stop();</div><div class="line">                    <span class="keyword">return</span> <span class="literal">NULL</span>;</div><div class="line">                &#125;</div><div class="line">            &#125;    </div><div class="line">        ) &amp;</div><div class="line">        make_filter&lt;<span class="keyword">float</span>*,<span class="keyword">float</span>&gt;(</div><div class="line">            filter::parallel,</div><div class="line">            [](<span class="keyword">float</span>* p)&#123;<span class="keyword">return</span> (*p)*(*p);&#125; </div><div class="line">        ) &amp;</div><div class="line">        make_filter&lt;<span class="keyword">float</span>,<span class="keyword">void</span>&gt;(</div><div class="line">            filter::serial,</div><div class="line">            [&amp;](<span class="keyword">float</span> x) &#123;sum+=x;&#125;</div><div class="line">        )</div><div class="line">    );</div><div class="line">    <span class="keyword">return</span> <span class="built_in">sqrt</span>(sum);</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>第一个filter生成数据（如从文件中读取数据等），第二个filter对产生的数据进行转换，第三个filter是对转换后的数据做累加。其中第二个filter是可以并行处理的，通过filter::parallel来指定其处理模式。<br><strong>5.parallel_sort</strong>：并行排序。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">1000000</span>;</div><div class="line"><span class="keyword">float</span> a[N];</div><div class="line"><span class="keyword">float</span> b[N];</div><div class="line">parallel_sort(a, a + N);</div><div class="line">parallel_sort(b, b + N, <span class="built_in">std</span>::greater&lt;<span class="keyword">float</span>&gt;());</div></pre></td></tr></table></figure></p>
<p><strong>6.parallel_invoke</strong>：并行调用，并行调用多个函数。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">()</span></span>;</div><div class="line"><span class="function"><span class="keyword">extern</span> <span class="keyword">void</span> <span class="title">bar</span><span class="params">(<span class="keyword">int</span>)</span></span>;</div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">RunFunctionsInParallel</span><span class="params">()</span> </span>&#123;</div><div class="line">    tbb::parallel_invoke(f, []&#123;bar(<span class="number">2</span>);&#125;, []&#123;bar(<span class="number">3</span>);&#125; );</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p><strong>TBB任务</strong><br><strong>task_group</strong>：表示可以等待或者取消的任务集合。<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">task_group g;</div><div class="line">g.run([]&#123;TestPrint(); &#125;);</div><div class="line">g.run([]&#123;TestPrint(); &#125;);</div><div class="line">g.run([]&#123;TestPrint(); &#125;);</div><div class="line">g.wait()</div></pre></td></tr></table></figure></p>
<h5 id="三、加速版ORB-SLAM3-FAST"><a href="#三、加速版ORB-SLAM3-FAST" class="headerlink" title="三、加速版ORB-SLAM3-FAST"></a>三、加速版ORB-SLAM3-FAST</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;已有的开源项目：<a href="https://github.com/hellovuong/ORB_SLAM3_FAST" target="_blank" rel="external">https://github.com/hellovuong/ORB_SLAM3_FAST</a> ，by hellovuong，at 2022.1。其不是直接加载for循环上的parallel_for，SLAM3_FAST框架是在图像的特征提取和后端优化部分做了tbb加速, 目前支持“TUM-VI和EuRoC”的“单目+双目”数据集。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实测，30s的Mono-TUM数据集，时间=0.02s数量级，Fast版本比Orb-Slam3原生版本确实都快0.002s左右，<h7 style="color:blue">提速17%~18%</h7>左右。但是该版本仅修改了少部分，还有大量Orb-Slam3的源码没有优化。</p>
<h5 id="四、TBB加速ORB-SLAM3"><a href="#四、TBB加速ORB-SLAM3" class="headerlink" title="四、TBB加速ORB-SLAM3"></a>四、TBB加速ORB-SLAM3</h5><h6 id="1-改造方法"><a href="#1-改造方法" class="headerlink" title="1.改造方法"></a>1.改造方法</h6><p>(1)代码加速：parallel_for/parallel_reduce/parallel_invoke，修改并行计算模块：<br><strong>OrbExtracter.cc</strong>, computeKeyPointsOctTree(); DistributeOctTree(); computeOrientation();<br><strong>OrbMatcher.cc</strong>, SearchByProjection(); ComputeThreeMaxima(); SearchForInitialization();<br><strong>Optimizer.cc</strong>, PoseOptimization().lock()/mono()/right-fisheye()/stereo();<br><strong>Frame.cc</strong>, ComputeStereoMatches(); UndistortKeyPoints();<br><strong>KeyFrame.cc</strong>: KeyFrame(); UpdateBestCovisibles(); TrackedMapPoints();<br><strong>KeyFrameDataBase.cc</strong>: -<br><strong>Map.cc</strong>: -<br><strong>MapPoint.cc</strong>: -   ComputeDistinctiveDescriptors();<br><strong>LocalMapping.cc</strong>: searchInNeighbors(); KeyFrameCulling();  InitializeIMU();<br><strong>LoopClosing.cc</strong>: deleteCommonRegionsFromBow();<br><strong>System.cc</strong>: -<br><strong>Tracking.cc</strong>, Track(); CreateInitialMapMonocular(); TrackWithMotionModel(); TrackLocalMap();<br>CreateNewKeyFrame(); SearchLocalPoints(); UpdateLocalKeyFrames();<br>(2)算法加速：前端(图像特征提取并行化)/后端优化/回环检测；</p>
<h6 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2.数据集"></a>2.数据集</h6><p>单目mono、单目IMU。共10个数据集，具体如下：</p>
<ul>
<li>mono_tum1</li>
<li>mono_tum2</li>
<li>mono_tum_vi_room4</li>
<li>mono_tum_vi_slides1</li>
<li>mono_euroc_MH01</li>
<li>mono_euroc_MH03</li>
<li>mono_imu_tum_vi_room4</li>
<li>mono_imu_tum_vi_slides1</li>
<li>mono_imu_euroc_MH01</li>
<li>mono_imu_euroc_MH03</li>
</ul>
<h6 id="3-统计指标"><a href="#3-统计指标" class="headerlink" title="3.统计指标"></a>3.统计指标</h6><p>按以下三个指标统计绝对时间、提速率。</p>
<ul>
<li>totalTime – 总运行时间</li>
<li>medianTrackingTime – 轨迹中位数时间</li>
<li>meanTrackingTime – 轨迹平均时间</li>
</ul>
<p>注：<h7 style="color:blue"><strong>提速率 = (测试时间-基准时间) / 基准时间 * 100%</strong></h7>。</p>
<h6 id="4-绝对时间统计"><a href="#4-绝对时间统计" class="headerlink" title="4.绝对时间统计"></a>4.绝对时间统计</h6><div class="table-container">
<table>
<thead>
<tr>
<th>totalTime</th>
<th>ori</th>
<th>fast</th>
<th>tbb</th>
</tr>
</thead>
<tbody>
<tr>
<td>mono_tum1</td>
<td>22.9099</td>
<td>20.3967</td>
<td>18.819</td>
</tr>
<tr>
<td>mono_tum2</td>
<td>128.491</td>
<td>116.23</td>
<td>110.91</td>
</tr>
<tr>
<td>mono_tum_vi_room4</td>
<td>82.30296326</td>
<td>74.69300079</td>
<td>69.7</td>
</tr>
<tr>
<td>mono_tum_vi_slides1</td>
<td>198.2973328</td>
<td>184.109024</td>
<td>156.94</td>
</tr>
<tr>
<td>mono_euroc_MH01</td>
<td>113.8627319</td>
<td>98.12454987</td>
<td>84.973</td>
</tr>
<tr>
<td>mono_euroc_MH03</td>
<td>83.69143677</td>
<td>70.68492126</td>
<td>59.8</td>
</tr>
<tr>
<td>mono_imu_tum_vi_room4</td>
<td>79.43052673</td>
<td>64.13159943</td>
<td>64.208</td>
</tr>
<tr>
<td>mono_imu_tum_vi_slides1</td>
<td>192.7673187</td>
<td>173.0284576</td>
<td>150.48</td>
</tr>
<tr>
<td>mono_imu_euroc_MH01</td>
<td>125.3083267</td>
<td>104.5439606</td>
<td>90.488</td>
</tr>
<tr>
<td>mono_imu_euroc_MH03</td>
<td>97.23898315</td>
<td>80.485672</td>
<td>62.913</td>
</tr>
</tbody>
</table>
</div>
<p><div align="center" style="width:75%;height:75%;margin:0 auto;"><img src="/resources/tbb-total-time.jpg" alt="figure-total-time"></div></p>
<center><h7 style="color:grey">图1 totalTime提速时间</h7></center>

<div class="table-container">
<table>
<thead>
<tr>
<th>medianTrackingTime</th>
<th>ori</th>
<th>fast</th>
<th>tbb</th>
</tr>
</thead>
<tbody>
<tr>
<td>mono_tum1</td>
<td>0.0246596</td>
<td>0.0229959</td>
<td>0.02178</td>
</tr>
<tr>
<td>mono_tum2</td>
<td>0.0225559</td>
<td>0.0210467</td>
<td>0.020612</td>
</tr>
<tr>
<td>mono_tum_vi_room4</td>
<td>0.031520639</td>
<td>0.029292193</td>
<td>0.0289</td>
</tr>
<tr>
<td>mono_tum_vi_slides1</td>
<td>0.031224145</td>
<td>0.028130215</td>
<td>0.025574</td>
</tr>
<tr>
<td>mono_euroc_MH01</td>
<td>0.028075576</td>
<td>0.024197916</td>
<td>0.021836</td>
</tr>
<tr>
<td>mono_euroc_MH03</td>
<td>0.027769892</td>
<td>0.024874028</td>
<td>0.020141</td>
</tr>
<tr>
<td>mono_imu_tum_vi_room4</td>
<td>0.030900763</td>
<td>0.025430075</td>
<td>0.025349</td>
</tr>
<tr>
<td>mono_imu_tum_vi_slides1</td>
<td>0.030611012</td>
<td>0.026431446</td>
<td>0.024964</td>
</tr>
<tr>
<td>mono_imu_euroc_MH01</td>
<td>0.031929269</td>
<td>0.025167587</td>
<td>0.022284</td>
</tr>
<tr>
<td>mono_imu_euroc_MH03</td>
<td>0.031767674</td>
<td>0.023637912</td>
<td>0.021577</td>
</tr>
</tbody>
</table>
</div>
<p><div align="center" style="width:75%;height:75%;margin:0 auto;"><img src="/resources/tbb-median-tracking-time.jpg" alt="figure-median-tracking-time"></div></p>
<center><h7 style="color:grey">图2 medianTrackingTime提速时间</h7></center>

<div class="table-container">
<table>
<thead>
<tr>
<th>meanTrackingTime</th>
<th>ori</th>
<th>fast</th>
<th>tbb</th>
</tr>
</thead>
<tbody>
<tr>
<td>mono_tum1</td>
<td>0.0287092</td>
<td>0.0263192</td>
<td>0.023583</td>
</tr>
<tr>
<td>mono_tum2</td>
<td>0.0247957</td>
<td>0.0223593</td>
<td>0.021404</td>
</tr>
<tr>
<td>mono_tum_vi_room4</td>
<td>0.036940288</td>
<td>0.034013015</td>
<td>0.031284</td>
</tr>
<tr>
<td>mono_tum_vi_slides1</td>
<td>0.035524424</td>
<td>0.032774099</td>
<td>0.028116</td>
</tr>
<tr>
<td>mono_euroc_MH01</td>
<td>0.030924153</td>
<td>0.027365711</td>
<td>0.023078</td>
</tr>
<tr>
<td>mono_euroc_MH03</td>
<td>0.030996829</td>
<td>0.027883305</td>
<td>0.022148</td>
</tr>
<tr>
<td>mono_imu_tum_vi_room4</td>
<td>0.035651043</td>
<td>0.03027271</td>
<td>0.028819</td>
</tr>
<tr>
<td>mono_imu_tum_vi_slides1</td>
<td>0.034533735</td>
<td>0.030789048</td>
<td>0.026958</td>
</tr>
<tr>
<td>mono_imu_euroc_MH01</td>
<td>0.03403268</td>
<td>0.029109169</td>
<td>0.024576</td>
</tr>
<tr>
<td>mono_imu_euroc_MH03</td>
<td>0.036014438</td>
<td>0.031513211</td>
<td>0.023301</td>
</tr>
</tbody>
</table>
</div>
<p><div align="center" style="width:75%;height:75%;margin:0 auto;"><img src="/resources/tbb-mean-tracking-time.jpg" alt="figure-mean-tracking-time"></div></p>
<center><h7 style="color:grey">图3 meanTrackingTime提速时间</h7></center>

<h6 id="5-提速率统计"><a href="#5-提速率统计" class="headerlink" title="5.提速率统计"></a>5.提速率统计</h6><div class="table-container">
<table>
<thead>
<tr>
<th>totalTime</th>
<th>ori</th>
<th>fast</th>
<th>tbb</th>
</tr>
</thead>
<tbody>
<tr>
<td>mono_tum1</td>
<td>0.00%</td>
<td>10.97%</td>
<td>17.86%</td>
</tr>
<tr>
<td>mono_tum2</td>
<td>0.00%</td>
<td>9.54%</td>
<td>13.68%</td>
</tr>
<tr>
<td>mono_tum_vi_room4</td>
<td>0.00%</td>
<td>9.25%</td>
<td>15.31%</td>
</tr>
<tr>
<td>mono_tum_vi_slides1</td>
<td>0.00%</td>
<td>7.16%</td>
<td>20.86%</td>
</tr>
<tr>
<td>mono_euroc_MH01</td>
<td>0.00%</td>
<td>13.82%</td>
<td>25.37%</td>
</tr>
<tr>
<td>mono_euroc_MH03</td>
<td>0.00%</td>
<td>15.54%</td>
<td>28.55%</td>
</tr>
<tr>
<td>mono_imu_tum_vi_room4</td>
<td>0.00%</td>
<td>19.26%</td>
<td>19.16%</td>
</tr>
<tr>
<td>mono_imu_tum_vi_slides1</td>
<td>0.00%</td>
<td>10.24%</td>
<td>21.94%</td>
</tr>
<tr>
<td>mono_imu_euroc_MH01</td>
<td>0.00%</td>
<td>16.57%</td>
<td>27.79%</td>
</tr>
<tr>
<td>mono_imu_euroc_MH03</td>
<td>0.00%</td>
<td>17.23%</td>
<td>35.30%</td>
</tr>
</tbody>
</table>
</div>
<p><div align="center" style="width:75%;height:75%;margin:0 auto;"><img src="/resources/tbb-total-time-ratio.jpg" alt="figure-total-time-ratio"></div></p>
<center><h7 style="color:grey">图4 totalTime提速率</h7></center>

<div class="table-container">
<table>
<thead>
<tr>
<th>medianTrackingTime</th>
<th>ori</th>
<th>fast</th>
<th>tbb</th>
</tr>
</thead>
<tbody>
<tr>
<td>mono_tum1</td>
<td>0.00%</td>
<td>6.75%</td>
<td>11.68%</td>
</tr>
<tr>
<td>mono_tum2</td>
<td>0.00%</td>
<td>6.69%</td>
<td>8.62%</td>
</tr>
<tr>
<td>mono_tum_vi_room4</td>
<td>0.00%</td>
<td>7.07%</td>
<td>8.31%</td>
</tr>
<tr>
<td>mono_tum_vi_slides1</td>
<td>0.00%</td>
<td>9.91%</td>
<td>18.10%</td>
</tr>
<tr>
<td>mono_euroc_MH01</td>
<td>0.00%</td>
<td>13.81%</td>
<td>22.22%</td>
</tr>
<tr>
<td>mono_euroc_MH03</td>
<td>0.00%</td>
<td>10.43%</td>
<td>27.47%</td>
</tr>
<tr>
<td>mono_imu_tum_vi_room4</td>
<td>0.00%</td>
<td>17.70%</td>
<td>17.97%</td>
</tr>
<tr>
<td>mono_imu_tum_vi_slides1</td>
<td>0.00%</td>
<td>13.65%</td>
<td>18.45%</td>
</tr>
<tr>
<td>mono_imu_euroc_MH01</td>
<td>0.00%</td>
<td>21.18%</td>
<td>30.21%</td>
</tr>
<tr>
<td>mono_imu_euroc_MH03</td>
<td>0.00%</td>
<td>25.59%</td>
<td>32.08%</td>
</tr>
</tbody>
</table>
</div>
<p><div align="center" style="width:75%;height:75%;margin:0 auto;"><img src="/resources/tbb-median-tracking-time-ratio.jpg" alt="figure-median-tracking-time-ratio"></div></p>
<center><h7 style="color:grey">图5 medianTrackingTime提速率</h7></center>

<div class="table-container">
<table>
<thead>
<tr>
<th>meanTrackingTime</th>
<th>ori</th>
<th>fast</th>
<th>tbb</th>
</tr>
</thead>
<tbody>
<tr>
<td>mono_tum1</td>
<td>0.00%</td>
<td>8.32%</td>
<td>17.86%</td>
</tr>
<tr>
<td>mono_tum2</td>
<td>0.00%</td>
<td>9.83%</td>
<td>13.68%</td>
</tr>
<tr>
<td>mono_tum_vi_room4</td>
<td>0.00%</td>
<td>7.92%</td>
<td>15.31%</td>
</tr>
<tr>
<td>mono_tum_vi_slides1</td>
<td>0.00%</td>
<td>7.74%</td>
<td>20.85%</td>
</tr>
<tr>
<td>mono_euroc_MH01</td>
<td>0.00%</td>
<td>11.51%</td>
<td>25.37%</td>
</tr>
<tr>
<td>mono_euroc_MH03</td>
<td>0.00%</td>
<td>10.04%</td>
<td>28.55%</td>
</tr>
<tr>
<td>mono_imu_tum_vi_room4</td>
<td>0.00%</td>
<td>15.09%</td>
<td>19.16%</td>
</tr>
<tr>
<td>mono_imu_tum_vi_slides1</td>
<td>0.00%</td>
<td>10.84%</td>
<td>21.94%</td>
</tr>
<tr>
<td>mono_imu_euroc_MH01</td>
<td>0.00%</td>
<td>14.47%</td>
<td>27.79%</td>
</tr>
<tr>
<td>mono_imu_euroc_MH03</td>
<td>0.00%</td>
<td>12.50%</td>
<td>35.30%</td>
</tr>
</tbody>
</table>
</div>
<p><div align="center" style="width:75%;height:75%;margin:0 auto;"><img src="/resources/tbb-mean-tracking-time-ratio.jpg" alt="figure-mean-tracking-time-ratio"></div></p>
<center><h7 style="color:grey">图6 meanTrackingTime提速率</h7></center>

<p>总体：Slam-FAST稳定<h7 style="color:blue">提速17%~18%</h7>，tbb改造版本<h7 style="color:red">提速20%~30%</h7>。<br>分析：除了tbb的代码级别改造，Slam-FAST版本还有其他优化部分没增加完，在此基础上可以继续修改优化。</p>
<h5 id="五、参考文献"><a href="#五、参考文献" class="headerlink" title="五、参考文献"></a>五、参考文献</h5><p>[1] TBB官网：<a href="https://software.intel.com/content/www/us/en/develop/tools/threading-building-blocks.html" target="_blank" rel="external">https://software.intel.com/content/www/us/en/develop/tools/threading-building-blocks.html</a><br>[2] Pro TBB: C++ Parallel Programming with Threading Building Blocks<br><a href="https://book.douban.com/subject/35004080/" target="_blank" rel="external">https://book.douban.com/subject/35004080/</a><br>[3] TBB指令集c++并行化：<a href="https://www.bilibili.com/video/av508224172/" target="_blank" rel="external">https://www.bilibili.com/video/av508224172/</a></p>
]]></content>
    
    <summary type="html">
    
      TBB并行加速库可以区别于传统的并发编程，其在多核上并行计算，区别于多线程并发(实质仍旧是时间片的轮换调度)，可以极大有效节省计算时间。Orb-Slam3是一个基于图像特征点提取的SLAM计算框架，但其开源版本的计算速率还有待优化。利用TBB并行优化可提高SLAM框架计算的实时性。
    
    </summary>
    
      <category term="Parallel Computing" scheme="http://www.dianacody.com/categories/Parallel-Computing/"/>
    
    
      <category term="Parallel Computing" scheme="http://www.dianacody.com/tags/Parallel-Computing/"/>
    
      <category term="Multicore Computing" scheme="http://www.dianacody.com/tags/Multicore-Computing/"/>
    
  </entry>
  
  <entry>
    <title>裂变传播网络中关键用户识别算法</title>
    <link href="http://www.dianacody.com/2021/04/02/2021-04-02-FissionPropagation/"/>
    <id>http://www.dianacody.com/2021/04/02/2021-04-02-FissionPropagation/</id>
    <published>2021-04-01T16:00:00.000Z</published>
    <updated>2021-08-23T01:33:41.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Keywords</strong>：<br>裂变传播、信息分发网络、深度复杂网络、高潜用户识别、关键意见领袖；<br>图算法社交网络关系挖掘、关键传播人群识别、关键意见领袖挖掘、PageRank &amp; LeaderRank；<br>深度复杂网络传播模型；</p>
<h5 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h5><h6 id="1-技术领域"><a href="#1-技术领域" class="headerlink" title="1.技术领域"></a>1.技术领域</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要应用方向涉及互联网中的用户社交网络信息传播、网络中关键路由节点识别等。</p>
<h6 id="2-业界现状"><a href="#2-业界现状" class="headerlink" title="2.业界现状"></a>2.业界现状</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;互联网用户增长最重要的就是获客，通常是挖掘基于口碑/分享等行为带来新用户的优质老用户，帮助信息分发推广传播，提高信息传播效率和准确率，帮助高效地获取新用户。主要分析用户社交行为数据，以帮助获取新用户，提高信息推广效果。现有技术通常基于用户在社交网络分享带来新用户行为和日常购买行为，构建每个用户N级关系网络拓扑结构；提取用户行为特征，建立模型计算每个用户节点在关系网络的传播影响力评分，以给用户个性化推荐分发信息。</p>
<h6 id="3-技术问题"><a href="#3-技术问题" class="headerlink" title="3.技术问题"></a>3.技术问题</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当前技术仍旧存在一些问题：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.常用做法没有将用户进行分层分簇处理，无法识别高效分发用户节点和分发效率较低的节点，只是给用户节点按统一权重优先级进行信息分发，整体信息分发效率不高；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.整个用户网络的节点数量及连接边是动态更新的，若动态网络中信息传播按此一般方法处理，对于连接较少的边缘节点难以获取其真实信息分发影响力，因此存在冷启动问题，需要长时间迭代才能使网络稳定。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;针对这些问题，我们提出了一种新方案，可以对这些问题进行改进：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.原始数据预处理，利用主题模型将用户分层分簇，达到精细化目的，再在每个簇中基于多维度用户特征构建网络结构，从而构建整个连接网络。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.对于新加入节点的冷启动问题，改进网络节点排名算法。第一，加入结合用户多维度特征的冷启动，有效避免新节点加入不能更快收敛的问题；第二，结合用户多维度特征，优化网络边计算权重公式，能更快更准确地发现网络高影响力节点。</p>
<h5 id="二、技术方案"><a href="#二、技术方案" class="headerlink" title="二、技术方案"></a>二、技术方案</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们提出了一种信息分发网络中关键用户识别的算法。总体步骤如下：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.通过用户多维度基础特征、社交网络活跃度特征，构建用户基础画像特征，利用模型综合评分，计算得到用户基础特征评分；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.通过用户沟通文本挖掘，文本LDA主题聚类得到top主题簇及每个主题簇下的用户、和通过用户行为聚类得到用户簇。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.通过用户社交网络分享/链接转发/关注/邀请等行为事件构建关系网络，生成图的邻接矩阵；历史转发分享行为事件的频率，作为特征，训练评分，用以修改网络边的权重；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.计算网络每个节点的出度和连接边权重，得到每个节点的影响力分数，和用户基础特征的评分加权融合。在每个簇内，(1)节点按影响力分数由高到低排名，取Top-N作为关键信息分发节点用户；(2)按影响力分数排名取分位数划分得到高潜/中潜/低潜用户分档。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;整体架构图如下：</p>
<p><div align="center" style="width:75%;height:75%;margin:0 auto;"><img src="/resources/fp-flow.jpg" alt="figure-fp-flow"></div></p>
<center><h7 style="color:grey">图1 整体基础架构图</h7></center>

<h6 id="1-基础画像特征"><a href="#1-基础画像特征" class="headerlink" title="1.基础画像特征"></a>1.基础画像特征</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;多维度用户基础画像特征建模，将用户特征分为几大类，基于此构建特征矩阵。</p>
<ul>
<li><strong>产品质量体验类</strong>：产品使用率、复购率；反馈好/中/差次数；反馈次数、反馈满意/不满意次数；</li>
<li><strong>利益折扣类</strong>：折扣敏感度、折扣金额、购买力；购买金额/累计购买金额、购买次数/累计购买次数；最近一次购买距现在时长；近半年购买数量；首次/末次购买时间；</li>
<li><strong>积分成就类</strong>：用户是否会员、积分数、积分使用情况；会员、优惠券、积分、金币数量、钻石数量；</li>
<li><strong>活跃因子类</strong>：用户登录行为（登录频率/登录时长）、浏览/点击/订单行为、用户微信活跃度、用户转发数、评论数；首次注册渠道/首次注册时间；近3/7/14/30/90天登陆app次数、近7/14/30天曝光次数、近7/14/30天点击次数；首次/末次登陆app时间；</li>
<li><strong>用户基本属性类</strong>：用户画像（性格外向/内向、积极/消极、用户地理位置、用户城市及等级）；性别、年龄、常驻省份、常驻城市、购买用户类型（RFM分层）</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过特征训练模型，得到用户评分，据此作为用户网络节点的影响力评分；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;类比RFM分层融合评分，每维特征分层打分然后得到综合得分，由此网络图节点的多维度影响力评分的计算方式如下：</p>
<script type="math/tex; mode=display">impact\_score = \sum_{i} score_{i}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;可得图节点归一化分数：</p>
<script type="math/tex; mode=display">normalization\_impact\_score = \frac{score_{i}-\min(impact\_score)}{\max(impact\_score)-\min(impact\_score)}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;记录用户历史数据中每次分享转发、关注、邀请等行为，例如用户A转发分享信息到用户B，则节点A和B之间建立连接边，对于历史数据中A若有多次转发给B，则增加A到B边的权重评分。由此建立整个网络的连接边及边的权重weight。网络每次迭代更新边的权重。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;连接边权重的计算公式，详见后4.3小节-改进网络节点排名算法，节点边权重因子<script type="math/tex">\omega_{ji}</script>归一化计算式。</p>
<h6 id="2-文本挖掘和用户情感分析"><a href="#2-文本挖掘和用户情感分析" class="headerlink" title="2.文本挖掘和用户情感分析"></a>2.文本挖掘和用户情感分析</h6><h6 id="2-1-文本预处理"><a href="#2-1-文本预处理" class="headerlink" title="2.1.文本预处理"></a>2.1.文本预处理</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过用户在微信等主要社交app上的沟通文本，提取用户文本相关特征。<br>(1) 取历史定长时间窗口（例如三个月），将每个用户的沟通文本按时间序列聚合为一个长句子，考虑词序上下文关系，得到文本句子集合；<br>(2) 文本过滤：去除标点符号、特殊符号、数字、非法字符、表情包、链接；<br>(3) 文本句子分词，得到二元组2-gram、三元组3-gram、四元组4-gram、…；<br>(4) 去除停用词stop-words；<br>(5) 由词向量得到句子向量，（采用word2vec等方法进行词向量训练）；并联合join上用户其他维度特征features；</p>
<h6 id="2-2-用户情感分析"><a href="#2-2-用户情感分析" class="headerlink" title="2.2.用户情感分析"></a>2.2.用户情感分析</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;利用机器学习模型对用户文本进行二分类(积极正向/消极负向)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型：贝叶斯文本分类器、或梯度树模型GBDT等。</p>
<h6 id="3-文本主题聚类"><a href="#3-文本主题聚类" class="headerlink" title="3.文本主题聚类"></a>3.文本主题聚类</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采用LDA主题模型对用户文本进行主题聚类，提取重要主题。主要为：第一，由文本提取出top主题；第二，获取每个主题下的词的分布；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)由文本聚类得到主题。文本切词，然后LDA聚类，得到topN主题。对于词个数太少（小于某个设定阈值）的簇，无法聚类为主题的，过滤掉。得到如下所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>主题</th>
<th>词簇</th>
</tr>
</thead>
<tbody>
<tr>
<td>Topic 1</td>
<td>[text1]: word1, word2, …, wordK</td>
</tr>
<tr>
<td>Topic 2</td>
<td>[text2]: word2, word3, … , wordM</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>Topic N</td>
<td>[textN]: word5, word7, …, wordX</td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)采用LDA主题模型训练，得到切词后每条文本，对应每个主题下的词的概率分布。如下所示：（表中数值为举例示意）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>prob</th>
<th>Topic 1</th>
<th>Topic 2</th>
<th>…</th>
<th>Topic N</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text 1</td>
<td>0.226</td>
<td>0.213</td>
<td>…</td>
<td>0.005</td>
</tr>
<tr>
<td>Text 2</td>
<td>0.526</td>
<td>0.498</td>
<td>…</td>
<td>0.004</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>Text M</td>
<td>0.154</td>
<td>0.112</td>
<td>…</td>
<td>0.001</td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)上表中，每条文本关联上user_id列，得到每个用户-文本对应在每个topic下的概率。然后选每个用户下最大概率的topic作为用户当前所属的topic。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>user_id</th>
<th>text</th>
<th>topic</th>
</tr>
</thead>
<tbody>
<tr>
<td>user_id 1</td>
<td>text 1</td>
<td>topic A</td>
</tr>
<tr>
<td>user_id 2</td>
<td>text 2</td>
<td>topic B</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>user_id M</td>
<td>text M</td>
<td>topic S</td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4)最终得到每个topic下的user_id集合（用户簇）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>topic</th>
<th>user_id_set</th>
</tr>
</thead>
<tbody>
<tr>
<td>topic A</td>
<td>user_id1, user_id3, user_id6, …, user_idN</td>
</tr>
<tr>
<td>topic B</td>
<td>user_id2, user_id5, user_id8, …, user_idT</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>topic N</td>
<td>user_id9, user_id13, user_id15, …, user_idX</td>
</tr>
</tbody>
</table>
</div>
<h6 id="4-网络图节点排名"><a href="#4-网络图节点排名" class="headerlink" title="4.网络图节点排名"></a>4.网络图节点排名</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于每个topic下的用户簇，识别该簇下的重要信息分发节点用户。</p>
<h6 id="4-1-PageRank网络节点排名算法"><a href="#4-1-PageRank网络节点排名算法" class="headerlink" title="4.1.PageRank网络节点排名算法"></a>4.1.PageRank网络节点排名算法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PageRank作为基础算法，该网页排名算法来源于Google。其原理是依据网页被链接的数量，计算该网页的重要性排名，按权重PR值由高到低排名。（每个网页权重PR值迭代更新直到收敛。）<br>基本思路：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) 若某网页被很多其他网页链接到，则该网页重要，PR值提高；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) 若某网页被某PR值很高的网页链接到，则该网页的PR值提高；<br>定义：</p>
<ul>
<li><strong>出链</strong>：一个网络节点对外链接到其他网络节点；</li>
<li><strong>入链</strong>：一个网络节点被其他外部节点链接到；</li>
<li><strong>无出链</strong>：一个网络节点不链接其他网络节点；</li>
<li><strong>只对自己出链</strong>：一个网络节点只链接给自身；</li>
<li><strong>PR值</strong>：依据网络节点的度，计算得到的评判分数；</li>
</ul>
<p>以一个网络中有ABCD四个节点为例，所有网页（节点）链接都可以分为以下4种情况：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>case1</strong>：节点都有出链。此时A节点PR值计算为：</p>
<p><table>
    <tr>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/fp-pr1.jpg" width="120px"></td>
        <td bgcolor="white">$$ PR(A)=\frac{PR(B)}{2}+\frac{PR(C)}{1} $$</td>
    </tr>
</table><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>case2</strong>：存在没有出链的节点。此时A节点PR值计算为：</p>
<p><table>
    <tr>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/fp-pr2.jpg" width="120px"></td>
        <td bgcolor="white">$$ PR(A) = \frac{PR(B)}{2}+\frac{PR(C)}{4} $$</td>
    </tr>
</table><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;节点C无出链，则其对A,B,D节点无PR值贡献。为了便于计算，对于没有出链的节点，强制设定让其对所有的节点都有出链，使得其对所有节点都有PR值贡献。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>case3</strong>：存在只对自己出链的节点。则A节点PR值计算公式为：(其中$\alpha$一般取值0.85)</p>
<p><table>
    <tr>
        <td width="120px" bgcolor="white"><img align="left" src="/resources/fp-pr3.jpg" width="120px"></td>
        <td bgcolor="white">$$ PR(A)=\alpha(\frac{PR(B)}{2})+\frac{1-\alpha}{4} $$</td>
    </tr>
</table><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;节点C为只对自己出链的节点。进入网页节点C一般会继续跳转，因为只对C自己出链，故假设会重定向到其他任意网页节点。设定存在一定概率为$\alpha$会重定向到其他任意网页节点。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;综上，单个节点的PR值一般计算公式：（其中，<script type="math/tex">M_{p_{i}}</script>为有出链到<script type="math/tex">p_{i}</script>的所有节点集合，<script type="math/tex">L(P_{j})</script>为有节点<script type="math/tex">p_{j}</script>的出链总数，$N$为节点总数，$\alpha$常量(一般0.85)）</p>
<script type="math/tex; mode=display">PR(p_{i}) = \alpha \sum_{p_{j} \in M_{p_{i}}} \frac{PR(p_{j})}{L(p_{j})}+\frac{1-\alpha}{N}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所有节点PR值一直迭代计算，直到以下两种情况之一发生即停止：1.每个节点的PR值前后误差小于自定义误差阈值；2.迭代次数超过自定义迭代次数阈值；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;该算法缺点：对新加入节点不友好。一个新节点入链相对较少，即使其可能是潜在高链接节点，要达到高PR值需要长时间迭代。</p>
<h6 id="4-2-LeaderRank网络节点排名算法"><a href="#4-2-LeaderRank网络节点排名算法" class="headerlink" title="4.2.LeaderRank网络节点排名算法"></a>4.2.LeaderRank网络节点排名算法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在PageRank基础上，LeaderRank是改进的网络节点排名算法。增加了全局公共节点Ground Node，此节点连接网络中全部节点。所以一个N节点M边的网络G(N,M)变成N+1节点M+2N边的网络G(N+1, M+2N)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;增加Ground Node的作用：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)因Ground Node连接了网络中所有节点，所以网络一定为连通图，保证了LeaderRank的收敛性，减小了整个网络半径，增加收敛速度。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)某i节点的信息来源（即由i节点发出的指向其余节点的边）的多少，反比于节点i流向Ground Node节点的个数。随着网络结构的变化，不同的节点会有不同的“跳转概率”，由此 LeaderRank算法不再需要参数“跳转概率c”。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;LeaderRank算法公式：</p>
<script type="math/tex; mode=display">s_{i}(t+1)=\sum_{j=1}^{N+1} \frac{a_{ji}}{k_{j}^{out}} s_{i}(t)</script><script type="math/tex; mode=display">S_{i}=s_{i}(t_{c})+\frac{s_{g}(t_{c})}{N}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，<script type="math/tex">a_{ji}</script>表示当前节点$j$是否有连接到节点$i$，<script type="math/tex">k_{j}^{out}</script>为节点$j$指向其他节点的连接个数，<script type="math/tex">s_{i}(t)</script>为$i$节点在$t$时刻得分；<script type="math/tex">t_{c}</script>为<script type="math/tex">s_{i}(t)</script>收敛时刻；<script type="math/tex">s_{i}(t_{c})</script>为<script type="math/tex">t_{c}</script>时刻$i$节点的得分；<script type="math/tex">s_{g}(t_{c})</script>为<script type="math/tex">t_{c}</script>时刻Ground Node节点的得分；<script type="math/tex">S_{i}</script>为$i$节点最终得分；</p>
<h6 id="4-3-改进的网络节点排名算法"><a href="#4-3-改进的网络节点排名算法" class="headerlink" title="4.3.改进的网络节点排名算法"></a>4.3.改进的网络节点排名算法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于两种算法本身存在的局限性问题，结合本场景的具体应用，做了如下改进：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.加入节点冷启动处理。对于新加入的节点，节点边的权重计算：默认其对外连接和外部节点到该节点的连接初始化为0；节点本身分数计算：同基础特征计算方式，即前述的归一化分数计算值作为默认数值。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.结合用户多维度特征，优化网络边计算权重公式，即网络图节点出度计算公式：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;节点边权重因子<script type="math/tex">\omega_{ji}</script>归一化计算公式：</p>
<script type="math/tex; mode=display">\omega_{ji}=\frac{\sum count_{j}}{\sum count_{total\_network}}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;归一化分数，此表示节点$j$对外连接数，除以整个网络对外出度的边之和。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结合上述LeaderRank两个计算式，有网络图节点出度计算公式：</p>
<script type="math/tex; mode=display">s_{j}(t+1)=\sum_{j=1}^{N} \frac{\omega_{ji}\cdot \alpha_{ji}}{k_{j}^{out}} s_{j}(t)</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于每个簇，计算每个用户节点的评分值和边权重值，用户基础评分特征分数加权融合。迭代更新，直至整个网络收敛。则有：</p>
<script type="math/tex; mode=display">F_{j}(t)=\alpha\cdot s_{j}(t) + \beta\cdot \sum_{i=1}^{N-1} \omega_{j}(t) + \gamma\cdot Imp_{j}(t)</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，<script type="math/tex">F_{j}(t)</script>为网络节点最终影响力分数，<script type="math/tex">s_{j}(t)</script>为当前节点$j$在当前$t$时刻节点评分值，<script type="math/tex">\omega_{j}(t)</script>为当前节点$j$对网络其他所有节点的出度边权重，对其求和得到所有出度边影响权重分数，<script type="math/tex">Imp_{j}(t)</script>为节点归一化影响力分数，即归一化分数的<script type="math/tex">normalization\_impact\_score</script>。此外，$\alpha$、$\beta$、$\gamma$ 分别为每个因子的加权融合的权重因子。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过以上改进，该算法能更快更准确地发现网络高影响力节点。同时因为加入了多维度计算的冷启动，有效避免了新节点加入不能更快收敛的问题。</p>
<h6 id="5-用户分层结果输出"><a href="#5-用户分层结果输出" class="headerlink" title="5.用户分层结果输出"></a>5.用户分层结果输出</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;每个簇中的用户节点，按综合因子评分由高到低排序，截取topK作为关键信息分发用户节点。（排名越高，该特征属性的权重分数越高）<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;排序用户中，用户结果分层，按分位数分为：高潜用户、中潜用户、低潜用户，作为结果输出。</p>
<h5 id="三、小结"><a href="#三、小结" class="headerlink" title="三、小结"></a>三、小结</h5><p>新的算法有很多优势：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.能对新加入的或者比较冷门的节点基于用户特征进行预处理，能更快促使网络收敛。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.将用户分层分簇之后对每个网络计算，使结果更精细化。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.大部分是基于已有历史用户数据建立网络，能识别哪些历史用户是关键节点，对于新加入网络的节点相对不太容易发现高可能性信息分发用户节点。</p>
<h5 id="四、名词解释"><a href="#四、名词解释" class="headerlink" title="四、名词解释"></a>四、名词解释</h5><p>1.<strong>PageRank</strong>：网页排名算法，由网页之间相互的超链接计算网页评分值，体现网页的相关性和重要性，常用于搜索引擎优化。通过网页链接关系来定一个页面的等级。<br>2.<strong>LeaderRank</strong>：对PageRank网页排名算法的改进算法。通过在网络中增加一个节点，将其与网络中的所有节点相连，得到一个强连接的新网络。根据公式不断进行迭代，直到达到稳定状态，能保证网络快速收敛。</p>
]]></content>
    
    <summary type="html">
    
      用户增长获客的一种重要的手段就是通过现有用户的社交关系网络来获取新用户。在裂变传播网络中，构建深度复杂网络模型，识别关键传播路由节点用户（关键意见领袖）就成了一个值得研究的问题。只有识别了高潜用户，才能更针对性地让这部分用户个性化地向更广泛的人群推广信息。
    
    </summary>
    
      <category term="Fission Propagation" scheme="http://www.dianacody.com/categories/Fission-Propagation/"/>
    
    
      <category term="Fission Propagation Network" scheme="http://www.dianacody.com/tags/Fission-Propagation-Network/"/>
    
      <category term="Information Distribution Network" scheme="http://www.dianacody.com/tags/Information-Distribution-Network/"/>
    
      <category term="Deep Complex Network" scheme="http://www.dianacody.com/tags/Deep-Complex-Network/"/>
    
      <category term="High-potential User Identification" scheme="http://www.dianacody.com/tags/High-potential-User-Identification/"/>
    
      <category term="Key Opinion Leaders(KOL) Mining" scheme="http://www.dianacody.com/tags/Key-Opinion-Leaders-KOL-Mining/"/>
    
  </entry>
  
  <entry>
    <title>基于联邦学习的用户深度转化率预估</title>
    <link href="http://www.dianacody.com/2021/03/10/2021-03-10-FederatedLearning/"/>
    <id>http://www.dianacody.com/2021/03/10/2021-03-10-FederatedLearning/</id>
    <published>2021-03-09T16:00:00.000Z</published>
    <updated>2021-08-23T01:33:27.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;联邦学习是一种加密的分布式机器学习技术，对于一些对用户隐私数据保护要求比较高、样本少量、特征少量的场景，若需要训练大规模机器学习模型，此时需要借助第三方媒介来获取填充特征，但是用户隐私数据却又不能公开，所以联合建模技术就是为了解决这种问题而提出。</p>
<h5 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h5><h6 id="1-技术领域"><a href="#1-技术领域" class="headerlink" title="1.技术领域"></a>1.技术领域</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要应用方向涉及互联网/在线教育中的数据联合建模、用户深层漏斗转化率预估等领域。</p>
<h6 id="2-业界现状"><a href="#2-业界现状" class="headerlink" title="2.业界现状"></a>2.业界现状</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于互联网/在线教育中的数据联合建模和深度转化率预估，一些业界常用的现有技术如下：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.基于联邦学习的分布式训练技术，应用于区块链、医疗数据建模、个性化搜索推荐，采用传统的类似逻辑回归等机器学习方法，数据样本在生成过程和传输过程分布在各个不同参与方，保证数据安全共享。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.现有技术对梯度进行压缩，以减小网络传输开销。客户端训练初始模型得到梯度后进行量化处理，训练后得到更新的梯度发送给服务端，以此进行梯度数据交互，期间过程中对梯度压缩保证不失真，保证结果的安全性和数据私密性。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现有技术存在的问题：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现有技术基本采用传统机器学习模型的训练方法，对用户深度等转化数据建模的方法需要深度学习等方法结合具体场景进行建模。现有部分技术在训练过程中采用了深度学习模型，但并未进行分层架构处理，所有参数在每次训练全部更新，计算量较大，难以复用。且尚未在用户深度转化率预估方面，有类似基于深度学习的参数传播方法建模技术存在。</p>
<h6 id="3-技术问题"><a href="#3-技术问题" class="headerlink" title="3.技术问题"></a>3.技术问题</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;教育行业属于对用户隐私数据敏感的行业，一个完整的用户成交转化漏斗流程如下：用户通过和辅导老师沟通后，购买试用体验课，然后购买正价系统课。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一，数据层面需要深度转化指标。对于教育行业，相比用户的试用体验课购买转化率，实际核心指标更关注用户的正价系统课购买转化率，此为深度转化模型。但用户深度正价课的成交数据为敏感保密数据，为营收核心指标，不能提供给第三方媒体平台。另外，投放方能获取到用户的特征非常稀疏，但同一用户在其他第三方媒体平台上可能会积累大量行为特征，投放方需要利用媒体平台的这些特征进行模型训练以预测用户深度转化率，同时需要用户数据直接在投放方的本地环境下（不出数据库），进行联合建模训练。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二，联合建模若仅利用传统机器学习模型预估购买转化率CVR等指标，存在建模学习数据较浅的问题，需要深度模型训练。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决以上问题，我们给出了一种新的技术方案，可以有这些收益：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.数据隐私保护。利用该分布式机器学习/深度学习建模技术，投放方和媒体方的各自数据都可以在本地私有云上完成共建模型训练，防止敏感数据泄露。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.通过联邦学习方式训练深度转化率deepCVR，即deepCVR=正价系统课购买人数÷使用体验课购买人数，该影响因子用以调整投放定价公式，直接对线上流量做实时预估，对eCPM的计算做出调整，从而优化平衡客户引流和投放。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.从计算广告角度，作为广告主投放信息，对接各个第三方媒体平台，综合利用各个媒体平台用户公有特征，丰富了特征池，模型训练效果更优。</p>
<h5 id="二、技术方案"><a href="#二、技术方案" class="headerlink" title="二、技术方案"></a>二、技术方案</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;整体架构图如下：</p>
<p><div align="center"><img src="/resources/fl-architecture.jpg" alt="figure-fl-architecture"></div></p>
<center><h7 style="color:grey">图1 联邦学习训练架构</h7></center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;整个模型训练过程中，数据传输的交互流程图如下：</p>
<p><div align="center"><img src="/resources/fl-data-exchange.jpg" alt="figure-fl-data-exchange"></div></p>
<center><h7 style="color:grey">图2 数据交互流程</h7></center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据交互过程中，媒体平台侧的样本加密数据（账号id+特征feature）和信息投放方的样本加密数据（账号id+特征feature+标签label），在信息投放方的私有云环境上进行PSI安全数据求交运算；然后在信息投放方本地的私有云上进行样本拼接；拼接后回传给媒体平台方，媒体平台方和信息投放方在各自私有云上进行联邦学习模型训练；深度转化deepCVR模型预测在媒体平台侧进行，最后进入信息流投放系统进行投放应用。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要步骤如下：1.训练框架部署；2.数据上传到私有云；3.PSI安全数据求交；4.样本拼接；5.模型离线训练及评估；6.模型在线预测；7.在线投放智能定价；</p>
<h6 id="1-训练框架部署"><a href="#1-训练框架部署" class="headerlink" title="1.训练框架部署"></a>1.训练框架部署</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;训练框架用于数据网络通信、数据安全交互、模型训练等。信息投放侧在自己私有云/集群上安装并部署训练框架，媒体平台方合作协助支持。框架部署完成后，双方上传小量样本测试数据并开放对外授权，此样本数据用以调通训练流程。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;正式模型训练和模型推理的计算机器资源，包括机器CPU核数、内存大小、硬盘大小、网络通信带宽、通信开放的HTTP和GRPC端口，按实际训练样本的数据量预估，保证能在特定的小时数之内，完成后续的数据求交、模型训练、模型推理等过程。</p>
<h6 id="2-数据上传私有云"><a href="#2-数据上传私有云" class="headerlink" title="2.数据上传私有云"></a>2.数据上传私有云</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;信息投放方和媒体平台方各自的数据会单独上传到各自私有云数据库，所有参与训练的用户id或设备号id均进行hash后md5加密处理，训练时直接对加密id和特征进行训练。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;信息投放方的数据如下，需要标注是否下单标签，下单日期，而特征则非必要；同时，信息投放方仍然可以利用用户画像、课程、购买力等基础特征融合到数据中。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>encoded_user_id</th>
<th>label</th>
<th>order_date</th>
<th>feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>加密用户id</td>
<td>下单标签(必选)</td>
<td>下单日期(必选)</td>
<td>特征(可选)</td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;媒体平台方的数据如下，不需要是否下单标签。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>encoded_user_id</th>
<th>feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>加密用户id</td>
<td>媒体全域特征(必选)</td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;双方数据定期批量更新的方式同步到各自私有云中，参与后续模型训练。</p>
<h6 id="3-PSI安全数据求交"><a href="#3-PSI安全数据求交" class="headerlink" title="3.PSI安全数据求交"></a>3.PSI安全数据求交</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对信息投放方提供的用户id和媒体平台侧提供的用户id取交集，观测样本的重合度。一般地，对于共有部分的数据，可以直接进行后续的样本特征拼接，参与后续模型训练；对于各自非共有部分的数据，则不参与训练。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通常情况下，媒体平台侧拥有海量用户id，而信息投放方的用户id相对而言则较少，如果重叠用户占信息投放方的用户id的60%~90%以上，则认为是有效的，可以进行后续的模型训练。如果双方用户id重叠程度过低，信息投放方需要舍弃大部分用户id不能参与模型训练，则需要另外修改数据。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;信息投放方的数据分为以下两种：第一，含有是否下单标签，有少部分特征，比如用户的行为、画像、基本属性等。有标签的部分数据是用户已经产生了是否下单行为；第二，存在相当一部分用户，没有是否下单行为，即仅已经激活注册，但没有深度行为，但有少部分行为/画像/属性等特征。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;媒体平台侧的海量数据，主要提供丰富的特征，比如电商、社交媒体、广告、内容等，没有用户针对特定场景的行为标签，这些标签需要信息投放方根据具体应用场景进行标注正负样本。</p>
<p><div align="center" style="width:60%;height:60%;margin:0 auto;"><img src="/resources/fl-sample-overlap.jpg" alt="figure-fl-sample-overlap"></div></p>
<center><h7 style="color:grey">图3 数据样本重叠度</h7></center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据求交过程如下：媒体平台侧的加密用户id，同步到信息投放方的数据库中，和信息投放方的加密用户id求交集。</p>
<p><div align="center"><img src="/resources/fl-psi.jpg" alt="figure-fl-psi"></div></p>
<center><h7 style="color:grey">图4 PSI安全数据求交过程</h7></center>

<h6 id="4-样本拼接"><a href="#4-样本拼接" class="headerlink" title="4.样本拼接"></a>4.样本拼接</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对数据求交后的id拼接训练数据。由于数据加密，用户id、下单标签label、特征feature不存在泄露问题，即信息投放方不会将用户及其深度下单转化数据、手机号等敏感信息，提供给媒体平台侧。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于重叠部分用户id，每条样本格式为：加密用户encoded_user_id、下单标签label、下单日期order_date、媒体全域特征features+投放方特征feature。后续模型预测此部分特征维度。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于非重叠部分用户id，主要集中在媒体平台方，每条样本格式为：加密用户encoded_user_id、下单标签label、下单日期order_date、媒体全域特征features。后续模型预测此部分特征维度。</p>
<h6 id="5-模型离线训练及评估"><a href="#5-模型离线训练及评估" class="headerlink" title="5.模型离线训练及评估"></a>5.模型离线训练及评估</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;联邦学习模型训练支持机器学习模型和深度学习模型，机器学习模型例如线性回归、逻辑回归、梯度树模型等。深度学习模型主要为NN深度神经网络。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;训练过程中，采用分布式的方式，信息投放方和媒体平台方各自在单独的本地私有训练机器上进行单独的模型训练（机器已部署训练框架），期间机器间仅交换模型每次迭代更新所需要的梯度，其中传输的梯度已加密传输。整个交互过程中没有明文信息，不能反解。依据同态加密原理，加密后再训练和训练后加密的结果相同，故可以用加密后再训练的结果替代训练后加密的方式。</p>
<h6 id="5-1-基于机器学习的模型训练"><a href="#5-1-基于机器学习的模型训练" class="headerlink" title="5.1.基于机器学习的模型训练"></a>5.1.基于机器学习的模型训练</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;采用梯度树模型GBDT进行训练，预测用户是否下单，为二分类模型。</p>
<h6 id="5-2-基于深度学习的模型训练"><a href="#5-2-基于深度学习的模型训练" class="headerlink" title="5.2.基于深度学习的模型训练"></a>5.2.基于深度学习的模型训练</h6><h6 id="5-2-1-联邦深度学习架构"><a href="#5-2-1-联邦深度学习架构" class="headerlink" title="5.2.1.联邦深度学习架构"></a>5.2.1.联邦深度学习架构</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型为NN深度神经网络，仍然训练二分类。基于深度模型的联邦训练，每次更新深度网络的一个子网络，同时更新前向/后向传播过程。</p>
<p><div align="center" style="width:90%;height:90%;margin:0 auto;"><img src="/resources/fl-dnn.jpg" alt="figure-fl-dnn"></div></p>
<center><h7 style="color:grey">图5 联邦深度DNN训练</h7></center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;联邦学习NN模型整体分为三层：基础模型层（Basic Model Layer）、交互模型层（Interactive Model Layer）、顶部模型层（Top Model Layer）。有标签的提供方为主动方（Active Party），没有标签的提供方为被动方（Passive Party）。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基础模型层在主动方和被动方都存在，并且各自维护单独一套DNN深度神经网络训练的基础模型，凡是包含特征的参与方都会有基础模型层；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;交互模型层，若主动方没有特征，那么交互模型层仅含有被动方的交互模型，若主动方有特征，那么交互模型层包含主动方的交互模型和被动方的交互模型，主动方获取被动方传输过来的加密梯度、以及基础模型层的其他输出，融合本方模型更新需要的梯度参数之后，更新整体模型参数，以便后续训练，交互模型层的输出，经过激活函数传到顶部模型层；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;顶部模型层只在主动方存在，此为类似应用层，主要计算模型的损失，并把误差反向传播回去。训练后输出最终模型预测得分结果。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在整个联邦深度学习训练过程中，需要在主动方和被动方之间每次相互计算和传播梯度，多次循环迭代后直至模型收敛，期间的计算开销和通信开销较大。我们在整个算法过程在此基础上作了一些改进，主要包括：(1) 对需要交互传输的参数进行分层优化，大部分需要计算的参数（包括梯度）会集中到交互模型层计算；(2) 梯度计算优化和加速方法，每次迭代均需要计算梯度和加密梯度，然后再相互传输，通过优化梯度的计算过程、多次小批量并行计算，加快训练迭代收敛速度，同时减少通信传输开销。</p>
<h6 id="5-2-2-分层网络优化"><a href="#5-2-2-分层网络优化" class="headerlink" title="5.2.2.分层网络优化"></a>5.2.2.分层网络优化</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;整个训练过程纵向拆分为基础模型层、交互模型层、顶部模型层。基础模型层只关注数据嵌入，之后接入的深度模型可以为任意NN模型；交互模型层关注相互传输的参数，每次迭代过程中，更新对训练值$\alpha$的权重$W$和偏置$\epsilon$。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度神经网络的训练结构，基础模型层、交互模型层、顶部模型层的模型都各自独立训练而互不干扰，每层模型有更好的可拓展性。基础模型层的DNN模型可以替换为其他NN深度模型，而真正联邦深度学习的重点则在关注交互模型层的数据安全性。加密和解密过程只存在于交互模型层，不会在整个神经网络中，从而提高建模效率。</p>
<h6 id="5-2-3-前向-反向传播、梯度计算过程"><a href="#5-2-3-前向-反向传播、梯度计算过程" class="headerlink" title="5.2.3.前向/反向传播、梯度计算过程"></a>5.2.3.前向/反向传播、梯度计算过程</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般地，深度学习的前向/后向传播过程如下：<br>(1) 计算<script type="math/tex">y_{p}=W*\alpha+\epsilon</script>，其中<script type="math/tex">y_{p}</script>为预测目标函数值，$W$为权重矩阵，$\alpha$为训练值，$\epsilon$为偏置矩阵（此处为随机噪声扰动值）。<br>(2) 计算<script type="math/tex">\delta=|y_{r}-y_{p}|</script>，求差值的绝对值，其中<script type="math/tex">y_{r}</script>为真实值，<script type="math/tex">y_{p}</script>为预测值，残差$\delta$为损失值$loss$；<br>(3) 计算<script type="math/tex">\delta_{t}^{l}=(y_{r}-y_{p})*\sigma_{t}^{'}</script>, 用残差$\delta$和偏导<script type="math/tex">\sigma_{t}^{'}</script>相互作算子乘积；<br>(4) 计算<script type="math/tex">\delta_{(\omicron,t)}^{l}=(W^{T} \delta_{t}^{l})∘f(s_{t}^{l})∘\sigma_{t}^{'}</script>，此处激活函数<script type="math/tex">f(*)</script>可以为$softmax$、$tanh$、$ReLU$等函数，具体依模型不同而激活函数不同；<br>(5) 计算<script type="math/tex">\delta_{(o,(t+1))}^{l}=(W^{T} \delta_{(o,t)}^{l})∘f'(s_{t}^{l})∘\sigma_{t}^{'}</script>，以上过程得到的当前$t$时刻的<script type="math/tex">\delta_{(o,t)}^{l}</script>值保存，继续用于计算在下一个$(t+1)$时刻的<script type="math/tex">\delta_{(o,(t+1))}^{l}</script>，后续依次循环迭代。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度学习的梯度下降过程如下：<br>(1) 计算权重$W$和偏置$\epsilon$的偏导，计算目标函数$L_{N}$的对数似然梯度。</p>
<script type="math/tex; mode=display">\frac{\partial L_{N}}{\partial W} = \sum_{l=1}^N \delta_{t}^{l}(h_{t}^{l})^{T}</script><script type="math/tex; mode=display">\frac{\partial L_{N}}{\partial \epsilon} = \sum_{l=1}^N \delta_{t}^{l}</script><p>(2) 依次循环计算$(t-1)$时刻的权值和偏置的偏导。<br>(3) 更新所有权值和偏置等网络参数。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在此联邦分布式训练的情景下，我们针对该场景作了参数拆分，分发到不同分布式训练方上进行训练。并且，参数在训练过程中是明文，但在传输过程中需要加密，因此需要拆分一部分参数用于加密（如训练因子$\alpha$、偏置噪声$\epsilon$），而另一部分参数则不需要加密。整体训练步骤如下，分为前向传播过程和反向传播过程。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前向传播过程中，基础模型层主动方和被动方分别计算前向传播梯度；交互模型层中，被动方对梯度加密后传输给主动方，主动方计算梯度加权系数，生成随机噪声，加上偏置<script type="math/tex">Z_{p}</script>后回传给被动方，被动方解密数据，计算累计噪声矩阵<script type="math/tex">\epsilon_{acc}</script>并乘上调整因子<script type="math/tex">\alpha_{p}</script>，然后传给主动方，主动方用激活函数计算后传给顶部模型层；顶部模型层中计算前向传播梯度。</p>
<p><div align="center"><img src="/resources/fl-forward-propagation.jpg" alt="figure-fl-forward-propagation"></div></p>
<center><h7 style="color:grey">图6 前向传播过程</h7></center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;反向传播过程中，在顶部模型层，主动方利用标签计算损失$loss$，即预测值和真实值的绝对值之差，得到残差$\delta$；交互模型层中，主动方生成噪声数据<script type="math/tex">\epsilon_{A}</script>并乘以因子<script type="math/tex">\alpha_{p}</script>后传给被动方，被动方生成随机噪声数据，加和后除以学习率$\eta$，加密<script type="math/tex">\epsilon_{acc}</script>后传给主动方，主动方依据公式加权计算权重，然后更新被动方交互层<script type="math/tex">W_{p}</script>，被动方解密<script type="math/tex">\delta_{p}</script>，然后继续向下反向传播；基础模型层计算后向传播梯度。</p>
<p><div align="center"><img src="/resources/fl-back-propagation.jpg" alt="figure-fl-back-propagation"></div></p>
<center><h7 style="color:grey">图7 反向传播过程</h7></center>

<h6 id="5-3-模型评估"><a href="#5-3-模型评估" class="headerlink" title="5.3.模型评估"></a>5.3.模型评估</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;后续模型迭代优化，仍然以二分类的AUC等离线指标作为离线评估标准。原始标签数据一般在主动方提供，所以模型评估部分只在主动方进行。</p>
<h6 id="6-模型在线预测"><a href="#6-模型在线预测" class="headerlink" title="6.模型在线预测"></a>6.模型在线预测</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;预测投放价格，进行灰度或全量的流量实验。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;灰度小流量实验分为：空白组、对照组、实验组。每个组之内细分不同小组实验，切分对应流量；灰度实验效果稳定后，逐步切分全量实验。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在线灰度实验组划分如下：</p>
<table>
    <tr><th>实验组</th><th>小流量组</th><th>实验</th></tr>
    <tr><td rowspan="3">空白组A</td><td>a1</td><td>base_model_1</td></tr>
    <tr><td>a2</td><td>base_model_2</td></tr>
    <tr><td>a3</td><td>base_model_3</td></tr>
    <tr><td rowspan="3">对照组B</td><td>b1</td><td>model_1</td></tr>
    <tr><td>b2</td><td>model_2</td></tr>
    <tr><td>b3</td><td>model_3</td></tr>
    <tr><td rowspan="3">实验组C</td><td>c1</td><td>deep_model_1</td></tr>
    <tr><td>c2</td><td>deep_model_2</td></tr>
    <tr><td>c3</td><td>deep_model_3</td></tr>
</table>

<h6 id="7-在线投放智能定价"><a href="#7-在线投放智能定价" class="headerlink" title="7.在线投放智能定价"></a>7.在线投放智能定价</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于用户深度转化率，从引流体验课，到系统课的价格预估公式。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;投放定价公式如下：</p>
<script type="math/tex; mode=display">eCPM = oBid * pCTR * pCVR * f(private\_deep\_cvr)</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，$eCPM$为每千次展示获得广告收入，该指标以衡量广告投放计费效果；$oBid$为广告转化出价；$pCTR$为预估点击率；$pCVR$为预估转化率；<script type="math/tex">private\_deep\_cvr</script>为模型输出结果即预估深度转化率；$f(*)$为扰动函数。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在线实验评估标准为：正价系统课成交数(deep_convert)提高、正价系统课成交率(deep_cvr)提高、信息投放方的投入产出比(ROI)提高。通过动态因子调整正价系统课的价格范围，达到整体最优化。</p>
<h5 id="三、小结"><a href="#三、小结" class="headerlink" title="三、小结"></a>三、小结</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相比现有技术，新的学习算法提出的方案：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.采用分布式联邦训练和深度模型，得到用户深度转化率的评估，同时在训练过程中数据加密传输，有效防止用户敏感数据泄露，保证数据的安全性。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.将分布式模型训练分层拆分，单独对交互模型层的参数进行优化，其余层的模型具有灵活可扩展性，框架结构可复用以便于多次实验迭代。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.对梯度计算进行优化，小批量进行梯度更新，减小网络通信传输开销，加快分布式训练收敛速度。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.投放定价公式用模型训练的private_deep_cvr权重因子修正，动态调节价格、动态修正eCPM数值，多次实验表明能带来较好的优化效果。</p>
<h5 id="四、名词解释"><a href="#四、名词解释" class="headerlink" title="四、名词解释"></a>四、名词解释</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.<strong>联邦学习(Federated Learning)</strong>：全称”<strong>安全联合传输学习</strong>“(<strong>Secure Federated Transfer Learning</strong>)，实质是一种加密的分布式机器学习技术，参与各方可以在不披露底层数据和底层数据的加密(混淆)形态的前提下共建模型，主要应用于对用户隐私数据敏感的场景。模型训练过程中，数据加密传输，特征和样本均在加密后拼接，然后在公有云上训练和预测，得到预测结果数据，解密后返回给各方，训练过程完全透明。主要用于解决行业间联合建模的样本重叠少、特征重叠少、两者均重叠少等问题。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这种技术的参与各方可以在不披露底层数据和底层数据的加密(混淆)形态的前提下共建模型。主要应用在金融领域的风控模型、用户信用评级模型，金融科技公司作为数据服务商，和第三方公司如银行/证券/保险/券商等合作数据建模。除了金融，目前在电商/教育/电信等对用户隐私数据敏感的行业也有应用。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.<strong>PSI数据求交</strong>(<strong>Private Set Intersection</strong>，隐私保护集合交集)：多方数据求交集，找出共同样本，然后数据在各自的第三方云上完成加密，加密之后用于模型训练。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.<strong>同态加密</strong>：一种密码学技术，同态加密后的数据经过模型训练得到结果再解密，和用同一种方式经过非加密模型训练得到的结果相同。</p>
]]></content>
    
    <summary type="html">
    
      在一些对用户隐私数据敏感的行业，用户样本数据和用户特征均有限的情况下，如何在保证数据安全的前提下，建立机器学习模型，对用户转化率进行预估？通常用到的技术就是联邦学习、数据联合建模等。
    
    </summary>
    
      <category term="Federated Learning" scheme="http://www.dianacody.com/categories/Federated-Learning/"/>
    
    
      <category term="Federated Learning" scheme="http://www.dianacody.com/tags/Federated-Learning/"/>
    
      <category term="Joint Modeling" scheme="http://www.dianacody.com/tags/Joint-Modeling/"/>
    
      <category term="Gradient Propagation" scheme="http://www.dianacody.com/tags/Gradient-Propagation/"/>
    
  </entry>
  
  <entry>
    <title>基于文本挖掘的用户转化率预估</title>
    <link href="http://www.dianacody.com/2020/12/20/2020-12-20-TextMining/"/>
    <id>http://www.dianacody.com/2020/12/20/2020-12-20-TextMining/</id>
    <published>2020-12-19T16:00:00.000Z</published>
    <updated>2021-08-23T03:58:19.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于文本挖掘的技术：对文本提取关键词后对其特征进行向量空间模型训练，之后进行文本分类/聚类，形成文章摘要和标签；或者利用特征向量生成模型用于排序；利用文本提取特征用作用户购买决策预测的技术，目前基于文本分类、文本挖掘的技术集中在文本标签提取、新词发现方法优化等。</p>
<h5 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于用户微信沟通文本提取用户特征，通过模型预测用户转化或购买决策的概率，并且可以通过新构建文本提取相应的个性化沟通话术，促进成交转化，从而提高用户下单转化率。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在文本挖掘领域，现有的技术主要集中在文本分析和标签生成，词向量模型为词嵌入模型，少有深度学习模型。结合文本挖掘技术，通过机器学习和深度学习的方法对文本表示进行改造，再结合机器学习模型对用户转化概率预测，能有效提高目标潜在用户的识别准确率、然后针对性采用相应策略沟通话术等。</p>
<h5 id="二、技术方案"><a href="#二、技术方案" class="headerlink" title="二、技术方案"></a>二、技术方案</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;利用用户沟通文本挖掘的转化概率预测算法，基于用户微信沟通文本，区分不同学科粒度（英语、语文、思维），预测用户购买课程的转化率，计算是否转化系统课的转化率CVR。即基于用户文本特征的是否下单的二分类预测。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>输入数据</strong>：user_id(用户id), content(原始文本), label(标签)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>输出结果</strong>：user_id(用户id), cvr_score(预测概率分数), predict_label(预测标签)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，CVR预测分数范围(0,1)区间，并据此给出用户分层。</p>
<p>整体流程图如下：</p>
<p><div align="center" style="width:80%;height:80%;margin:0 auto;"><img src="/resources/text-architecture.jpg" alt="figure-text-architecture"></div></p>
<center><h7 style="color:grey">图1 整体流程图</h7></center>

<p>用户文本数据处理全流程架构图如下：</p>
<p><div align="center"><img src="/resources/text-flow.jpg" alt="figure-text-flow"></div></p>
<center><h7 style="color:grey">图2 文本数据处理全流程架构</h7></center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要拆分为两大部分：第一，词向量模型部分，该部分主要是通过对原始文本进行特征提取得到文本的词向量表示，进而得到句子向量表示；第二，机器学习模型部分，该部分主要是用得到的句子向量矩阵，以结构化数据的形式输入，用机器学习模型，得到用户转化概率的预测分数。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;模型训练集用全部学科的沟通文本进行训练，模型预测部分区分不同学科（英语、语文、思维）进行预测。</p>
<h6 id="2-1-文本向量模型"><a href="#2-1-文本向量模型" class="headerlink" title="2.1.文本向量模型"></a>2.1.文本向量模型</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于定长时间窗口内，每个用户的文本按时间序列聚合为句子，考虑词序上下文关系，处理流程如下：</p>
<blockquote>
<p>原始文本&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;文本句子集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;文本词集&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;词向量&amp;句子向量(特征)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;转化率分数<br>&nbsp;content&nbsp;&nbsp;&nbsp;—&gt;&nbsp;&nbsp;&nbsp;sentences&nbsp;&nbsp;&nbsp;—&gt; &nbsp;&nbsp;&nbsp;words&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—&gt;&nbsp;&nbsp;&nbsp;&nbsp;word_vector(feature)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;—&gt; &nbsp;&nbsp;&nbsp;cvr_score</p>
</blockquote>
<h6 id="2-1-1-文本特征提取（词向量化）"><a href="#2-1-1-文本特征提取（词向量化）" class="headerlink" title="2.1.1.文本特征提取（词向量化）"></a>2.1.1.文本特征提取（词向量化）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据用户微信沟通文本，提取用户文本特征。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和传统机器学习训练不同，直接用文本的分词结果、或者直接用词语表示，无法作为监督机器学习模型的输入格式，需要将词表示为定长维度的词向量，这样训练的特征维度可以采用固定值（此时词向量的每一维feature无实际意义）。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) 取定长时间窗口内的用户沟通文本，用户历史过去三个月的微信沟通文本；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) 每个用户user_id的沟通文本text按时间序列聚合为一行句子，考虑词序上下文关系，得到文本句子集合；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) 文本过滤：去除标点符号、特殊符号、数字、非法字符、表情包、链接；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4) 文本句子分词：得到二元组2-gram、三元组3-gram、四元组4-gram、…；得到如下格式所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>user_id</th>
<th>label</th>
<th>sentence</th>
<th>words</th>
</tr>
</thead>
<tbody>
<tr>
<td>user_id1</td>
<td>label1</td>
<td>sentence1</td>
<td>[ word1, …, wordN ]</td>
</tr>
<tr>
<td>user_id2</td>
<td>label2</td>
<td>sentence2</td>
<td>[ word1, …, wordM ]</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>user_idX</td>
<td>labelY</td>
<td>sentenceS</td>
<td>[ word1, …, wordT ]</td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;原始文本：（此处已经将用户在一定时间窗内的文本，按时间序列，聚合为一个长句子）</p>
<p><div align="center" style="width:90%;height:90%;margin:0 auto;"><img src="/resources/text-content.jpg" alt="figure-text-content"></div></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对每行句子正则过滤、分词，得到如下表示：（其中sentence为句子集合组成的数组）</p>
<p><div align="center"><img src="/resources/text-words.jpg" alt="figure-text-words"></div></p>
<h6 id="2-1-2-由词向量-wordvec-生成句子向量-seqvec"><a href="#2-1-2-由词向量-wordvec-生成句子向量-seqvec" class="headerlink" title="2.1.2.由词向量(wordvec)生成句子向量(seqvec)"></a>2.1.2.由词向量(wordvec)生成句子向量(seqvec)</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5) 去除词频低于设定阈值的词；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(6) 去除停用词stop-words；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(7) 由词组得到词向量，采用word2vec和fastText词向量模型进行词向量模型训练，其中词向量维数采用固定长度，滑动窗口gram大小采用固定长度，上下文模型采用Skip-Gram或CBOW；得到的词向量分别为w2v_wordvec、fst_wordvec。如下所示：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>word</th>
<th>word_vector</th>
</tr>
</thead>
<tbody>
<tr>
<td>word1</td>
<td>[ feature1, …, featureN ]</td>
</tr>
<tr>
<td>word2</td>
<td>[ feature1, …, featureM ]</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>wordN</td>
<td>[ feature1, …, featureT ]</td>
</tr>
</tbody>
</table>
</div>
<p><div align="center"><img src="/resources/text-dict.jpg" alt="figure-text-dict"></div></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(8) 通过词向量生成句子向量。得到词向量表示后，整个句子的向量可以通过该句子中所有词向量求平均值得到，传统不需要训练的方法是<strong>句子向量=avg(词向量)</strong>。由此可以如下表示用户的文本特征：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>user_id</th>
<th>label</th>
<th>sequence_vector</th>
</tr>
</thead>
<tbody>
<tr>
<td>user_id1</td>
<td>label1</td>
<td>[ seq_feature1, …, seq_featureN ]</td>
</tr>
<tr>
<td>user_id2</td>
<td>label2</td>
<td>[ seq_feature1, …, seq_featureM ]</td>
</tr>
<tr>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr>
<td>user_idX</td>
<td>labelY</td>
<td>[ seq_feature1, …, seq_featureT ]</td>
</tr>
</tbody>
</table>
</div>
<p>通过调整词向量模型的部分参数：词组gram的滑动窗口大小、词向量维数，确定最优词向量模型参数组合。</p>
<blockquote>
<p>【备注】<br>a) 还有其他通过词向量得到句子向量的方法，类似神经网络类的方法，详见其他文献参考。<br>b) 浅层语义模型（如word2vec）更关注词向量的产生，而预训练语言模型（高层语义模型）主要指能产生上下文相关的特征表示，能更好地捕捉上下文关系，使特征feature表示更精确，这直接关系到之后接入机器学习模型的训练数据形态，影响模型最终效果。本文由于目前关注词本身，故此处并未采用预训练模型。对于预训练语言模型，详见其他文献参考。</p>
</blockquote>
<p>训练集句子向量表示：</p>
<p><div align="center" style="width:70%;height:70%;margin:0 auto;"><img src="/resources/text-wordvec.jpg" alt="figure-text-wordvec"></div></p>
<h6 id="2-2-机器学习模型训练"><a href="#2-2-机器学习模型训练" class="headerlink" title="2.2.机器学习模型训练"></a>2.2.机器学习模型训练</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将用户沟通文本转化为可以用监督机器学习的句子向量特征表示后，用机器学习模型训练。此处接入随机森林(Random Forest)、逻辑回归(Logistic Regression)、梯度树(GBDT)模型。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如图1所示，单独对于每个词向量模型word2vec和fastText得到的句子向量w2v_seqvec、fst_seqvec依次输入不同的机器学习模型RF、LR、GBDT，通过和各个机器学习模型组合，分别预测用户下单/不下单的二分类概率输出。</p>
<h5 id="三、优化方向"><a href="#三、优化方向" class="headerlink" title="三、优化方向"></a>三、优化方向</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.词向量部分仅从词本身的角度采用浅层语义模型，即仍然是基于词频统计（包含文本滑动窗口），提取的特征未能完全表示上下文特征信息表示。改进方向：可以用预训练模型，直接获取上下文相关特征表示，能更好地表征句子信息。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.机器学习模型部分，此处是每次对接单个模型接入。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)考虑组合特征、上下文相关信息，输入模型特征未做特征交叉。迭代model版本可引入交叉特征。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)尝试模型融合。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.引入隐层语义特征表示，而不仅仅是基于词本身作为的特征。</p>
]]></content>
    
    <summary type="html">
    
      基于文本挖掘方法对用户购买转化率进行预测。
    
    </summary>
    
      <category term="NLP" scheme="http://www.dianacody.com/categories/NLP/"/>
    
    
      <category term="Text Mining" scheme="http://www.dianacody.com/tags/Text-Mining/"/>
    
      <category term="Natural Language Processing" scheme="http://www.dianacody.com/tags/Natural-Language-Processing/"/>
    
      <category term="CVR Prediction" scheme="http://www.dianacody.com/tags/CVR-Prediction/"/>
    
  </entry>
  
  <entry>
    <title>营销广告定价平台应用实践</title>
    <link href="http://www.dianacody.com/2019/12/15/2019-12-15-Advertiser/"/>
    <id>http://www.dianacody.com/2019/12/15/2019-12-15-Advertiser/</id>
    <published>2019-12-14T16:00:00.000Z</published>
    <updated>2021-08-22T13:20:43.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-引言"><a href="#I-引言" class="headerlink" title="I. 引言"></a>I. 引言</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一般的营销广告系统来说，主要目标是吸引用户、合理出价促成用户转化。广告是否合理精准投放，直接决定了用户、广告主、平台三方能否平衡获利。广告定价的业务指标是寻求投入产出比（ROI）、实收（GMV）、转化率（CVR）这三个优化指标的平衡，我们团队在营销广告这个领域，从工程和算法这两个方面持续进行优化，目前取得了一些效果。</p>
<center><h4><b>工程篇</b></h4></center>

<h5 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h5><h6 id="1-业务目标"><a href="#1-业务目标" class="headerlink" title="1.业务目标"></a>1.业务目标</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;寻求投入产出比（ROI）、实收（GMV）、转化率（CVR）这三个优化指标的平稳提升，其三者呈不完全线性相关。我们可以控制的是：广告门槛（Advertise Threshold）、广告出价成本（Advertise Amount）这两个变量。优化的实质就是满足这些目标约束条件下，找到最大化的广告门槛和最小化的出价成本。</p>
<p><div align="center" style="width:50%;height:50%;margin:0 auto;"><img src="/resources/ad-multiobjective-optimization.jpg" alt="figure-ad-multiobjective-optimization"></div></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于多目标优化问题，应将其转化为单目标优化，我们把多目标拆分为单目标，但需要注意每个目标内指标均为正相关。优化这些单个目标的同时，不能牺牲其他指标来实现，其他指标必须是基本持平、甚至提升。</p>
<h6 id="2-系统架构"><a href="#2-系统架构" class="headerlink" title="2.系统架构"></a>2.系统架构</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;整体系统架构如下，纵向来看，分为接口层、业务策略层、算法模型层、监控体系层、数据指标层，其中实时和定时调度、日志记录贯穿整个体系，以及底层的基础平台设施。</p>
<p><div align="center"><img src="/resources/ad-architecture.jpg" alt="figure-ad-architecture"></div></p>
<h5 id="二、实现"><a href="#二、实现" class="headerlink" title="二、实现"></a>二、实现</h5><h6 id="1-接口层"><a href="#1-接口层" class="headerlink" title="1.接口层"></a>1.接口层</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于直接触达用户的方式，目前的有广告定价平台暂时为三种：智能权益投放、push通知推送、短信推送。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一种是为了保证已经在app上留存的用户的转化率，重点优化转化率CVR，分为定时和实时投放两种方式。</p>
<ul>
<li>定时权益投放：需要事先建立投放策略模板，按照用户不同活跃时间段定时投放。</li>
<li>实时权益投放：根据用户当前客单价在线计算预估广告出价。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第二三种是为了引流，吸引用户到广告平台上来，重点优化点击率CTR，通过对接第三方厂商的消息推送SDK接口，实现app通知推送和短消息推送。</p>
<ul>
<li>智能文案：这里我们事先配置好投放文案模板，根据用户历史行为、热销商品等召回不同的商品和价格，填充到文案模板中，这样可以做到个性化文案精准匹配用户。</li>
<li>时段投放 &amp; 频率控制：主要是用户疲劳度控制，计算用户活跃度，一天中不同时段给用户进行消息推送，尽可能多地促成打开率和点击率提升。比如中午和晚上用户使用手机比较活跃的时段，同时也会结合用户历史点击率，调整发送同时的条数、内容；周末、节假日用户使用手机频率较平时上升，也会在这个时间段集中对用户进行推送。</li>
</ul>
<h6 id="2-业务策略层"><a href="#2-业务策略层" class="headerlink" title="2.业务策略层"></a>2.业务策略层</h6><h6 id="2-1-用户分层-amp-人群圈选"><a href="#2-1-用户分层-amp-人群圈选" class="headerlink" title="2.1.用户分层 &amp; 人群圈选"></a>2.1.用户分层 &amp; 人群圈选</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;电商营销的核心是获客吸引、用户点击、用户转化、用户留存，对不同用户我们按照价值维度进行用户分层，按标签圈选人群，将广告投放给相应广告定价的目标人群。借鉴电商运营中的RFM用户价值分层体系，用户按照不同维度进行分层打分，例如历史转化率、活跃度、购买力、权益敏感度，综合评判打分决定了广告定价。</p>
<p><div align="center"><img src="/resources/ad-user-score.jpg" alt="figure-ad-user-score"></div></p>
<ul>
<li>历史转化率：对于偏向高转化率等优质用户，由于用户黏性本身较高，所以广告出价应该在保证这部分用户同时尽量降低出价成本、提升GMV；相反地对于偏向低转化率用户，应该适当用策略促其转化。</li>
<li>活跃度：对于偏向高活跃等优质用户，广告出价应该在这部分人群身上降低出价成本、提升GMV；对于中间段用户，我们细分了其活跃时段，分早间、中午、晚间分别对用户进行不同广告推送，最大化总体转化率；相反地对于低活跃度用户，应该适当用策略促其转化。</li>
<li>购买力：高购买力用户，可以适当提升广告出价以提升GMV， 重点是对于中间部分用户，优化目标是合理预估广告出价，在最小化出价成本的同时，尽量逼近其用户历史真实成交价格，降低广告成本、促其转化。</li>
<li>权益敏感度：对于敏感度不是很高的用户，尽量提升广告出价门槛，降低广告成本，其余则相反。但是要保证整体用户的广告成本下降，广告实收提升。</li>
</ul>
<h6 id="2-2-定价策略"><a href="#2-2-定价策略" class="headerlink" title="2.2.定价策略"></a>2.2.定价策略</h6><h6 id="2-2-1-基本计算方法"><a href="#2-2-1-基本计算方法" class="headerlink" title="2.2.1.基本计算方法"></a>2.2.1.基本计算方法</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在定价策略方面，用户不同价值维度的分级评分，综合相加，决定了广告的基准出价。<br>定义用户总评分$(Threshold\;Coefficient)$：</p>
<script type="math/tex; mode=display">threshold\_coefficient\_score = \sum_{i=1}^n score_i</script><p>可得归一化分数$(Normalization\;Threshold\;Coefficient\;Score)$：</p>
<script type="math/tex; mode=display">normalization\_threshold\_coefficient\_score = \frac{score_i-\min(threshold\_coefficient\_score)}{\max(threshold\_coefficient\_score)-\min(threshold\_coefficient\_score)}</script><p>定义预估广告定价门槛$(Price\;Threshold)$如下：</p>
<script type="math/tex; mode=display">price\_threshold = avg\_per\_order\_fee \times (1-threshold\_coefficient)</script><p>其中，avg_per_order_fee为用户平均订单金额，threshold_coefficient为门槛系数。<br>定义预估定价成本 ($ Price\;Amount $) 如下，其中feeRate为基准费率：</p>
<script type="math/tex; mode=display">price\_amount = price\_threshold \times fee\_rate \times amount\_coefficient</script><p>该预估模型计算出的广告出价门槛、广告出价成本，存放在离线数据库中。</p>
<h6 id="2-2-2-频率控制"><a href="#2-2-2-频率控制" class="headerlink" title="2.2.2.频率控制"></a>2.2.2.频率控制</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对广告投放频率进行控制，这个主要区分在定时和实时策略上，在定时策略中，周中和周末、上午下午和晚上时段，按照用户疲劳度控制都会有调节。</p>
<h6 id="2-2-3-兜底策略"><a href="#2-2-3-兜底策略" class="headerlink" title="2.2.3.兜底策略"></a>2.2.3.兜底策略</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于一些新人用户，类似于推荐系统中的冷启动策略，会给出一些兜底的定价。同时，对于华北、华东、华中、华南等不同地区，我们进行了人群分析，发现不同地区的用户在购买力和折扣敏感度上的差异，会细分调整系数。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;例如华南和华北的对比差异如下，我们看到华南大区的购买力稍高，且整体的折扣敏感度较高。所以对比华北大区，在华南大区的广告的兜底基准定价就会下调价格门槛，同时增加出价成本，促使不同分层用户进行转化。</p>
<p><div align="center"><img src="/resources/ad-purchase.jpg" alt="figure-ad-purchase"></div></p>
<p><div align="center"><img src="/resources/ad-discount-sensitivity.jpg" alt="figure-ad-discount-sensitivity"></div></p>
<h6 id="2-2-4-费率限制"><a href="#2-2-4-费率限制" class="headerlink" title="2.2.4. 费率限制"></a>2.2.4. 费率限制</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了使整体ROI得到保证，我们为广告定价的费率做了范围限制，在基准费率的基础之上，实际投放给用户的广告费率会在上下浮动一定百分比，但百分比会被限制在一定阈值范围之内，防止有部分用户的薅羊毛情况，同时也需要保证新人用户的权益优惠力度。</p>
<h6 id="2-2-5-费率调节-amp-步长调节"><a href="#2-2-5-费率调节-amp-步长调节" class="headerlink" title="2.2.5. 费率调节 &amp; 步长调节"></a>2.2.5. 费率调节 &amp; 步长调节</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于不同分层用户，我们做到了动态费率，在保证实收GMV和转化CVR的基础之上，尽可能地降低费率，从而降低补贴率，提升整体的ROI。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时，对于每个广告的预估定价，由于步长过大导致精度不够高，在策略后期，我们将步长调整到更精细的粒度。</p>
<h6 id="2-3-转化漏斗"><a href="#2-3-转化漏斗" class="headerlink" title="2.3.转化漏斗"></a>2.3.转化漏斗</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于广告出价方面，我们进行了线上小流量迭代多组实验，在之前的实验结论基础上，我们对用户转化漏斗进行分析。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用户转化流程：</p>
<p><div align="center" style="width:50%;height:50%;margin:0 auto;"><img src="/resources/ad-user-convert.jpg" alt="figure-ad-user-convert"></div></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过分析转化漏斗发现，各个环节的人群占比直接关系到广告预估出价。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从下单转化到用券转化，在之前小流量实验中，有将近60%~70%的用户，其最终用券转化的客单价和广告预估出价差距在真实阈值 $\alpha$ 范围之内，可以认为这部分用户预估是精准的，但仍旧有相当一部分用户（约30%）尚未达到预估门槛而未发生转化、少部分约10%用户达到预估门槛但未发生转化。对于前一种用户，优化方向采取用折扣后客单价替代总的客单价，从而更准确地预估定价门槛；对于后一种用户，只能从其平时消费行为习惯分析着手，但这部分用户基本上也很难再发生转化。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从点击转化到下单转化，这个环节有大量用户流失，也就是说，在来访率一定的情况下，下单率偏低，这个跟app的产品设计、用户的购物路径习惯有关。一般地，这些跟app内一些推荐位、广告位的曝光有关，而且若采用一般的点击、加车、购买加权置顶方式，还是很容易出现马太效应，所以策略上我们做了改进，同时考虑了广告展位的穿插展示多样性，在下单率方面有一定提升。</p>
<h6 id="3-算法模型层"><a href="#3-算法模型层" class="headerlink" title="3.算法模型层"></a>3.算法模型层</h6><h6 id="3-1-离线模型"><a href="#3-1-离线模型" class="headerlink" title="3.1.离线模型"></a>3.1.离线模型</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在离线模型中，我们采用了分层打分模型、线性逼近模型、多分类模型等。</p>
<ul>
<li><strong>分层打分模型</strong>：如前所述，按照用户各个不同维度价值体系进行分层打分，最终得到综合评分，直接决定广告定价门槛系数、广告定价成本系数、费率系数。从而计算出广告定价门槛、广告定价成本。</li>
<li><strong>线性逼近模型</strong>：通过拉取用户近期90天的购买行为，按照时间序列预估用户在下一单的成交成本，对于需要不同转化的用户设定不同的广告定价门槛，达到整体GMV/客单价的提升。</li>
<li><p><strong>多分类模型</strong>：主要指未下单、下单未用券、下单且用券来给用户分类，离线预估其CVR，然后从召回候选集列表的topN个广告中，选择门槛最大、定价成本最低的广告进行投放，以保证最大化ROI。</p>
<h6 id="3-2-在线模型"><a href="#3-2-在线模型" class="headerlink" title="3.2.在线模型"></a>3.2.在线模型</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个主要是在线多特征动态预估，在线预估的定价反过来会定期更新离线模型。</p>
<h6 id="3-3-在线预估框架"><a href="#3-3-在线预估框架" class="headerlink" title="3.3.在线预估框架"></a>3.3.在线预估框架</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;智能广告定价算法引擎 Ad-Server，其框架图如下：<br><div align="center" style="width:80%;height:80%;margin:0 auto;"><img src="/resources/ad-server.jpg" alt="figure-ad-server"></div></p>
</li>
<li><p><strong>召回阶段</strong>：在多路并发召回中，会召回不同的候选广告列表，比如按照用户最大客单价进行的预估定价、按照用户历史最低折扣价进行的预估定价、按照用户真实折扣后客单进行的预估定价……除了某些单一场景，还会增加不同场景下的预估召回结果。</p>
</li>
<li><strong>结果汇总</strong>：把不同召回源结果汇总。</li>
<li><strong>过滤调整</strong>：费率浮动限制、动态费率调整、步长调整等。</li>
<li><strong>结果排序</strong>：当有多个广告定价时，优先级高的应该按照什么策略出价等。</li>
<li><strong>结果封装</strong>：结果触达用户投放展示。</li>
</ul>
<h6 id="4-监控体系层"><a href="#4-监控体系层" class="headerlink" title="4.监控体系层"></a>4.监控体系层</h6><h6 id="4-1-离线数据监控"><a href="#4-1-离线数据监控" class="headerlink" title="4.1.离线数据监控"></a>4.1.离线数据监控</h6><ul>
<li><strong>生产任务监控</strong>：任务失败、任务运行超时监控、任务占用资源量。</li>
<li><strong>数据量监控</strong>：离线数据特征是否稳定。</li>
<li><strong>导数任务监控</strong>：写入数据量、数据写入速度。</li>
</ul>
<h6 id="4-2-在线性能监控"><a href="#4-2-在线性能监控" class="headerlink" title="4.2.在线性能监控"></a>4.2.在线性能监控</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对接Monitor监控平台，这里主要观测的是系统性能指标：如TPS/QPS、响应时间RT、系统并发数、平均请求PV、TP99/TP999等。</p>
<h6 id="4-3-业务指标监控"><a href="#4-3-业务指标监控" class="headerlink" title="4.3.业务指标监控"></a>4.3.业务指标监控</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于广告定价，还特别地对业务指标进行分钟级监控，以能做到实时响应。主要包括：</p>
<ul>
<li>发送率、匹配率、成功率；</li>
<li>定价的上限率、下限率、范围比率；</li>
<li>召回数、召回时间；</li>
<li>来访率、使用率/转化率、实收/毛利率、客单价/笔单价、ROI/补贴率；</li>
</ul>
<h6 id="5-数据指标层"><a href="#5-数据指标层" class="headerlink" title="5.数据指标层"></a>5.数据指标层</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们进行了多版实验迭代，主要针对以下数据指标进行统计：</p>
<ul>
<li><strong>定价成本指标</strong>：主要包括营销补贴率、ROI、用券补贴率等。</li>
<li><strong>定价收益指标</strong>：主要包括实收GMV、客单价/笔单价、毛利率等。</li>
<li><strong>转化留存指标</strong>：券使用率、下单转化率、用券转化率等。</li>
</ul>
<hr>
<center><h4><b>算法篇</b></h4></center>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在工程篇的开头，我们提到，广告定价平台的业务指标是寻求投入产出比（ROI）、实收（GMV）、转化率（CVR）这三个优化指标的平衡。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;寻求投入产出比（ROI）、实收（GMV）、转化率（CVR）这三个优化指标的平稳提升，其三者呈不完全线性相关。其中可以控制广告门槛（Advertise Threshold）和广告出价成本（Advertise Amount）这两个变量。优化的实质就是满足这些目标约束条件下，找到最大化的广告门槛和最小化的出价成本。</p>
<h5 id="一、问题分析"><a href="#一、问题分析" class="headerlink" title="一、问题分析"></a>一、问题分析</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于多目标优化问题，应将其转化为单目标优化，我们把多目标拆分为单目标，但需要注意每个目标内指标均为正相关。优化这些单个目标的同时，不能牺牲其他指标来实现，其他指标必须是基本持平、甚至提升。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一般意义上讲，广告竞价排名的排序分公式计算如下，其为多目标的线性加和，而优化目标就是寻求合适的 $\alpha$、$\beta$、$\gamma$，计算单个广告位的各个展示广告的排序质量分，得到总的广告展示排序后，获得总体的ROI、GMV、CVR三者最优：</p>
<script type="math/tex; mode=display">rank\_score = \alpha\cdot ROI + \beta\cdot GMV + \gamma\cdot CVR</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中，每个因子的计算方式如下。</p>
<script type="math/tex; mode=display">ROI = \frac{1}{subsidy\_rate} = \frac{GMV}{subsidy\_fee} = \frac{UV_{visit}\;\cdot CVR \cdot Price}{UV_{convert}\;\cdot \overline{subsidy}}</script><script type="math/tex; mode=display">GMV = UV_{convert}\;\cdot CVR \cdot Price</script><script type="math/tex; mode=display">CVR = \frac{UV_{convert}}{UV_{visit}}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当中，ROI和补贴率(Subsidy Rate)为反比，即ROI为总实收(GMV)除以总补贴费(Subsidy Fee)。分子GMV等于来访UV乘以转化率(CVR)，再乘以客单价(Price)；分母总补贴费(Subsidy Fee)等于转化UV乘以人均补贴费(avg(Subsidy))。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;总实收(GMV)，等于转化UV乘以转化率CVR乘以客单价(Price)；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;转化率CVR等于转化UV除以来访UV；</p>
<h5 id="二、问题建模"><a href="#二、问题建模" class="headerlink" title="二、问题建模"></a>二、问题建模</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在计算广告领域，不是一个类似于搜索推荐的排序问题，而是一个多目标最优化、在给定空间中，找函数最优解的问题。 广告定价预估转为凸优化问题，在解空间里找到最优解，用历史数据取预估，解空间的降维。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第一阶段的目标是最小化广告出价补贴费用，即最大化ROI。其数学表示为：</p>
<script type="math/tex; mode=display">\min(subsidy\_fee) = \min(\frac{1}{ROI}) = \max(ROI)</script><script type="math/tex; mode=display">
\begin{equation}
        s.t. \begin{cases}
        A \le th \le B  \\
        C \le amt \le D \\
        0 < E \le amt < th \le F \\
        G \le fee\_rate \le H \\
        \end{cases}
\end{equation}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中费率定义如下：</p>
<script type="math/tex; mode=display">fee\_rate = \frac{amt}{th-amt}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中th为广告出价门槛（Advertise Threshold），amt为广告出价成本（Advertise Amount），FeeRate为初始化约定费率，定义如上所示。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解空间范围定义：</p>
<ul>
<li>A: 最小门槛值，常量</li>
<li>B: 最大门槛值，常量</li>
<li>C: 最小成本值，常量</li>
<li>D: 最大成本值，常量</li>
<li>E/F：出价成本小于出价门槛，有常量范围限制</li>
<li>G: 费率下限值，常量</li>
<li>H: 费率上限值，常量</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由上定义公式可以推导出，其余两个变量GMV和CVR的约束是保持其不下降，最大化ROI。通过拆分，两者之积等于人均出价成本除以ROI，再乘以券转化率$(CVR_{useCoupon}\;\;\;)$。</p>
<script type="math/tex; mode=display">CVR \times Price = \frac{UV_{convert}\;\;\cdot \overline{amt}}{UV_{visit}\;\cdot ROI} = \frac{\overline{amt}}{ROI}\;\cdot \frac{UV_{convert}}{UV_{visit}} = \frac{\overline{amt}}{ROI}\;\cdot CVR_{useCoupon}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以控制的变量仅为2个，门槛th和出价成本amt。对于此非线性规划问题，目标是在广告转化率有保证、费率符合要求的情况下求解出门槛最大的广告定价。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;解空间如下图阴影部分的矩形面积，即在函数 $y=x$ 的下方部分围成的梯形。</p>
<p><div align="center" style="width:60%;height:60%;margin:0 auto;"><img src="/resources/ad-axis.jpg" alt="figure-ad-axis"></div></p>
<ul>
<li>选择LR模型学习训练数据，目的是提高线上求解速度和性能</li>
<li>Label: 用户是否有广告转化</li>
<li>特征包括用户特征和广告特征及交叉特征三部分</li>
<li>广告特征主要包括广告门槛th、出价成本amt、费率</li>
<li>用户特征包括平均客单价、平均总价、折扣率、折扣订单率、用户历史转化率等</li>
<li>交叉特征包括门槛客单价比，门槛总价比，定价成本折扣比等</li>
</ul>
<h5 id="三、非线性规划"><a href="#三、非线性规划" class="headerlink" title="三、非线性规划"></a>三、非线性规划</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于解空间的枚举数量较小，所以采用全局搜索的方法，即能在时间复杂度为O(1)的情况下求出最优的门槛和定价成本组合。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;伪代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span>(th&gt;min_th &amp;&amp; amt&gt;min_amt):</div><div class="line">    <span class="keyword">if</span>(CvrCheck(th, amt) &amp;&amp; feeRateCheck(th, amt)):</div><div class="line">        find = true;</div><div class="line">        <span class="keyword">break</span>;</div><div class="line">    <span class="keyword">if</span>(!CvrCheck(th, amt)):</div><div class="line">        adjustStep(th, amt);</div><div class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(feeRateCheck(th, amt)):</div><div class="line">        adjustFeeRateLimit(th, amt);</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">break</span>;</div></pre></td></tr></table></figure></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过以上求解方法可以得到全局最优的广告定价，在目前的ROI和CVR优化上取得了一些效果，但是仍然还有提升空间可以进一步优化。</p>
]]></content>
    
    <summary type="html">
    
      对于一般的营销广告系统来说，主要目标是吸引用户、合理出价促成用户转化。广告是否合理精准投放，直接决定了用户、广告主、平台三方能否平衡获利。广告定价的业务指标是寻求投入产出比(ROI)、实收(GMV)、转化率(CVR)这三个优化指标的平衡。
    
    </summary>
    
      <category term="Computational Advertising" scheme="http://www.dianacody.com/categories/Computational-Advertising/"/>
    
    
      <category term="Computational advertising" scheme="http://www.dianacody.com/tags/Computational-advertising/"/>
    
      <category term="Multiobjective optimization" scheme="http://www.dianacody.com/tags/Multiobjective-optimization/"/>
    
      <category term="Operations research" scheme="http://www.dianacody.com/tags/Operations-research/"/>
    
      <category term="Convex optimization" scheme="http://www.dianacody.com/tags/Convex-optimization/"/>
    
  </entry>
  
  <entry>
    <title>Recommendation Datum Query System (RDQS)</title>
    <link href="http://www.dianacody.com/2018/01/05/2018-01-05-RDQS/"/>
    <id>http://www.dianacody.com/2018/01/05/2018-01-05-RDQS/</id>
    <published>2018-01-04T16:00:00.000Z</published>
    <updated>2019-02-27T00:59:59.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-Project-Source-Code"><a href="#I-Project-Source-Code" class="headerlink" title="I. Project Source Code"></a>I. Project Source Code</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project source code URL: (This project is not open source at present.)</p>
<h5 id="II-Introduction"><a href="#II-Introduction" class="headerlink" title="II. Introduction"></a>II. Introduction</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The offline profile data mainly include user profile, commodity profile, recallable source data information and user’s account information, etc. However, these data are stored in different database services, such as HBase, Redis, Hive tables, Memory Cache, and middleware data services encapsulated based on these underlying databases. Therefore, it is so inconvenient to query and integrate different data, even observe and debug data, in that case, the visual interface is required for offline profile data. So based on the Spring’s MVC structure, the java web system named “Recommendation Datum Query System (RDQS)” has been developed.</p>
<h5 id="III-Architecture-Design"><a href="#III-Architecture-Design" class="headerlink" title="III. Architecture Design"></a>III. Architecture Design</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The data logical hierarchical framework is designed as follows:<br><img src="/resources/rdqs-architecture.jpg" alt="figure-rdqs-architecture"><br>(1) <strong>Database Layer</strong>: It accesses to databases and data services, and preliminarily processes these data fetched from databases.<br>(2) <strong>Data Logical Layer</strong>: It is a data structure independent of the database, and it integrates data according to the business logic.<br>(3) <strong>Front-end Interface Layer</strong>: It focuses on the product user experience design (UED), and optimization of visual and operational experience effects.</p>
<h5 id="IV-Back-end"><a href="#IV-Back-end" class="headerlink" title="IV. Back-end"></a>IV. Back-end</h5><h6 id="4-1-Database-Layer"><a href="#4-1-Database-Layer" class="headerlink" title="4.1 Database Layer"></a>4.1 Database Layer</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First, data truncation. In order to prevent from returning results timeout because of get numerous data at one time, so generally, it is necessary to truncate the retrieved data to less than 200. In the meanwhile, the users need not view more data on the interface.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    Secondly, concurrent data fetching. When the user choose to query several profile data at one time, the amount of data requested is large, it can usually as much as millions. Such data need to be loaded within 0.1 seconds, so that there is no obvious delay in human visual observation. Thus the parallel optimization is needed for fetching data logic.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Third, data preloading. By preloading the data previously read and frequently accessed to the cache, rather than getting data from databases each time, it can accelerate the query.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fourth, data rolling loading. For declining the delay of data access, the back-end paging method is used to request to load data step by step. It actually only requests the data of the first page when the user browsing the first page, while the user moving to the second, it requests the back-end database again to retrieve the data of the second page.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fifth, data formatting. This is mainly to make it convenient for front-end display.</p>
<h6 id="4-2-Data-Logical-Layer"><a href="#4-2-Data-Logical-Layer" class="headerlink" title="4.2 Data Logical Layer"></a>4.2 Data Logical Layer</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First, encapsulation of underlying data. According to the business logic, the data can be divided into four parts as user profile, commodity profile, recallable source data and user account information, and the retrieval index is developed for these parts. The user need to choose which catalog they required for their query, then they will get certain data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Second, query of separate platform account such as user-id, uuid and sku-id. The mapping table of cross-platform accounts is added, and updated regularly. The data is read directly.</p>
<h6 id="4-3-Data-Display-Layer"><a href="#4-3-Data-Display-Layer" class="headerlink" title="4.3 Data Display Layer"></a>4.3 Data Display Layer</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The interface for front-end to back-end interaction is provided, and the front-end and the back-end can send HTTP requests to each other for data interaction.</p>
<h5 id="V-Front-end"><a href="#V-Front-end" class="headerlink" title="V. Front-end"></a>V. Front-end</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It is based on JavaScript (jQuery) and Bootstrap. Because the system is not a large-scale system, and the front-end module has not been separated as a front-end subsystem at present. In fact, some sophisticated front-end framework such as AngularJS and React cannot meet the requirements currently, so it is sufficient to build some components with JavaScript directly.</p>
<h6 id="5-1-Interference-Reconfiguration"><a href="#5-1-Interference-Reconfiguration" class="headerlink" title="5.1 Interference Reconfiguration"></a>5.1 Interference Reconfiguration</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The interface reconfiguration is to better adapt to back-end data.</p>
<h6 id="5-2-Front-end-Componentization-Front-end-Engineering"><a href="#5-2-Front-end-Componentization-Front-end-Engineering" class="headerlink" title="5.2 Front-end Componentization (Front-end Engineering)"></a>5.2 Front-end Componentization (Front-end Engineering)</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Front-end engineering is a complex subject, from the simplest point of view, it is front-end componentization. A page is made up of separated parts, and these parts are independent with each other, which can be abstracted for subsequent reuse. According to function modules, the development files structure and division of team work are arranged easily, and there are more advantages to do following maintenance. It is better than according to the file catalogue. The topics about front-end engineering are detailed in my other series of blogs.</p>
<h6 id="5-3-Search-Function"><a href="#5-3-Search-Function" class="headerlink" title="5.3 Search Function"></a>5.3 Search Function</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It supports user to select different accounts to query data interactively. The user can choose from the dropdown menu to query with user-id, uuid and sku-id. It also supports keyboard up and down selection, the same applies to mouse click selection, then the requests are sent to background to return data. When inputting query content, it can pop up the drop-down menu automatically, and there are several suggestions matching for input content. By pressing the keyboard up and down keys, one can select in the following input prompts, and the selected items will automatically fill the text into the search input box. And the item can also be selected by mouse click.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The interactive selection scheme is shown in the following figure:<br><img src="/resources/rdqs-search.jpg" alt="figure-rdqs-search"></p>
<h6 id="5-4-Multilevel-Catalog-Display"><a href="#5-4-Multilevel-Catalog-Display" class="headerlink" title="5.4 Multilevel Catalog Display"></a>5.4 Multilevel Catalog Display</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The hierarchical multilevel directory display is a three-level menu. When a catalog is selected, the data only for this catalog can be retrieved. It is similar to the general information retrieval system, so it is no necessary to elaborate on it.</p>
<h5 id="VI-Other"><a href="#VI-Other" class="headerlink" title="VI. Other"></a>VI. Other</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It has been expanded to a unified data service platform, docking all offline data.</p>
<h5 id="VII-Reference"><a href="#VII-Reference" class="headerlink" title="VII. Reference"></a>VII. Reference</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      The data visualization system based on java-web.
    
    </summary>
    
      <category term="Web" scheme="http://www.dianacody.com/categories/Web/"/>
    
    
      <category term="Web" scheme="http://www.dianacody.com/tags/Web/"/>
    
      <category term="Spring" scheme="http://www.dianacody.com/tags/Spring/"/>
    
      <category term="javascript" scheme="http://www.dianacody.com/tags/javascript/"/>
    
      <category term="jQuery" scheme="http://www.dianacody.com/tags/jQuery/"/>
    
      <category term="Bootstrap" scheme="http://www.dianacody.com/tags/Bootstrap/"/>
    
      <category term="Freemarker" scheme="http://www.dianacody.com/tags/Freemarker/"/>
    
  </entry>
  
  <entry>
    <title>Scene Recommendation</title>
    <link href="http://www.dianacody.com/2017/06/26/2017-06-26-SceneRecommendation/"/>
    <id>http://www.dianacody.com/2017/06/26/2017-06-26-SceneRecommendation/</id>
    <published>2017-06-25T16:00:00.000Z</published>
    <updated>2019-02-27T01:38:04.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-Project-Source-Code"><a href="#I-Project-Source-Code" class="headerlink" title="I. Project Source Code"></a>I. Project Source Code</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project source code URL: (This project is not open source at present.)</p>
<h5 id="II-Introduction"><a href="#II-Introduction" class="headerlink" title="II. Introduction"></a>II. Introduction</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generally speaking, the commodity classification is built according to categories, and it can be divided into three categories of different levels at most. However, such a classification is difficult to meet the purchase needs of users in specific situations. For example, commodities such as travel backpack, sports shoes, sports clothes, walking sticks, selfie stick do not belong to a category, but they may belong to a specific scene “outdoor hiking”, in that case, the selfie stick can be recommended to the user to satisfy the purpose of scene recommendation. Such scenarios summarized artificially usually exist generally and practically, and it has commercial value. Similarly, other scenario themes can be abstracted. In order to enrich the recall data system, the improved LDA topic model is used to implement scene recommendation.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The final goal is to cluster commodities across different categories to topic scene. Based on user’s click and order behaviors, the commodities are divided into many pairs or n-tuples as “scene”, and data are recommended for each other in the same scene. The data is processed by pig and python.<br>It is specified that:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.The scene is a single, fine-grained, explicit theme;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.The commodities of a scene are not all in a three-scale category, otherwise the scene is the same as the three-scale category;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.There is sufficient vector distance between scenes;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.The scene theme’s coverage is over 99% so that the buckets of data tuples are prevented from skewing seriously.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the early stage, the number of scene themes designed artificially is 2000. Later, the number is 10000 by automatic data mining.</p>
<h5 id="III-FP-Growth-Scheme-Based-on-Frequent-Itemset"><a href="#III-FP-Growth-Scheme-Based-on-Frequent-Itemset" class="headerlink" title="III. FP-Growth Scheme Based on Frequent Itemset"></a>III. FP-Growth Scheme Based on Frequent Itemset</h5><h6 id="3-1-Data-Acquisition"><a href="#3-1-Data-Acquisition" class="headerlink" title="3.1 Data Acquisition"></a>3.1 Data Acquisition</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The most recent data of click and order are from user session. The latest data of user is extracted such as search, click, add cart and order, and combine these data into tuples of two to four, just as (click, order), (order, order), (search-word + cart, order). Then, the higher ranking combinations extracted by each method are regarded as the combinations with higher confidence. By collecting data for 30 days, the top N combinations with high frequency can be used as candidate set. The candidate sets of 3-4 months are used, and then the data of each month are intersected to get some sets.</p>
<h6 id="3-2-Data-Evaluation"><a href="#3-2-Data-Evaluation" class="headerlink" title="3.2 Data Evaluation"></a>3.2 Data Evaluation</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Method 1: Randomly select 10% of the combinations from each method, and artificially evaluate the proportion of scenarios in the combinations.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Method 2: Choose one month order and some click sequences of proportional sampling as evaluation data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These combinations are used to predict the evaluation data and to observe how much data can be matched, i.e. the recall rate of each method.</p>
<h6 id="3-3-Recall-Data-Clustering-and-Marking"><a href="#3-3-Recall-Data-Clustering-and-Marking" class="headerlink" title="3.3 Recall Data Clustering and Marking"></a>3.3 Recall Data Clustering and Marking</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The similarity of these rule methods can be calculated for clustering. For example, the overlap degree of each element such as product-word, cid3, cid2, cid1 in the combination can be calculated, and these elements are then labeled artificially.</p>
<h6 id="3-4-Conclusion"><a href="#3-4-Conclusion" class="headerlink" title="3.4 Conclusion"></a>3.4 Conclusion</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;By analyzing the data, it can be found that if ordered by frequency, the product word combinations with frequencies more than 10 are concentrated in the daily use commodity categories of higher sales, such as “wet paper towel” and “shampoo”. After clustering, only about 400 combinations are found, and the distribution of them cannot cover most commodity categories. Thus another scheme can be considered to mine scene data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;At present, the automatic mining method based on frequency will result to the clusters concentrate to high-frequency purchased commodities. To ensure the coverage of topic scenarios, some scenes are defined artificially, and then the automatically mined data have been added to these scenarios to enrich these scenarios.</p>
<h5 id="IV-Topic-Model-Scheme-Based-on-LDA"><a href="#IV-Topic-Model-Scheme-Based-on-LDA" class="headerlink" title="IV. Topic Model Scheme Based on LDA"></a>IV. Topic Model Scheme Based on LDA</h5><h6 id="4-1-Topic-Classification-System"><a href="#4-1-Topic-Classification-System" class="headerlink" title="4.1 Topic Classification System"></a>4.1 Topic Classification System</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;From the previous data mining method based on frequent itemsets, it is found that the scenes of automatic mining are heavily clustered, and some distances of scenes are too close to each other. In fact, the user behavior is too little, too centralized, and there is lack of cold start data based on scene, resulting in serious skewing of data buckets.<br><img src="/resources/scene-histogram.jpg" alt="figure-scene-histogram"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;So it is necessary to design a scene classification system from the top level view to reshape the scene data mined automatically. The current three-level classification system is not  suitable for the current data completely. By establishing the classification system, on the one hand, the distribution of scenarios in the whole station commodities can be known, and the perception of scenarios can be built, on the other hand, it can ensure that the scenarios established would have a higher coverage for all the commodities in the future. The current classification system is mainly according to the second category, named “domain category-topic”. There are mainly 12 categories and about 200 scene topics.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;At present, two methods are combined:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Method 1: Establishing the framework of scene classification system artificially, and filling in it with product words manually.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Method 2: Mining data automatically, finding the commodities user purchased, it can assign the data to the framework buckets defined artificially.</p>
<h6 id="4-2-Automatic-Data-Mining"><a href="#4-2-Automatic-Data-Mining" class="headerlink" title="4.2 Automatic Data Mining"></a>4.2 Automatic Data Mining</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The top N combinations are picked as the candidate from these combinations extracted by each method. For example, the topic scene model “daily cleaning” can be extracted from the combination of bath lotion, shampoo, toothbrush and toothpaste.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;But there are the following problems:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First, there are many repetitions of the scenes of four product word, and it needs to be merged. But at present, they cannot form a new topic scene after merging.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Second, scenes based on sku-level have too few skus and need generalization as well.</p>
<h6 id="4-3-Structural-Flowchart"><a href="#4-3-Structural-Flowchart" class="headerlink" title="4.3 Structural Flowchart"></a>4.3 Structural Flowchart</h6><p><img src="/resources/scene-process.jpg" alt="figure-scene-process"><br><strong> Stage 1: 2000 scenes are required. </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Candidate combinations are put into existing first-level categories, and these data are artificially mapped to topics to define scenarios. And the commodities are gotten from the scene. At present, a scenario list is built manually, the next is to find the certain corresponding product words of the scene, and then mapped to sku.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Process: scene - &gt; product word - &gt; commodity sku.<br><strong> Stage 2: 10000 scenes are required. </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The cost of manpower is too high with the method at first stage. To consider another method, the solution is that it can cluster product word and commodity sku first.<br><strong> ① Calculate the distance between words </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A) Calculate by word-embedding method;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B) Calculate with mutual information and Chi-square statistics;<br><strong> ② K-means++ Clustering </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A) Guarantee that there are 5-7 product words in a cluster;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B) In the case of clustering, recall the SKU purchased by three days.<br><strong>【Scene Automatic Mining】</strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It is considerable to cluster the four-tuple product words and SKU that satisfy thresholds.<br><strong> ① K-means </strong><br>The distance between words is calculated by word-embedding and cosine similarity.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Advantages: Some results have been finished, and the effect of product word is good, which is worth trying, but the effect of sku is not good.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Disadvantages: It is not reasonable in theory, for there are similar scenes of product word.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Solutions: Review the results of product word artificially, and then mapped to sku.<br><strong> ② LDA </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Advantages: Regard the scene as a topic, and then set the number of topics to get product word or sku.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Disadvantages: If four-tuple is considered as a sequence, however, LDA is not suitable for short text.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Solutions: A longer sequence such as a 7-day click sequence can be considered as a document, which ensures that a document has many duplicate items without the interference of stop words.<br><strong> ③ Brown-cluster </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Advantages: It belongs to hierarchical clustering and can control the heap size of clustering after clustering.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Disadvantages: It is more effective for sequences and cannot guarantee for tuple data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Solutions: A longer sequence is used to ensure that there are 5-7 product words in most clusters by controlling some thresholds. In the case of clustering, the commodity skus purchased within one month are recalled, and the sliding time window is 3-7 days. Finally, the skus of top 300 are selected as the recall data for each cluster.</p>
<h6 id="4-4-Parameter-Adjustment-amp-Evaluation"><a href="#4-4-Parameter-Adjustment-amp-Evaluation" class="headerlink" title="4.4 Parameter Adjustment &amp; Evaluation"></a>4.4 Parameter Adjustment &amp; Evaluation</h6><p><strong>(1) The scene sets are regarded as the evaluated objects. </strong> According to the scale of the whole scene set, the coverage of classification system, the minimum distance between scenes, and the average number of topic scene, the following evaluation indicators are defined.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) The total number of scenes and the number of classified scenes.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) The coverage rate of scene category, overall topic and category.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) The average number of topic scenes and the average number of a classified topic scenes.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(d) In the whole scene set, the minimum distance between scenes and the minimum distance between classified scenes.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(e) The average number of product words and skus in scenes and the average number of product words and skus in categories.<br><strong>(2) A single scene is regarded as the evaluation object. </strong> According to the accuracy of scene, traffic of scene, sales volume and price range, the following evaluation indicators are defined.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) The accuracy of scene.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) The proportion of product traffic involved in the scene within a month.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(c) The proportion of sales involved in the scene within a month.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(d) Overall price range of products in the scene.</p>
<h6 id="4-5-Online-Interfaces"><a href="#4-5-Online-Interfaces" class="headerlink" title="4.5 Online Interfaces"></a>4.5 Online Interfaces</h6><p><strong>(1) Offline Data </strong>: was stored into 3 HBase tables</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>TableName</th>
<th>Field1</th>
<th>Field2</th>
<th>Field3</th>
<th>Field4</th>
<th>Field5</th>
</tr>
</thead>
<tbody>
<tr>
<td>SceneTable</td>
<td>scene_id</td>
<td>scene_name</td>
<td>scene_sku</td>
<td>scene_image</td>
<td>scene_info</td>
</tr>
<tr>
<td>ListTable</td>
<td>scene_id</td>
<td>list_id</td>
<td>list_sku</td>
<td>/</td>
<td>/</td>
</tr>
<tr>
<td>ArticleTable</td>
<td>scene_id</td>
<td>article_id</td>
<td>article_text</td>
<td>/</td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<p><strong>(2) Online Interfaces </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>(a) Input parameters of request </strong>: recommendation position id, user id, type=0. <strong>Returned results</strong>: 10 scene id, scene name, scene picture, scene description.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>(b) Input parameters of request </strong>: recommendation position id, user id, type=1. <strong>Returned results</strong>: scene id, scene name, scene picture.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>(c) Input parameters of request </strong>: recommendation position id, user id, scene id. <strong>Returned results</strong>: sku, list id, list sku, article id, article text.</p>
<h6 id="4-6-Ranking-Scheme-of-Recall-Results"><a href="#4-6-Ranking-Scheme-of-Recall-Results" class="headerlink" title="4.6 Ranking Scheme of Recall Results"></a>4.6 Ranking Scheme of Recall Results</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The ranking involves two parts: the ranking between scene modules and the ranking of sku in a scene. The ranking of sku in a scene can refer to traditional schemes of recommendation position ranking. In the meanwhile, the ranking can be considered from two perspectives, one is matching similarity of users and scene, the other is scene triggering.</p>
<h6 id="4-6-1-Matching-Similarity-of-Users-and-Scenes"><a href="#4-6-1-Matching-Similarity-of-Users-and-Scenes" class="headerlink" title="4.6.1 Matching Similarity of Users and Scenes"></a>4.6.1 Matching Similarity of Users and Scenes</h6><p><strong> (a) Unilateral feature of scene </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Such features are these, within one week, the proportion of scene browsing, the proportion of users who browsing scenes, the proportion of sku sales volume in the scene, the proportion of users who ordered, and the repurchase cycle of scene.<br><strong> (b) Bilateral features of User-Scene </strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Action</th>
<th>Windows</th>
<th>Granularity/Attribute</th>
<th>Statistic</th>
</tr>
</thead>
<tbody>
<tr>
<td>browse</td>
<td>Recently click: 1 time, 6 times, 10 times, 3 minutes, 10 minutes, 1 hour, 1day, 2 days, 3 days</td>
<td>scene</td>
<td>number, ratio</td>
</tr>
<tr>
<td>order</td>
<td>1 day, 2 days, 1 week, 2 weeks, 1 month, 2 months, 3 months</td>
<td>scene</td>
<td>number, ratio</td>
</tr>
<tr>
<td>add-cart</td>
<td>1 day, 2 days, 1 week</td>
<td>scene</td>
<td>number, ratio</td>
</tr>
</tbody>
</table>
</div>
<p><strong> (c) Multilateral features of user-scenario-product words </strong><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Considering the problem of cold start. When the users do not have scene data, according to the coincidence degree of user’s recent behavior and scene product word, these scenes with top 5 coincidence degree can be gotten, whose IDs and scores can also be gained. </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Action</th>
<th>Windows</th>
<th>Granularity/Attribute</th>
<th>Statistic</th>
</tr>
</thead>
<tbody>
<tr>
<td>browse</td>
<td>Recently click: 50 times</td>
<td>Confidence degree of scene</td>
<td>score</td>
</tr>
<tr>
<td>order</td>
<td>3 days, 2 weeks, 1 month, 2 months, 3 months</td>
<td>Confidence degree of scene</td>
<td>score</td>
</tr>
<tr>
<td>add-cart</td>
<td>1 day, 1 week</td>
<td>Confidence degree of scene</td>
<td>score</td>
</tr>
</tbody>
</table>
</div>
<h6 id="4-6-2-Scene-Triggering"><a href="#4-6-2-Scene-Triggering" class="headerlink" title="4.6.2 Scene Triggering"></a>4.6.2 Scene Triggering</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The weight of product words in the scene can be considered.</p>
<h5 id="V-Work-Results"><a href="#V-Work-Results" class="headerlink" title="V. Work Results"></a>V. Work Results</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A patent paper entitled “Scene recommendation based on topic models” has been published.</p>
<h5 id="VI-References"><a href="#VI-References" class="headerlink" title="VI. References"></a>VI. References</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      The LDA topic model is improved to aggregate commodities across different categories to scene topics.
    
    </summary>
    
      <category term="Algorithm" scheme="http://www.dianacody.com/categories/Algorithm/"/>
    
    
      <category term="LDA" scheme="http://www.dianacody.com/tags/LDA/"/>
    
      <category term="pig" scheme="http://www.dianacody.com/tags/pig/"/>
    
      <category term="Topic Modeling" scheme="http://www.dianacody.com/tags/Topic-Modeling/"/>
    
      <category term="Data Mining" scheme="http://www.dianacody.com/tags/Data-Mining/"/>
    
      <category term="N-Gram" scheme="http://www.dianacody.com/tags/N-Gram/"/>
    
      <category term="K-means" scheme="http://www.dianacody.com/tags/K-means/"/>
    
      <category term="word-embedding" scheme="http://www.dianacody.com/tags/word-embedding/"/>
    
      <category term="Word to Vector" scheme="http://www.dianacody.com/tags/Word-to-Vector/"/>
    
      <category term="Brown Cluster" scheme="http://www.dianacody.com/tags/Brown-Cluster/"/>
    
  </entry>
  
  <entry>
    <title>Commodity Profile</title>
    <link href="http://www.dianacody.com/2017/06/18/2017-06-18-ItemProfile/"/>
    <id>http://www.dianacody.com/2017/06/18/2017-06-18-ItemProfile/</id>
    <published>2017-06-17T16:00:00.000Z</published>
    <updated>2019-02-27T01:33:43.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-Project-Source-Code"><a href="#I-Project-Source-Code" class="headerlink" title="I. Project Source Code"></a>I. Project Source Code</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project source code URL: (This project is not open source at present.)</p>
<h5 id="II-Introduction"><a href="#II-Introduction" class="headerlink" title="II. Introduction"></a>II. Introduction</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The same as user profile, the commodities actually have their own set of separate features. By calculating multidimensional features of commodity, it can establish a hierarchy system, basic commodity profile. Such multidimensional features include gender, age, category levels, product word, brand word, hot sale, price, level of purchasing power, etc.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are also connections among commodities. Under each categories, the relevance of commodities, the topic model clustering and hierarchical abstraction can be done by different dimensions of brand word, product word and extended attribute. It is another hierarchy system, but also belongs to the field of commodity profile. The most typical application of it is that the relevant commodities can be aggregated as different clusters named topics or scenes, that is, personalized recommendation.</p>
<h5 id="III-Basic-Commodity-Profile"><a href="#III-Basic-Commodity-Profile" class="headerlink" title="III. Basic Commodity Profile"></a>III. Basic Commodity Profile</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Commodity gender</strong>: some commodities are generally sexual distinguishable, for example, some specific handbags are for men and some are for women, which can reflect gender preferences. Of course, there are still some commodities without gender distinction.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Commodity age</strong>: some commodities have age range, for example, the shoes include baby shoes, teenager shoes, adult shoes, elderly shoes, etc., which can reflect age preference. But some commodities are without age distinction.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Commodity category</strong>: the categories divided according to macro levels.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Product word and Brand word</strong>: They are basic attributes of commodity, including specification parameters.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Price and Purchasing power level</strong>: A mass consumer product or a high-level consumer product, and the level of consumption.</p>
<h5 id="IV-Extended-Commodity-Profile"><a href="#IV-Extended-Commodity-Profile" class="headerlink" title="IV. Extended Commodity Profile"></a>IV. Extended Commodity Profile</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Relevant commodity</strong>: the similarity and correlation of commodities can be obtained from user’s daily behaviors. These commodities often combined together must have correlation degree with each other. The higher the correlation degree is, the more likely that the attributes of the related commodities can be used as subsidiary features of the dominant commodity.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Aggregated commodity sets</strong>: Clustering by FP-growth or topic models, and candidates are recommended for each other in the same topic set. This is described in detail in “The application of scene recommendation”.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Hierarchical abstraction</strong>: Hierarchical subordination among commodities.</p>
<h5 id="V-Reference"><a href="#V-Reference" class="headerlink" title="V. Reference"></a>V. Reference</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      The multidimensional features of commodities.
    
    </summary>
    
      <category term="Profile" scheme="http://www.dianacody.com/categories/Profile/"/>
    
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
      <category term="Item Profile" scheme="http://www.dianacody.com/tags/Item-Profile/"/>
    
  </entry>
  
  <entry>
    <title>Tracing Analysis of User Purchasing</title>
    <link href="http://www.dianacody.com/2017/05/29/2017-05-29-PurchaseTrace/"/>
    <id>http://www.dianacody.com/2017/05/29/2017-05-29-PurchaseTrace/</id>
    <published>2017-05-28T16:00:00.000Z</published>
    <updated>2019-02-27T01:43:36.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-Project-Source-Code"><a href="#I-Project-Source-Code" class="headerlink" title="I. Project Source Code"></a>I. Project Source Code</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project source code URL: (This project is not open source at present.)</p>
<h5 id="II-Introduction"><a href="#II-Introduction" class="headerlink" title="II. Introduction"></a>II. Introduction</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The patterns of user behavior are researched by statistics of tracing analysis of user purchasing. The session based logs are analyzed by hour, day, week, month and quarter to discover patterns of user’s purchasing preference. The ratio of user is summarized to get distribution of population.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Typical patterns of user behavior are labeled, for building user profile.</p>
<h6 id="2-1-Preference-Attributes-of-Commodity-Three-Level-Category"><a href="#2-1-Preference-Attributes-of-Commodity-Three-Level-Category" class="headerlink" title="2.1 Preference Attributes of Commodity Three-Level Category"></a>2.1 Preference Attributes of Commodity Three-Level Category</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The confidence degree reflects the user’s potential preference. A user browsed a lot of commodities, for example, he filtered for more than 100 times, so there is a confidence degree about 99% which indicate the specific preference. However, only according to the 100 times filters, it cannot ensure 99.9%, maybe only 99.5% confidence degree can indicate the preference. Much more attentions should be paid to search word, preference and filter word, ranking of commodity lists, and clicks or browses in the searching result lists, because there are common attributes within the product detail page lists. Or a counterexample, the user stayed on the list for a very short time, only filtering for effective information, regardless noise. <em>(Note: The dwell-time is a statistical instantaneous time. The time in a session mainly used to judge the mistaken click and filtering, not to judge the long-term of user behaviors, that is, the compound dwell-time within many sessions is meaningless.)</em><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the aspect of user profile, the product keyword and the key attribute word are extracted, including automatic mining of common attributes on user’s repurchase cycle, product word, browsing, click and product attributes.</p>
<h6 id="2-2-Paths-of-User’s-Added-Cart-and-Order"><a href="#2-2-Paths-of-User’s-Added-Cart-and-Order" class="headerlink" title="2.2 Paths of User’s Added-Cart and Order"></a>2.2 Paths of User’s Added-Cart and Order</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;According to the single user, observe which entrance the user come in. Finally, the macro-survey is made as statistics of all user results, observation of the total population proportion.</p>
<h6 id="2-3-Paths-of-User’s-Click-and-Follow"><a href="#2-3-Paths-of-User’s-Click-and-Follow" class="headerlink" title="2.3 Paths of User’s Click and Follow"></a>2.3 Paths of User’s Click and Follow</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This part of the user profile is user behavior, not related to order.</p>
<h5 id="III-Schemes"><a href="#III-Schemes" class="headerlink" title="III. Schemes"></a>III. Schemes</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; The session is used as a unit to record the user’s access behavior completely, and the full path of user’s access is outlined completely, and according to these, the data is analyzed.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Steps:<br>(1) By reading the output data edited by Session Pruner, all the sessions of user and their purchasing paths could be found by the statistics of logs for a long period of time, such as more than half a year, and whether there are certain patterns and rules can be summarized.<br>(2) Two ways<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(a) Define and mark the typical patterns existing in some users, and mark and portray them accordingly.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(b) The similar as above, with the deep statistical analysis of each user’s session data by the time axis, some repeated patterns can be found, and should be defined and marked.<br>(3) According to these inherent patterns, the recommendation results and algorithms are optimized.</p>
<h5 id="IV-Data-Analysis"><a href="#IV-Data-Analysis" class="headerlink" title="IV. Data Analysis"></a>IV. Data Analysis</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Time: 1 day, 3 days, 7 days. The number of skus within dwell time, average time and total time was counted.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Features: It is multiplied by time window as 1day, 3days, 7days.</p>
<h6 id="4-1-Experimental-Data"><a href="#4-1-Experimental-Data" class="headerlink" title="4.1 Experimental Data"></a>4.1 Experimental Data</h6><div class="table-container">
<table>
<thead>
<tr>
<th>Data Source</th>
<th>Time</th>
<th>Data Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>browse and add-cart data</td>
<td>1 month</td>
<td>1.1 GB</td>
</tr>
<tr>
<td>order data</td>
<td>3 month</td>
<td>3.0 GB</td>
</tr>
</tbody>
</table>
</div>
<h6 id="4-2-Methods-and-Results"><a href="#4-2-Methods-and-Results" class="headerlink" title="4.2 Methods and Results"></a>4.2 Methods and Results</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) The skus searched and purchased are aggregated into four-tuples, and the frequency of the four-tuples is counted. The tuples are clustered based on Jaccard similarity.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) These four tuples in method 1 are used as corpus to train word to vector model, and then are clustered by K-means method.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) The product words purchased by the same user for six months are collected as corpus to train word to vector model, and then are clustered by K-means method.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4) Due to the strong dependence of K-means method on the selection of initial nodes, so the algorithms are executed for many times to merge different parts of data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5) Based on one set of data result clustered by k-means algorithm, the cosine similarity between all product word vectors and each cluster is calculated for the secondary partition.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(6) The large-scale category are tried to be clustered secondarily with K-means algorithm. However, the effect is not satisfactory, and it still cannot be subdivided.</p>
<h6 id="4-3-Results-Analysis"><a href="#4-3-Results-Analysis" class="headerlink" title="4.3 Results Analysis"></a>4.3 Results Analysis</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) There are some clusters containing too many, mixed and disordered product words, and cannot be aggregated into a specific pattern. Furthermore, the data is sparse.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) There are some product words in a cluster with high similarity to each other.</p>
<h5 id="V-Improvement-Scheme"><a href="#V-Improvement-Scheme" class="headerlink" title="V. Improvement Scheme"></a>V. Improvement Scheme</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) Reduce the time span of order data.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) The data sampling should follow the rule of user patterns.</p>
<h5 id="VI-References"><a href="#VI-References" class="headerlink" title="VI. References"></a>VI. References</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      The patterns of user behavior are researched according to user logs.
    
    </summary>
    
      <category term="Profile" scheme="http://www.dianacody.com/categories/Profile/"/>
    
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
      <category term="User Profile" scheme="http://www.dianacody.com/tags/User-Profile/"/>
    
  </entry>
  
  <entry>
    <title>User Profile</title>
    <link href="http://www.dianacody.com/2017/03/30/2017-03-30-UserProfile/"/>
    <id>http://www.dianacody.com/2017/03/30/2017-03-30-UserProfile/</id>
    <published>2017-03-29T16:00:00.000Z</published>
    <updated>2019-02-27T01:49:30.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-Project-Source-Code"><a href="#I-Project-Source-Code" class="headerlink" title="I. Project Source Code"></a>I. Project Source Code</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project source code URL: (This project is not open source at present.)</p>
<h5 id="II-Introduction"><a href="#II-Introduction" class="headerlink" title="II. Introduction"></a>II. Introduction</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The patterns of user are different from each other, but almost of them have general regular patterns, which reflect personalized features of users, that is, user profile. By calculating user’s features in different dimensions, it can do personalized recommendation for users with these data effectively.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The user profile includes long-term user profile and real-time user profile.<br><img src="/resources/user-profile.jpg" alt="figure-user-profile"></p>
<h5 id="III-Long-term-User-Profile"><a href="#III-Long-term-User-Profile" class="headerlink" title="III. Long-term User Profile"></a>III. Long-term User Profile</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The long-term user profile, whose data derive from user logs during at least several months, that is multidimensional user features, and it can be stored into offline databases. It includes gender, age, area, interest of commodity categories, preference of product word, preference of brand word, extended attribute, purchasing power, preference of shop, preference of discount, gender and age of their children, and so on. The user logs are processed, and some crucial information can be extracted from original SDK event tracking logs, in that way, the session logs are gotten completely. All the steps above are to upgrade data granularity of user profile.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong> User’s deep behavior </strong>: dwell time, browse comments.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong> User’s initiative feedback </strong>: search, write comments, complaints.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong> User’s cross-platform behavior </strong>: the accounts of different devices correspond to the same user. By uniforming behaviors of them, we can establish a vivid user profile.</p>
<h5 id="IV-Real-time-User-Profile"><a href="#IV-Real-time-User-Profile" class="headerlink" title="IV. Real-time User Profile"></a>IV. Real-time User Profile</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The real-time user profile includes real-time feedback, current behaviors, whose granularity is in seconds level, e.g.30 seconds, 1 hour, 6 hours, 1day, 3 days, 1 week.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The flow of user’s real-time behavior can be calculated as real-time features, and Storm is used for the calculation.</p>
<h5 id="V-Reference"><a href="#V-Reference" class="headerlink" title="V. Reference"></a>V. Reference</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      The multidimensional features of users.
    
    </summary>
    
      <category term="Profile" scheme="http://www.dianacody.com/categories/Profile/"/>
    
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
      <category term="User Profile" scheme="http://www.dianacody.com/tags/User-Profile/"/>
    
  </entry>
  
  <entry>
    <title>Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank</title>
    <link href="http://www.dianacody.com/2017/02/23/2017-02-23-DeepVisual/"/>
    <id>http://www.dianacody.com/2017/02/23/2017-02-23-DeepVisual/</id>
    <published>2017-02-22T16:00:00.000Z</published>
    <updated>2018-11-30T09:25:38.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>One consideration to make is that large modern CNNs require large amounts of training data. The amount of examples available to an individual query model can be in the low hundreds, particularly for queries in the long tail. This makes training one deep CNN per query from scratch prone to overfitting. Transfer learning is a popular method for dealing with this problem, with many expels in computer vision.<br>Learning to rank search results has received considerable attention over the past decade, and it is at the core of modern information retrieval. </p>
<h5 id="Works"><a href="#Works" class="headerlink" title="Works"></a>Works</h5><h6 id="1-Multimodal-Listing-Embedding"><a href="#1-Multimodal-Listing-Embedding" class="headerlink" title="(1) Multimodal Listing Embedding"></a>(1) <strong>Multimodal Listing Embedding</strong></h6><p>Our listing contains text information such as descriptive title and tags, as well as an image of the item for sale. To measure the value of including image information, we embed listings in a multimodal space (consisting of high-level text …)</p>
<h6 id="2-Training-Models"><a href="#2-Training-Models" class="headerlink" title="(2) Training Models"></a>(2) <strong>Training Models</strong></h6><p>we extend our existing learning to rank models with image information. At first, we embed listings for the learning to rank task in both single modality and multimodal settings. Then, the listing embedded in both modalities are incorporated into a learning to rank framework.</p>
]]></content>
    
    <summary type="html">
    
      Search is at the heart of modern e-commerce. Traditional models optimize over a few hand-constructed features based on the item’s text. Now, there is a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural net work.
    
    </summary>
    
      <category term="Deep Visual" scheme="http://www.dianacody.com/categories/Deep-Visual/"/>
    
    
      <category term="Learning to rank" scheme="http://www.dianacody.com/tags/Learning-to-rank/"/>
    
      <category term="Computer vision" scheme="http://www.dianacody.com/tags/Computer-vision/"/>
    
      <category term="Deep learning" scheme="http://www.dianacody.com/tags/Deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Targeted Content for a Real-Time Activity Feed, For First Time Visitors to Power Users</title>
    <link href="http://www.dianacody.com/2017/01/22/2017-01-22-RealTimeFeed/"/>
    <id>http://www.dianacody.com/2017/01/22/2017-01-22-RealTimeFeed/</id>
    <published>2017-01-21T16:00:00.000Z</published>
    <updated>2018-11-30T09:26:02.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>The Activity Feed is a frequently updated stream of content in the form of stories that flow from the top to the bottom of the page as time goes on. we addressed these issues in a new version of the Activity Feed that was built and deployed to all of our tens of millions of uses. These users cover the full gamut of our website engagement levels, ranging from the first-time visitor to the long-time power user. </p>
<h5 id="Activity-Feeds"><a href="#Activity-Feeds" class="headerlink" title="Activity Feeds"></a>Activity Feeds</h5><p>Activity feeds are loosely time-ordered sets of varied content, and are commonly found on popular social networks such as Twitter, Facebook, and LinkedIn. The content in feeds is drawn from a set of content products. Typically, content producers are nodes in a user’s social graph, which represents the user’s interests based on explicit user action, for example, “following” another user. We call content generated from a user’s social graph organic content. We also note that the time-ordered nature of the AF makes it ideal for keeping up with the actions of your social graph, the users with whom one shares commonalities. Because AFs are singular source for “what is new” amongst a peer group, they often serve as a primary landing page for users.</p>
]]></content>
    
    <summary type="html">
    
      The Activity Feed (AF) is our taking on the ubiquitous “web feed” - a continuous stream of aggregated content, personalized for each user. These streams have become the defacto means of serving advertisements in the context of social media.
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Data Mining" scheme="http://www.dianacody.com/tags/Data-Mining/"/>
    
      <category term="Large-scale Systems" scheme="http://www.dianacody.com/tags/Large-scale-Systems/"/>
    
  </entry>
  
  <entry>
    <title>Style in the Long Tail, Discovering Unique Interests with Latent Variable Models in Large Scale Social E-commerce</title>
    <link href="http://www.dianacody.com/2017/01/21/2017-01-21-LongTail/"/>
    <id>http://www.dianacody.com/2017/01/21/2017-01-21-LongTail/</id>
    <published>2017-01-20T16:00:00.000Z</published>
    <updated>2018-11-30T09:26:58.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h5><p>An online marketplace for handmade and vintage items, with over 30 million active users and 30 million active listings. This is a marketplace known for tis diverse and eclectic content (e.g. Figure 1); people come in order to find those unusual items that match the peculiarities of their style. Indeed, in its entirety could be considered part of the e-commerce long tail; in addition to wide ranging functions and styles, the handmade and vintage nature of the site means that most items for sale are unique.</p>
<h5 id="Works"><a href="#Works" class="headerlink" title="Works"></a>Works</h5><h6 id="1-Recommendation-Systems"><a href="#1-Recommendation-Systems" class="headerlink" title="(1)Recommendation Systems"></a>(1)<strong>Recommendation Systems</strong></h6><p>recommender systems are nothing new, with the first papers on collaborative filtering appearing in the 1990s.The range of techniques available when building recommender systems is vast, too board to cover here. For a good overview of common techniques, we urge the curious reader to read the survey of Adomavicius and Tuzhilin. Also of note is the work of Koren, Kolinsky and others describing the approaches that won the Netflix prize.</p>
<h6 id="2-Latent-Dirichlet-Allocation-LDA"><a href="#2-Latent-Dirichlet-Allocation-LDA" class="headerlink" title="(2)Latent Dirichlet Allocation (LDA)"></a>(2)<strong>Latent Dirichlet Allocation (LDA)</strong></h6><p>Latent Dirichlet Allocation (LDA) is an unsupervised, probabilistic, generative model that aims to find a low dimensional description that can summarize the contents of large document collections. </p>
<h5 id="Identifying-User-Interests"><a href="#Identifying-User-Interests" class="headerlink" title="Identifying User Interests"></a>Identifying User Interests</h5><h6 id="1-Social-E-commerce"><a href="#1-Social-E-commerce" class="headerlink" title="(1)Social E-commerce"></a>(1)<strong>Social E-commerce</strong></h6><p>There are four important entities:<br>“User”: Anyone registered on our website, including sellers<br>“Seller”: our user who own a shop<br>“Shop”: A collection of items sold by the same seller. Each shop has its own online storefront.<br>“Listing”: Products/items listed in a shop, each with its unique listing id.<br>To give an idea of scale, we currently have approximately 1 million active sellers/shops, 30 million active listings, and 30 million active members.</p>
<h6 id="2-Inferring-User-Interests"><a href="#2-Inferring-User-Interests" class="headerlink" title="(2)Inferring User Interests"></a>(2)<strong>Inferring User Interests</strong></h6><p>Our use of LDA is based on the premise that users with similar interests will act upon similar listings. We chose to user the social action of “favoriting” listings as a reliable signal for user style. This is done in lieu of more traditional user intent signals, for instance “purchasing” as is commonly done in collaborative filter development. </p>
<h5 id="References"><a href="#References" class="headerlink" title="References"></a>References</h5><p>[1] …<br>[2] …</p>
]]></content>
    
    <summary type="html">
    
      An online marketplace for handmade and vintage goods with over 30 million diverse listings, the problem of capturing the taste is particularly important - users come to the site specifically to find items that match their eclectic styles.
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Collaborative Filtering" scheme="http://www.dianacody.com/tags/Collaborative-Filtering/"/>
    
      <category term="Topic Modeling" scheme="http://www.dianacody.com/tags/Topic-Modeling/"/>
    
  </entry>
  
  <entry>
    <title>Accompany Plan</title>
    <link href="http://www.dianacody.com/2016/12/25/2016-12-25-AccompanyPlan/"/>
    <id>http://www.dianacody.com/2016/12/25/2016-12-25-AccompanyPlan/</id>
    <published>2016-12-24T16:00:00.000Z</published>
    <updated>2019-02-27T01:56:41.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-Project-Source-Code"><a href="#I-Project-Source-Code" class="headerlink" title="I. Project Source Code"></a>I. Project Source Code</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project source code URL: (This project is not open source at present.)</p>
<h5 id="II-Introduction"><a href="#II-Introduction" class="headerlink" title="II. Introduction"></a>II. Introduction</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is a new user entrance. If the user does not have a registered profile, then the interfaces of user prediction are called and the predicted users are displayed in the front-end. In that case, the user can quickly transform the predicted children data into children profile. In this way, the conversion rate of children’s profile can be improved, and the accuracy of children’s model prediction can be improved by combining online filling and off-line BI, and the range of recallable data sources can be expanded.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data Flow: User’s children profile -&gt; Mining new users -&gt; Recall predicted children by accompany plan -&gt; Update user profile -&gt;… (Loop)<br><img src="/resources/accompany-flow.jpg" alt="figure-accompany-flow"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data logic: Users who fill in high-quality potential children profile are regarded as seed users for new user prediction. By data analysis, it is found that as the growth of children, the commodities purchased by users are with obvious sequence characteristics. And the behavioral features can be extracted from the mother’s preference of commodities, categories, brand, product attributes in different time stages. It is obvious that the characteristics of commodities purchased by mother at their children’s different ages are different, so more potential users can be predicted by these characteristics.</p>
<h5 id="III-Main-Contents"><a href="#III-Main-Contents" class="headerlink" title="III. Main Contents"></a>III. Main Contents</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This is a mini recommendation system, and the upgrade development is based on the original system, including online interface and offline model.</p>
<h6 id="3-1-Upgrade-of-Online-Interface"><a href="#3-1-Upgrade-of-Online-Interface" class="headerlink" title="3.1 Upgrade of Online Interface"></a>3.1 Upgrade of Online Interface</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It includes predicting user’s age, ranking interface of children model, and displaying of different filter according to user’s specific pattern.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Label recommendation: After the first-level labels are ranked, the second-level label are ranked logically. (The second-level label is the sub-label of the first-level label displayed). And then whether the user has a subscription tag or not is recognized. If so, display the subscription tag in chronological order and the rest in default order.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data flow: Commodity category -&gt; User account -&gt; Quality score of related category -&gt; Summary of labels.</p>
<h6 id="3-2-Upgrade-of-Offline-User-Feature-Dimension"><a href="#3-2-Upgrade-of-Offline-User-Feature-Dimension" class="headerlink" title="3.2 Upgrade of Offline User Feature Dimension"></a>3.2 Upgrade of Offline User Feature Dimension</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The 200 third-level categories were added, and the algorithm rules were formulated according to the different extended attributes of user’s age, gender and season, and the quality scores of candidate sets were calculated as the reason for recalling and recommending commodities.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The results are with the following dimensions: three-level classification, age (school age) label, gender label, attribute label, commodity id, ranking quality score of candidate set, as shown in the table below.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Three-level categories</th>
<th>Age</th>
<th>Gender</th>
<th>Season</th>
<th>Extended attribute</th>
<th>sku-id</th>
<th>Quality score of ranking</th>
</tr>
</thead>
<tbody>
<tr>
<td>12345</td>
<td>72~83 months (6-year-old)</td>
<td>male</td>
<td>summer</td>
<td>1</td>
<td>1234567890</td>
<td>4.19615</td>
</tr>
</tbody>
</table>
</div>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Based on the three-level categories of commodities, the discrimination degree of the goods in age, gender and season is determined to filter the data, and the sku quality score and the ranking of labels or three-level categories are determined.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Data recall: According to the parameters from the front-end of the children’s age, gender and season, the corresponding products can be found and displayed in groups with the different label attributes they bind.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Online filtering: There is mainly some inventory filtering.</p>
<h5 id="IV-Enhanced-Value"><a href="#IV-Enhanced-Value" class="headerlink" title="IV. Enhanced Value"></a>IV. Enhanced Value</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong> Scenarioalization of User Labels </strong>: The recall data sets of categories were expanded.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong> Precisionalization of commodity recommendation </strong>: refine all categories in age, gender and season, and the accuracy of commodity recommendation was improved.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong> Renovation of user profile </strong>: The user behaviors were transformed into user profile, so that more users are participated in the iteration of children model prediction.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;By the end of the project, the application has owned one million active users, and the PV/UV value has been promoted.</p>
<h5 id="V-References"><a href="#V-References" class="headerlink" title="V. References"></a>V. References</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      The mini recommendation system.
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Recommender System" scheme="http://www.dianacody.com/tags/Recommender-System/"/>
    
      <category term="Online" scheme="http://www.dianacody.com/tags/Online/"/>
    
      <category term="Java" scheme="http://www.dianacody.com/tags/Java/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Personalized Recommendation System (Offline)</title>
    <link href="http://www.dianacody.com/2016/09/15/2016-09-15-RecOffline/"/>
    <id>http://www.dianacody.com/2016/09/15/2016-09-15-RecOffline/</id>
    <published>2016-09-14T16:00:00.000Z</published>
    <updated>2019-02-27T02:07:49.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-Project-Source-Code"><a href="#I-Project-Source-Code" class="headerlink" title="I. Project Source Code"></a>I. Project Source Code</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project source code URL: (This project is not open source at present.)</p>
<h5 id="II-Introduction"><a href="#II-Introduction" class="headerlink" title="II. Introduction"></a>II. Introduction</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The feature learning is to recognize and use features automatically. There are many successful cases with modern deep learning in the field of feature learning, which learn the description of features automatically in the unsupervised or semi-supervised way and in compressed form. The results are used to support advanced achievements in such fields as speech recognition, image classification, object recognition and others. The abstract feature expression can be acquired automatically, but only according to this, the feature results of learning cannot be understood and utilized, i.e. they can only be used in a way of black box. At the same time, how to create features similar to those with good results becomes more difficult.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Generally, the feature engineering goes through the following steps: data collection, data preprocessing, data modeling, feature design, feature selection, model evaluation, model iteration.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The model training in personalized system is mainly aimed at user’s past behavior, such as searching, browsing, click, adding cart and order. The appropriate features are selected to train model and predict user’s future behavior.</p>
<h5 id="III-Data-Collection"><a href="#III-Data-Collection" class="headerlink" title="III. Data Collection"></a>III. Data Collection</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The user feature logs are collected by parameter server, including some records of basic features such as user behavior location, time, frequency, preference, etc. The time windows are usually multi-day and multi-month, ranging from 7 to 30 days, depending on the model requirements of different experimental recommendation positions.</p>
<h5 id="IV-Data-Preprocessing"><a href="#IV-Data-Preprocessing" class="headerlink" title="IV. Data Preprocessing"></a>IV. Data Preprocessing</h5><h6 id="4-1-Data-Washing-and-Formatting"><a href="#4-1-Data-Washing-and-Formatting" class="headerlink" title="4.1 Data Washing and Formatting"></a>4.1 Data Washing and Formatting</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The data washing is to remove the noise in the data, such as some abnormal data, misreported data, missing data and so on. The data formatting is to organize the data into the input format needed by the model training framework and to facilitate observation.</p>
<h6 id="4-2-Data-Sampling"><a href="#4-2-Data-Sampling" class="headerlink" title="4.2 Data Sampling"></a>4.2 Data Sampling</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;According to the event tracking, the click and order data of each recommendation position are sampled proportionally. The sampling proportion varies according to the model of the different experimental recommendation position. Generally speaking, the click, post-click order and direct order are labeled as positive samples, which are labeled as action = 0, and are subdivided into action = 1, 2, 3 and so on, while the non-click and non-browse are labeled as negative samples, which are labeled as action = 0. Meanwhile, the positive samples need different sampling strategies, for example, a part of the click samples can be used as positive samples, and another part can be used as negative samples, which depends on what strategy used in training. The sample is a tag with a series of features, and it is either labeled order or click.</p>
<h6 id="4-3-Data-Conversion"><a href="#4-3-Data-Conversion" class="headerlink" title="4.3 Data Conversion"></a>4.3 Data Conversion</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As mentioned above.</p>
<h5 id="V-Model-Training"><a href="#V-Model-Training" class="headerlink" title="V. Model Training"></a>V. Model Training</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The artificial feature construction and automatic training of feature extraction are both implemented.</p>
<h6 id="5-1-Artificial-Feature-Construction"><a href="#5-1-Artificial-Feature-Construction" class="headerlink" title="5.1 Artificial Feature Construction"></a>5.1 Artificial Feature Construction</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The features are mainly used as initialization features, especially for some less active users. For example, the statistics are made in time window of one week or one month, filtering user’s browsing, click, adding cart, order as features. As shown below, each dimension is cross-multiplied to get about tens of basic features.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>sku</th>
<th>cid3</th>
<th>brand</th>
<th>productWord</th>
</tr>
</thead>
<tbody>
<tr>
<td>sku-7day-browse-total</td>
<td>cid3-7day-browse-total</td>
<td>brand-7day-browse-total</td>
<td>pwd-7day-browse-total</td>
</tr>
<tr>
<td>sku-7day-browse-avg</td>
<td>cid3-7day-browse-avg</td>
<td>brand-7day-browse-avg</td>
<td>pwd-7day-browse-avg</td>
</tr>
<tr>
<td>sku-7day-browse-ratio-all-station</td>
<td>cid3-7day-browse-ratio-all_station</td>
<td>brand-7day-browse-ratio-all-station</td>
<td>pwd-7day-browse-ratio-all-station</td>
</tr>
<tr>
<td>sku-7day-browse-ratio-category</td>
<td>cid3-7day-browse-ratio-category</td>
<td>brand-7day-browse-ratio-category</td>
<td>pwd-7day-browse-ratio-category</td>
</tr>
<tr>
<td>sku-7day-browse-users-total</td>
<td>cid3-7day-browse-users-total</td>
<td>brand-7day-browse-users-total</td>
<td>pwd-7day-browse-users-total</td>
</tr>
<tr>
<td>sku-7day-browse-users-avg</td>
<td>cid3-7day-browse-users-avg</td>
<td>brand-7day-browse-users-avg</td>
<td>pwd-7day-browse-users-avg</td>
</tr>
<tr>
<td>sku-7day-browse-users-ratio-all-station</td>
<td>cid3-7day-browse-users-ratio-all-station</td>
<td>brand-7day-browse-users-ratio-all-station</td>
<td>pwd-7day-browse-users-ratio-all-station</td>
</tr>
<tr>
<td>sku-7day-browse-users-ratio-category</td>
<td>cid3-7day-browse-users-ratio-category</td>
<td>brand-7day-browse-users-ratio-category</td>
<td>pwd-7day-browse-users-ratio-category</td>
</tr>
<tr>
<td>sku-7day-sale-total</td>
<td>cid3-7day-sale-total</td>
<td>brand-7day-sale-total</td>
<td>pwd-7day-sale-total</td>
</tr>
<tr>
<td>sku-7day-sale-avg</td>
<td>cid3-7day-sale-avg</td>
<td>brand-7day-sale-avg</td>
<td>pwd-7day-sale-avg</td>
</tr>
<tr>
<td>sku-7day-sale-ratio-all-station</td>
<td>cid3-7day-sale-ratio-all-station</td>
<td>brand-7day-sale-ratio-all-station</td>
<td>pwd-7day-sale-ratio-all-station</td>
</tr>
<tr>
<td>sku-7day-sale-ratio-category</td>
<td>cid3-7day-sale-ratio-category</td>
<td>brand-7day-sale-ratio-category</td>
<td>pwd-7day-sale-ratio-category</td>
</tr>
<tr>
<td>sku-7day-order-total</td>
<td>cid3-7day-order-total</td>
<td>brand-7day-order-total</td>
<td>pwd-7day-order-total</td>
</tr>
<tr>
<td>sku-7day-order-avg</td>
<td>cid3-7day-order-avg</td>
<td>brand-7day-order-avg</td>
<td>pwd-7day-order-avg</td>
</tr>
<tr>
<td>sku-7day-order-ratio-all-station</td>
<td>cid3-7day-order-ratio-all-station</td>
<td>brand-7day-order-ratio-all-station</td>
<td>pwd-7day-order-ratio-all-station</td>
</tr>
<tr>
<td>sku-7day-order-ratio-category</td>
<td>cid3-7day-order-ratio-category</td>
<td>brand-7day-order-ratio-category</td>
<td>pwd-7day-order-ratio-category</td>
</tr>
<tr>
<td>sku-7day-conversion-rate-total</td>
<td>cid3-7day-conversion-rate-total</td>
<td>brand-7day-conversion-rate-total</td>
<td>pwd-7day-conversion-rate-total</td>
</tr>
<tr>
<td>sku-7day-conversion-rate-avg</td>
<td>cid3-7day-conversion-rate-avg</td>
<td>brand-7day-conversion-rate-avg</td>
<td>pwd-7day-conversion-rate-avg</td>
</tr>
</tbody>
</table>
</div>
<h6 id="5-2-Automatic-Training-of-Feature-Extraction"><a href="#5-2-Automatic-Training-of-Feature-Extraction" class="headerlink" title="5.2 Automatic Training of Feature Extraction"></a>5.2 Automatic Training of Feature Extraction</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For the samples of highly active users, there are many features. The models are trained by Xgboost and TensorFlow directly to extract features automatically.</p>
<h5 id="VI-Model-Evaluation"><a href="#VI-Model-Evaluation" class="headerlink" title="VI. Model Evaluation"></a>VI. Model Evaluation</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Feature Importance;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hit rate of prediction (click@4/20, order@4/20);<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Online evaluating indicators (CTR/CVR);<br>There is an example to illustrate the comparison of different models:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>model</th>
<th>Experiments</th>
<th>trainSet</th>
<th>testSet</th>
<th>weight</th>
<th>click/order@N</th>
</tr>
</thead>
<tbody>
<tr>
<td>model_v1</td>
<td>101</td>
<td>7 days</td>
<td>2 days</td>
<td>50:5:1</td>
<td>0.23695</td>
</tr>
<tr>
<td>model_v2</td>
<td>102</td>
<td>30 days</td>
<td>2 days</td>
<td>50:5:1</td>
<td>0.26951</td>
</tr>
<tr>
<td>model_v3</td>
<td>103</td>
<td>30 days</td>
<td>2 days</td>
<td>50:10:1</td>
<td>0.25782</td>
</tr>
</tbody>
</table>
</div>
<h5 id="VII-Model-Iteration"><a href="#VII-Model-Iteration" class="headerlink" title="VII. Model Iteration"></a>VII. Model Iteration</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To adjust model iteratively, the model adjustment strategies of different experiment positions are also different. From the comparison among these above examples, it can be found that:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1) The offline indicators can be improved by increasing the number of training sets.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2) By adjusting the sampling proportion, the noise can be reduced, and it is better to sampling rigorously. The premise is that the training data sample is large enough.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3) Weight settings, it is setting of training positive and negative samples. The weight should be adjusted to overall positive correlation with each action proportion in the samples, rather than simply increase or decrease the weight of click and order, and it should be in accordance with the sample proportion.</p>
<h5 id="VIII-References"><a href="#VIII-References" class="headerlink" title="VIII. References"></a>VIII. References</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      The feature engineering and algorithmic logic of offline personalized recommendation system.
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Recommender System" scheme="http://www.dianacody.com/tags/Recommender-System/"/>
    
      <category term="python" scheme="http://www.dianacody.com/tags/python/"/>
    
      <category term="Offline" scheme="http://www.dianacody.com/tags/Offline/"/>
    
      <category term="Hive" scheme="http://www.dianacody.com/tags/Hive/"/>
    
      <category term="pig" scheme="http://www.dianacody.com/tags/pig/"/>
    
  </entry>
  
  <entry>
    <title>Personalized Recommendation System (Online)</title>
    <link href="http://www.dianacody.com/2016/07/29/2016-07-29-RecOnline/"/>
    <id>http://www.dianacody.com/2016/07/29/2016-07-29-RecOnline/</id>
    <published>2016-07-28T16:00:00.000Z</published>
    <updated>2019-02-27T02:14:08.000Z</updated>
    
    <content type="html"><![CDATA[<h5 id="I-Project-Source-Code"><a href="#I-Project-Source-Code" class="headerlink" title="I. Project Source Code"></a>I. Project Source Code</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Project source code URL: (This project is not open source at present.)</p>
<h5 id="II-Introduction"><a href="#II-Introduction" class="headerlink" title="II. Introduction"></a>II. Introduction</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Personalized user behavior and cold starting data models are got to build different scoring strategies online, and the same as offline. With these strategies, specific data are selected from the offline recallable data source, and are filtered and ranked as results for the online recommendation. AB-Testing experiments are observed online.</p>
<h5 id="III-Schemes"><a href="#III-Schemes" class="headerlink" title="III. Schemes"></a>III. Schemes</h5><p>The process structure is as follows.<br><img src="/resources/rec-online.jpg" alt="figure-rec-online"></p>
<h6 id="3-1-Parameters-Parsing"><a href="#3-1-Parameters-Parsing" class="headerlink" title="3.1 Parameters Parsing"></a>3.1 Parameters Parsing</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It includes the parsing parameters in configuration files, such as user account, user configuration, and experimental configuration.</p>
<h6 id="3-2-User-Model-Extraction"><a href="#3-2-User-Model-Extraction" class="headerlink" title="3.2 User Model Extraction"></a>3.2 User Model Extraction</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The models are obtained from various user behavior model services, such as real-time behavior model, long-term models as user profile, and some models trained by offline features.</p>
<h6 id="3-3-Data-Recall"><a href="#3-3-Data-Recall" class="headerlink" title="3.3 Data Recall"></a>3.3 Data Recall</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The following steps are independent of each other and do not interfere with each other, so they can run in parallel.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Based on collaborative filtering, the nearest result sets of similar users and similar commodities are gained, and then data is filtered by rules to get the candidate set lists.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Based on cold start data, candidate sets are read from user profile and commodity profile, and then data is filtered by rules to get candidate set lists.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Based on the recall results of the specific algorithm (omitted here), the data is read from the database and filtered by rules to get candidate set lists.</p>
<h6 id="3-4-Data-Fusion-and-Ranking"><a href="#3-4-Data-Fusion-and-Ranking" class="headerlink" title="3.4 Data Fusion and Ranking"></a>3.4 Data Fusion and Ranking</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The multi-model fusion is done, and the score weights are calculated, then the data are ranked according to the scores of different sets.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The quality score can also be gained by offline machine learning, and machine learning can also be used for online feature calculation.</p>
<h6 id="3-5-Data-Filtering"><a href="#3-5-Data-Filtering" class="headerlink" title="3.5 Data Filtering"></a>3.5 Data Filtering</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The ranked sets are filtered according to specific rules.</p>
<h6 id="3-6-Data-Alternation"><a href="#3-6-Data-Alternation" class="headerlink" title="3.6 Data Alternation"></a>3.6 Data Alternation</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;According to different bucket-dividing strategies, the alternation of category, brand word and product word is carried out to ensure the diversity of the results.</p>
<h6 id="3-7-Results-Display"><a href="#3-7-Results-Display" class="headerlink" title="3.7 Results Display"></a>3.7 Results Display</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The results are gained and displayed.</p>
<h5 id="IV-References"><a href="#IV-References" class="headerlink" title="IV. References"></a>IV. References</h5><p>[1]…<br>[2]…<br>[3]…</p>
]]></content>
    
    <summary type="html">
    
      The algorithmic logic of online personalized recommendation system.
    
    </summary>
    
      <category term="Recommender System" scheme="http://www.dianacody.com/categories/Recommender-System/"/>
    
    
      <category term="Recommender System" scheme="http://www.dianacody.com/tags/Recommender-System/"/>
    
      <category term="Online" scheme="http://www.dianacody.com/tags/Online/"/>
    
      <category term="Java" scheme="http://www.dianacody.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>最大匹配算法扩展</title>
    <link href="http://www.dianacody.com/2014/11/16/2014-11-16-HMM2/"/>
    <id>http://www.dianacody.com/2014/11/16/2014-11-16-HMM2/</id>
    <published>2014-11-15T16:00:00.000Z</published>
    <updated>2018-12-03T04:14:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于简单的中文分词匹配法做了扩展，其中比较有名的就是台湾蔡志浩老师1996年写的“MMSEG: A Word Identification System for Mandarin Chinese Text Based on Two Variants of the Maximum Matching Algorithm”，在这篇文章的页面中，不仅介绍了相关的中文分词算法，并且提供了一个C版本的mmseg供研究使用，目前根据该文及其代码移植的mmseg程序版本包括C++版、Java版、Python版及Ruby版，影响甚广。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此文是英文版本，建议有条件的读者直接读原文。不过国内也有该文的简介文章：《MMSeg分词算法简述》，原文似乎出自www.solol.org。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MMSEG中文分词系统的可以由一句话总结：The system consisted of a lexicon, two matching algorithms, and four ambiguity resolution rules（该系统包括一个词典，两种匹配算法，以及四种歧义消解规则）：</p>
<h5 id="1-词典（The-Lexicon）"><a href="#1-词典（The-Lexicon）" class="headerlink" title="1.词典（The Lexicon）"></a>1.词典（The Lexicon）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分两种形式，对于单个汉字的汉语词，除了汉字本身外，还包括其统计频率（这个频率属于先验知识，可以来自于已经人工分好词的训练语料库），而对于二字长及以上的汉语词，只要词条本身就可以了。</p>
<h5 id="2-匹配算法（Matching-Algorithm）"><a href="#2-匹配算法（Matching-Algorithm）" class="headerlink" title="2.匹配算法（Matching Algorithm）"></a>2.匹配算法（Matching Algorithm）</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) 简单匹配:对于字符串中的汉字Cn，用词典匹配以Cn开头的子串并查找所有可能的匹配；<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) 复杂匹配:对于字符串中的汉字Cn，查找所有可能以Cn开头的三词chunks，无论第一个汉语词是否有歧义。</p>
<h5 id="3-歧义消解规则（Ambiguity-Resolution-Rules）"><a href="#3-歧义消解规则（Ambiguity-Resolution-Rules）" class="headerlink" title="3.歧义消解规则（Ambiguity Resolution Rules）"></a>3.歧义消解规则（Ambiguity Resolution Rules）</h5><h6 id="规则一：最大匹配-Maximum-matching"><a href="#规则一：最大匹配-Maximum-matching" class="headerlink" title="规则一：最大匹配(Maximum matching)"></a>规则一：最大匹配(Maximum matching)</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a) 简单最大匹配算法,也就是我们常说的最大匹配法，不过作者采取的是正向匹配，并且按长度从小到大搜索词典：假设C1,C2,….代表一个字符串中的汉字，首先搜索词典，看 <em>C1</em>是否为一个单字组成的词语，然后搜索 <em>C1C2</em>来看是否为两个汉字组成的词语，以此类推，直至找到字典中最长的匹配。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b) 复杂最大匹配算法,由Chen 和Liu（1992）提出，其核心的假设是：The most plausible segmentation is the three-word chunk with maximum length. 请注意three-word chunk，可以将其翻译为“三词语块”，这也是MMSEG中比较核心的一个概念，这个最大匹配规则考虑问题比较全面，在对句子中的某个词进行切分时，如果有歧义拿不定主意，就再向后展望两个汉语词，并且找出所有可能的“三词语块”。例如，对于如下的“三词语块”，请注意括号中是注明的语块长度（以汉语单字为基本单位）：</p>
<ul>
<li><em>C1</em> <em>C2</em> <em>C3C4</em>（4）</li>
<li><em>C1C2</em> <em>C3C4</em> <em>C5</em>（5）</li>
<li><em>C1C2</em> <em>C3C4</em> <em>C5C6</em>（6）</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最大长度的“三词语块”是第3个，所以其第一汉语词<em>C1C2</em>将被作为正确的分词形式。以此类推，接下来我们从C3开始，找出所有可能的“三词语块”，重复上述规则，直到句子的最后一个词被划分。直观一点，对于以“眼”开头的如下5个“三词语块”,利用该规则，则“眼看”是正确的词语划分：</p>
<ul>
<li>眼看 就要 来了（6）</li>
<li>眼看 就要 来（5）</li>
<li>眼看 就 要(4)</li>
<li>眼 看 就要(4)</li>
<li>眼 看 就(3)</li>
</ul>
<h6 id="规则二：最大平均词长（Largest-average-word-length）"><a href="#规则二：最大平均词长（Largest-average-word-length）" class="headerlink" title="规则二：最大平均词长（Largest average word length）"></a>规则二：最大平均词长（Largest average word length）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在句子的末尾，很可能得到的“三词语块”只有一个或两个词（其他位置补空），例如，对于如下两个“三词语块”，他们拥有同样的长度：</p>
<ul>
<li><em>C1</em> <em>C2</em> <em>C3</em>（平均词长=1）</li>
<li><em>C1C2C3</em>（平均词长=3）</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这时规则1就无法解决其歧义消解问题，因此引入规则2：最大平均词长，也就是从这些语块中找出平均词长最大的语块，并选取其第一词语作为正确的词语切分形式。这个规则的前提假设是：It is more likely to encounter multi-character words than one-character words（在句子中遇到多字-词语的情况比单字-词语更有可能）.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此，上述两个“三词语块”中第二个<em>C1C2C3</em>就是最佳候选。直观一点，对于如下位于句尾三种形式的“三词语块”：</p>
<ul>
<li>国际化（平均词长=3）</li>
<li>国际 化（平均词长=1.5）</li>
<li>国 际 化（平均词长=1）</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在规则1无法求解的情况下，根据规则2，则“国际化”为最佳候选语块，因此该语块的第一个词“国际化”就是最佳的分词形式。</p>
<h6 id="规则三：最小词长方差（Smallest-variance-of-word-lengths）"><a href="#规则三：最小词长方差（Smallest-variance-of-word-lengths）" class="headerlink" title="规则三：最小词长方差（Smallest variance of word lengths）"></a>规则三：最小词长方差（Smallest variance of word lengths）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;还有一些歧义是规则一和规则二无法解决的，例如，如下的两个“三词语块”拥有同样的长度和同样的平均词长：</p>
<ul>
<li><em>C1C2</em> <em>C3C4</em> <em>C5C6</em></li>
<li><em>C1C2C3</em> <em>C4</em> <em>C5C6</em></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因此引入规则三：最小词长方差，也就是找出词长方差最小的语块，并选取其第一个词语作为正确的词语切分形式。在概率论和统计学中，一个随机变量的方差（Variance）描述的是它的离散程度，也就是该变量离其期望值的距离。因此该规则的前提假设是：Word lengths are usually evenly distributed（句子中的词语长度经常是均匀分布的）。直观来说，对于如下两个“三词语块”：</p>
<ul>
<li>研究 生命 起源</li>
<li>研究生 命 起源</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其长度为6，平均词长为2，规则一和规则二无能无力，利用规则三：</p>
<ul>
<li>语块1的方差 = ((2-2)^2+(2-2)^2+(2-2)^2)/3 = 0</li>
<li>语块2的方差 = ((3-2)^2+(1-2)^2+(2-2)^2)/3 = 2/3</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;则语块1为最佳候选，因此该语块的第一个词“研究”为最佳的分词形式。</p>
<h6 id="规则四：最大单字词语语素自由度之和（Largest-sum-of-degree-of-morphemic-freedom-of-one-character-words）"><a href="#规则四：最大单字词语语素自由度之和（Largest-sum-of-degree-of-morphemic-freedom-of-one-character-words）" class="headerlink" title="规则四：最大单字词语语素自由度之和（Largest sum of degree of morphemic freedom of one-character words）"></a>规则四：最大单字词语语素自由度之和（Largest sum of degree of morphemic freedom of one-character words）</h6><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如下所示，例子中的两个“三词语块”拥有同样的长度、平均词长及方差，因此上述三个规则都无法解决其歧义消解问题：</p>
<ul>
<li><em>C1</em> <em>C2</em> <em>C3C4</em></li>
<li><em>C1</em> <em>C2C3</em> <em>C4</em></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这两个语块都包括了两个单字（one-character）词语和一个两字（two-character）词语，规则四主要关注其中的单字词语。直观来看，有些汉字很少作为词语出现，而另一些汉字则常常作为词语出现，从统计角度来看，在语料库中出现频率高的汉字就很可能是一个单字词语，反之可能性就小。计算单词词语语素自由度之和的公式是对“三词语块”中的单字词语频率取对数并求和（The formula used to calculate the sum of degree of morphemic freedom is to sum log(frequency) of all one-character word(s) in a chunk.）规则四则选取其中和最大的语块，并将该语块的第一词语作为最佳的词语切分形式。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;关于MMSEG中文分词系统的框架就介绍到此，需要指出的是：</p>
<p>“It has to be noted that MMSEG was not designed to be a “professional level” system whose goal is 100% correct identification. Rather, MMSEG should be viewed as a general platform on which new ambiguity resolution algorithms can be tested.”</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;所以，不要认为有了MMSEG就可以解决中文分词的问题，更应该将MMSEG视为一个基本的平台，在该平台的基础上，可以尝试添加新的歧义消解算法以解决中文分词中的难点问题。</p>
]]></content>
    
    <summary type="html">
    
      基于简单的中文分词匹配法做了扩展，中文分词算法提供了一个C版本的mmseg供研究使用。
    
    </summary>
    
      <category term="NLP" scheme="http://www.dianacody.com/categories/NLP/"/>
    
    
      <category term="HMM" scheme="http://www.dianacody.com/tags/HMM/"/>
    
  </entry>
  
  <entry>
    <title>中文分词-最大匹配算法</title>
    <link href="http://www.dianacody.com/2014/11/07/2014-11-07-HMM/"/>
    <id>http://www.dianacody.com/2014/11/07/2014-11-07-HMM/</id>
    <published>2014-11-06T16:00:00.000Z</published>
    <updated>2018-04-30T10:47:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>正向最大匹配法算法如下所示:</p>
<p>逆向匹配法思想与正向一样，只是从右向左切分，这里举一个例子：</p>
<p>输入例句：S1=”计算语言学课程有意思” ；</p>
<p>定义：最大词长MaxLen = 5；S2= ” “；分隔符 = “/”；</p>
<p>假设存在词表：…，计算语言学，课程，意思，…；</p>
<p>最大逆向匹配分词算法过程如下：</p>
<ol>
<li><p>S2=””；S1不为空，从S1右边取出候选子串W=”课程有意思”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有意思”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”有意思”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”意思”</p>
</li>
<li><p>查词表，“意思”在词表中，将W加入到S2中，S2=” 意思/”，并将W从S1中去掉，此时S1=”计算语言学课程有”；</p>
</li>
<li><p>S1不为空，于是从S1左边取出候选子串W=”言学课程有”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程有”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程有”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”有”，这W是单字，将W加入到S2中，S2=“ /有 /意思”，并将W从S1中去掉，此时S1=”计算语言学课程”；</p>
</li>
<li><p>S1不为空，于是从S1左边取出候选子串W=”语言学课程”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”言学课程”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程”；</p>
</li>
<li><p>查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程”；</p>
</li>
<li><p>查词表，“意思”在词表中，将W加入到S2中，S2=“ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=”计算语言学”；</p>
</li>
<li><p>S1不为空，于是从S1左边取出候选子串W=”计算语言学”；</p>
</li>
<li><p>查词表，“计算语言学”在词表中，将W加入到S2中，S2=“计算语言学/ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=””；</p>
</li>
<li><p>S1为空，输出S2作为分词结果，分词过程结束。</p>
</li>
</ol>
<p>相应程序示例：</p>
<p>准备文件：建立一个词表文件wordlexicon，格式如下</p>
<ul>
<li>计算语言学</li>
<li>课程</li>
<li>意思</li>
</ul>
<p>输入文件：test,格式如下</p>
<ul>
<li>计算语言学课程有意思</li>
</ul>
<p>编译后执行如下：SegWord.exe test，输出分词结果文件：SegmentResult.txt</p>
<p><strong>源代码如下：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">// Dictionary.h</div><div class="line">#include &lt;iostream&gt;</div><div class="line">#include &lt;string&gt;</div><div class="line">#include &lt;fstream&gt;</div><div class="line">#include &lt;sstream&gt;</div><div class="line">#include &lt;hash_map&gt;</div><div class="line">using namespace std;</div><div class="line">using namespace stdext;</div><div class="line">class CDictionary</div><div class="line">&#123;</div><div class="line">	public:</div><div class="line">		CDictionary(); //将词典文件读入并构造为一个哈希词典</div><div class="line">		~CDictionary();</div><div class="line">		int FindWord(string w); //在哈希词典中查找词</div><div class="line">	private:</div><div class="line">		string strtmp; //读取词典的每一行</div><div class="line">		string word; //保存每个词</div><div class="line">		hash_map&lt;string, int&gt; wordhash; // 用于读取词典后的哈希</div><div class="line">		hash_map&lt;string, int &gt;::iterator worditer; //</div><div class="line">		typedef pair&lt;string, int&gt; sipair;</div><div class="line">&#125;;</div><div class="line">//将词典文件读入并构造为一个哈希词典</div><div class="line">CDictionary::CDictionary()</div><div class="line">&#123;</div><div class="line">	ifstream infile(“wordlexicon”); // 打开词典</div><div class="line">	if (!infile.is_open()) // 打开词典失败则退出程序</div><div class="line">	&#123;</div><div class="line">		cerr &lt;&lt; &quot;Unable to open input file: &quot; &lt;&lt; &quot;wordlexicon&quot;</div><div class="line">&lt;&lt; &quot; -- bailing out!&quot; &lt;&lt; endl;</div><div class="line">		exit(-1);</div><div class="line">	&#125;</div><div class="line">	while (getline(infile, strtmp, &apos;\\n&apos;)) // 读入词典的每一行并将其添加入哈希中</div><div class="line">	&#123;</div><div class="line">		istringstream istr(strtmp);</div><div class="line">		istr &gt;&gt; word; //读入每行第一个词</div><div class="line">		wordhash.insert(sipair(word, 1)); //插入到哈希中</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">CDictionary::~CDictionary()</div><div class="line">&#123;</div><div class="line">&#125;</div><div class="line"></div><div class="line">//在哈希词典中查找词，若找到，则返回，否则返回</div><div class="line">int CDictionary::FindWord(string w)</div><div class="line">&#123;</div><div class="line">	if (wordhash.find(w) != wordhash.end())</div><div class="line">	&#123;</div><div class="line">		return 1;</div><div class="line">	&#125;</div><div class="line">	else</div><div class="line">	&#123;</div><div class="line">		return 0;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><strong>主程序main.cpp</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div></pre></td><td class="code"><pre><div class="line">#include “Dictionary.h”</div><div class="line"># define MaxWordLength 10 // 最大词长为个字节（即个汉字）</div><div class="line"># define Separator “/ ” // 词界标记</div><div class="line">CDictionary WordDic; //初始化一个词典</div><div class="line">//对字符串用最大匹配法（正向或逆向）处理</div><div class="line">string SegmentSentence(string s1)</div><div class="line">&#123;</div><div class="line">	string s2 = “”; //用s2存放分词结果</div><div class="line">	while(!s1.empty())</div><div class="line">	&#123;</div><div class="line">		int len =(int) s1.length(); // 取输入串长度</div><div class="line">		if (len &gt; MaxWordLength) // 如果输入串长度大于最大词长</div><div class="line">		&#123;</div><div class="line">			len = MaxWordLength; // 只在最大词长范围内进行处理</div><div class="line">		&#125;</div><div class="line">		//string w = s1.substr(0, len); // （正向用）将输入串左边等于最大词长长度串取出作为候选词</div><div class="line">		string w = s1.substr(s1.length() – len, len); //逆向用</div><div class="line">		int n = WordDic.FindWord(w); // 在词典中查找相应的词</div><div class="line">		while(len &gt; 2 &amp;&amp; n == 0) // 如果不是词</div><div class="line">		&#123;</div><div class="line">			len -= 2; // 从候选词右边减掉一个汉字，将剩下的部分作为候选词</div><div class="line">			//w = w.substr(0, len); //正向用</div><div class="line">			w = s1.substr(s1.length() – len, len); //逆向用</div><div class="line">			n = WordDic.FindWord(w);</div><div class="line">		&#125;</div><div class="line">		//s2 += w + Separator; // (正向用）将匹配得到的词连同词界标记加到输出串末尾</div><div class="line">		w = w + Separator; // (逆向用)</div><div class="line">		s2 = w + s2 ; // (逆向用)</div><div class="line">		//s1 = s1.substr(w.length(), s1.length()); //(正向用)从s1-w处开始</div><div class="line">		s1 = s1.substr(0, s1.length() – len); // (逆向用)</div><div class="line">	&#125;</div><div class="line">	return s2;</div><div class="line">&#125;</div><div class="line">//对句子进行最大匹配法处理，包含对特殊字符的处理</div><div class="line">string SegmentSentenceMM (string s1)</div><div class="line">&#123;</div><div class="line">	string s2 = “”; //用s2存放分词结果</div><div class="line">	int i;</div><div class="line">	int dd;</div><div class="line">	while(!s1.empty() )</div><div class="line">	&#123;</div><div class="line">		unsigned char ch = (unsigned char)s1[0];</div><div class="line">		if (ch &lt; 128) // 处理西文字符</div><div class="line">		&#123;</div><div class="line">			i = 1;</div><div class="line">			dd = (int)s1.length();</div><div class="line">			while (i &lt; dd &amp;&amp; ((unsigned char)s1[i] &lt; 128) &amp;&amp; (s1[i] != 10) &amp;&amp; (s1[i] != 13)) // s1[i]不能是换行符或回车符</div><div class="line">			&#123;</div><div class="line">				i++;</div><div class="line">			&#125;</div><div class="line">			if ((ch != 32) &amp;&amp; (ch != 10) &amp;&amp; (ch != 13)) // 如果不是西文空格或换行或回车符</div><div class="line">			&#123;</div><div class="line">				s2 += s1.substr(0,i) + Separator;</div><div class="line">			&#125;</div><div class="line">			else</div><div class="line">			&#123;</div><div class="line">				//if (ch == 10 || ch == 13) // 如果是换行或回车符，将它拷贝给s2输出</div><div class="line">				if (ch == 10 || ch == 13 || ch == 32) //谢谢读者mces89的指正</div><div class="line">				&#123;</div><div class="line">					s2 += s1.substr(0, i);</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">			s1 = s1.substr(i,dd);</div><div class="line">			continue;</div><div class="line">		&#125;</div><div class="line">		else</div><div class="line">		&#123;</div><div class="line">			if (ch &lt; 176) // 中文标点等非汉字字符</div><div class="line">			&#123;</div><div class="line">				i = 0;</div><div class="line">				dd = (int)s1.length();</div><div class="line">				while(i &lt; dd &amp;&amp; ((unsigned char)s1[i] &lt; 176) &amp;&amp; ((unsigned char)s1[i] &gt;= 161)</div><div class="line">				&amp;&amp; (!((unsigned char)s1[i] == 161 &amp;&amp; ((unsigned char)s1[i+1] &gt;= 162 &amp;&amp; (unsigned char)s1[i+1] &lt;= 168)))</div><div class="line">				&amp;&amp; (!((unsigned char)s1[i] == 161 &amp;&amp; ((unsigned char)s1[i+1] &gt;= 171 &amp;&amp; (unsigned char)s1[i+1] &lt;= 191)))</div><div class="line">				&amp;&amp; (!((unsigned char)s1[i] == 163 &amp;&amp; ((unsigned char)s1[i+1] == 172 || (unsigned char)s1[i+1] == 161)</div><div class="line">				|| (unsigned char)s1[i+1] == 168 || (unsigned char)s1[i+1] == 169 || (unsigned char)s1[i+1] == 186</div><div class="line">				|| (unsigned char)s1[i+1] == 187 || (unsigned char)s1[i+1] == 191)))</div><div class="line">			&#123;</div><div class="line">				i = i + 2; // 假定没有半个汉字</div><div class="line">			&#125;</div><div class="line">			if (i == 0)</div><div class="line">			&#123;</div><div class="line">				i = i + 2;</div><div class="line">			&#125;</div><div class="line">			if (!(ch == 161 &amp;&amp; (unsigned char)s1[1] == 161)) // 不处理中文空格</div><div class="line">			&#123;</div><div class="line">				s2+=s1.substr(0, i) + Separator; // 其他的非汉字双字节字符可能连续输出</div><div class="line">			&#125;</div><div class="line">			s1 = s1.substr(i, dd);</div><div class="line">			continue;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">		// 以下处理汉字串</div><div class="line">		i = 2;</div><div class="line">		dd = (int)s1.length();</div><div class="line">		while(i &lt; dd &amp;&amp; (unsigned char)s1[i] &gt;= 176)</div><div class="line">		&#123;</div><div class="line">			i += 2;</div><div class="line">		&#125;</div><div class="line">		s2 += SegmentSentence(s1.substr(0, i));</div><div class="line">		s1 = s1.substr(i,dd);</div><div class="line">	&#125;</div><div class="line">	return s2;</div><div class="line">&#125;</div><div class="line"></div><div class="line">int main(int argc, char *argv[])</div><div class="line">&#123;</div><div class="line">	string strtmp; //用于保存从语料库中读入的每一行</div><div class="line">	string line; //用于输出每一行的结果</div><div class="line">	ifstream infile(argv[1]); // 打开输入文件</div><div class="line">	if (!infile.is_open()) // 打开输入文件失败则退出程序</div><div class="line">	&#123;</div><div class="line">		cerr &lt;&lt; &quot;Unable to open input file: &quot; &lt;&lt; argv[1]</div><div class="line">&lt;&lt; &quot; -- bailing out!&quot; &lt;&lt; endl;</div><div class="line">		exit(-1);</div><div class="line">&#125;</div><div class="line">	ofstream outfile1(&quot;SegmentResult.txt&quot;); //确定输出文件</div><div class="line">	if (!outfile1.is_open())</div><div class="line">	&#123;</div><div class="line">		cerr &lt;&lt; &quot;Unable to open file：SegmentResult.txt&quot;</div><div class="line">&lt;&lt; &quot;--bailing out!&quot; &lt;&lt; endl;</div><div class="line">		exit(-1);</div><div class="line">	&#125;</div><div class="line">	while (getline(infile, strtmp, &apos;n&apos;)) //读入语料库中的每一行并用最大匹配法处理</div><div class="line">	&#123;</div><div class="line">		line = strtmp;</div><div class="line">		line = SegmentSentenceMM(line); // 调用分词函数进行分词处理</div><div class="line">		outfile1 &lt;&lt; line &lt;&lt; endl; // 将分词结果写入目标文件</div><div class="line">	&#125;</div><div class="line">	return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>补充说明：如果使用正向匹配法，请将源代码中的相关注释 “//“互换。</p>
]]></content>
    
    <summary type="html">
    
      中文分词在中文信息处理中是最最基础的，无论机器翻译亦或信息检索还是其他相关应用，如果涉及中文，都离不开中文分词，因此中文分词具有极高的地位。中文分词入门最简单应该是最大匹配法了。
    
    </summary>
    
      <category term="NLP" scheme="http://www.dianacody.com/categories/NLP/"/>
    
    
      <category term="HMM" scheme="http://www.dianacody.com/tags/HMM/"/>
    
  </entry>
  
</feed>
